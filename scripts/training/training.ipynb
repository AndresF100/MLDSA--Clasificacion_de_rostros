{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generadores de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7534 images belonging to 1680 classes.\n",
      "Found 3829 images belonging to 1680 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_folder = \"../data_acquisition/train\"\n",
    "test_folder = \"../data_acquisition/test\"\n",
    "\n",
    "\n",
    "image_size = (250, 250)\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                rotation_range=5,\n",
    "                                                                width_shift_range=0.1,\n",
    "                                                                height_shift_range=0.1,\n",
    "                                                                shear_range=0.01,\n",
    "                                                                zoom_range=0.01,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                fill_mode='constant')\n",
    "\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "# Creación de los generadores\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory = train_folder + \"/\",\n",
    "                                                    target_size = image_size,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(directory = test_folder + \"/\",\n",
    "                                                       target_size = image_size,\n",
    "                                                       batch_size = batch_size,\n",
    "                                                       class_mode = 'categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se experimentará con un modelo preentrenado y con el mismo modelo agregando fine tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ya existe el subdirectorio o el archivo mlruns.\n"
     ]
    }
   ],
   "source": [
    "!mkdir mlruns\n",
    "\n",
    "# mlflow server --backend-store-uri sqlite:///tracking.db --default-artifact-root file:mlruns -p 5000 \n",
    "\n",
    "token = \"2YBABEo5mjbSd6lnmoNbFAGJbsf_5Rc66NjJRLbinDmw12UTr\" \n",
    "os.environ[\"NGROK_TOKEN\"] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: C:\\Users\\andre\\AppData\\Local/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2YBABEo5mjbSd6lnmoNbFAGJbsf_5Rc66NjJRLbinDmw12UTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-12-04T22:55:11-0500 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=C:\\\\Users\\\\andre\\\\AppData\\\\Local/ngrok/ngrok.yml legacy_path=C:\\\\Users\\\\andre\\\\.ngrok2\\\\ngrok.yml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<NgrokTunnel: \"https://c281-2800-484-6173-2700-94d0-a25f-966a-4e52.ngrok-free.app\" -> \"http://localhost:5000\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-12-04T22:55:26-0500 lvl=warn msg=\"failed to open private leg\" id=cd3caa9ffd32 privaddr=localhost:5000 err=\"dial tcp [::1]:5000: connectex: No connection could be made because the target machine actively refused it.\"\n",
      "t=2023-12-04T22:55:27-0500 lvl=warn msg=\"failed to open private leg\" id=9e2da9eda364 privaddr=localhost:5000 err=\"dial tcp [::1]:5000: connectex: No connection could be made because the target machine actively refused it.\"\n"
     ]
    }
   ],
   "source": [
    "# taskkill /f /im ngrok.exe\n",
    "from pyngrok import ngrok\n",
    "ngrok.connect(5000, \"http\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo preentrenado + fine tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 250, 250, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 256, 256, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 125, 125, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 125, 125, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 125, 125, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 127, 127, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 63, 63, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 63, 63, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 63, 63, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 63, 63, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 63, 63, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 63, 63, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 63, 63, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 63, 63, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 8, 8, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                weights='imagenet',\n",
    "                                                input_shape=(250,250,3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento(epochs=10):\n",
    "        \n",
    "    model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=(250,250,3))\n",
    "\n",
    "    # Congelamos el extractor de características (Transfer Learning)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "    # Creamos una capa de pooling para consolidar los feature maps de salida en 1024 valores\n",
    "    pool = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
    "    # Agregamos una capa densa\n",
    "    dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
    "    # Agregamos dropout para regularización\n",
    "    drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "    # Agregamos una capa de salida\n",
    "    dense2 = tf.keras.layers.Dense(units=train_generator.num_classes, activation=\"softmax\")(drop1)\n",
    "    # Definimos nuestro modelo de transfer learning\n",
    "    ft_model = tf.keras.models.Model(inputs=[model.input], outputs=[dense2])\n",
    "    # Compilamos el modelo\n",
    "    ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "    # ft_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Definimos el callback\n",
    "    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"warming_up_{epochs}_epochs.h5\",\n",
    "                                                    monitor=\"val_loss\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    hist_ft = ft_model.fit(x=train_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=train_generator.samples//128,\n",
    "                        callbacks=[best_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def mapeo_indices(listado):\n",
    "\n",
    "        listado_clases = list(validation_generator.class_indices.keys())\n",
    "\n",
    "        true_index = np.argmax(listado,axis=1)\n",
    "        salida_real = [listado_clases[i] for i in true_index]\n",
    "\n",
    "        return salida_real\n",
    "\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(validation_generator)):\n",
    "        X_val_batch, y_val_batch = validation_generator.next()\n",
    "        y_pred_batch = ft_model.predict(X_val_batch)\n",
    "        y_true += mapeo_indices(y_val_batch)\n",
    "        y_pred += mapeo_indices(y_pred_batch)\n",
    "\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
    "        })\n",
    "\n",
    "\n",
    "    mlflow.sklearn.log_model(ft_model, f\"model_{epochs}_epochs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_id: 955390070555724847\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "exp_id = mlflow.create_experiment(name=\"modelo_base_con_fine\", artifact_location=\"mlruns/\")\n",
    "print(f\"exp_id: {exp_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3932 - accuracy: 0.0032\n",
      "Epoch 1: val_loss improved from inf to 7.33877, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 36s 509ms/step - loss: 7.3932 - accuracy: 0.0032 - val_loss: 7.3388 - val_accuracy: 0.0034\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3048 - accuracy: 0.0043\n",
      "Epoch 2: val_loss improved from 7.33877 to 7.29551, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 27s 462ms/step - loss: 7.3048 - accuracy: 0.0043 - val_loss: 7.2955 - val_accuracy: 0.0034\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1731 - accuracy: 0.0032\n",
      "Epoch 3: val_loss did not improve from 7.29551\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.1731 - accuracy: 0.0032 - val_loss: 7.3232 - val_accuracy: 0.0026\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1718 - accuracy: 0.0032\n",
      "Epoch 4: val_loss improved from 7.29551 to 7.28912, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 459ms/step - loss: 7.1718 - accuracy: 0.0032 - val_loss: 7.2891 - val_accuracy: 0.0026\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1165 - accuracy: 0.0022\n",
      "Epoch 5: val_loss did not improve from 7.28912\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 7.1165 - accuracy: 0.0022 - val_loss: 7.3192 - val_accuracy: 0.0042\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1078 - accuracy: 0.0054\n",
      "Epoch 6: val_loss did not improve from 7.28912\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 7.1078 - accuracy: 0.0054 - val_loss: 7.3279 - val_accuracy: 0.0050\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1008 - accuracy: 0.0086\n",
      "Epoch 7: val_loss improved from 7.28912 to 7.25179, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 455ms/step - loss: 7.1008 - accuracy: 0.0086 - val_loss: 7.2518 - val_accuracy: 0.0047\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0615 - accuracy: 0.0043\n",
      "Epoch 8: val_loss improved from 7.25179 to 7.25120, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.0615 - accuracy: 0.0043 - val_loss: 7.2512 - val_accuracy: 0.0047\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0688 - accuracy: 0.0097\n",
      "Epoch 9: val_loss did not improve from 7.25120\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 7.0688 - accuracy: 0.0097 - val_loss: 7.2807 - val_accuracy: 0.0050\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0740 - accuracy: 0.0054\n",
      "Epoch 10: val_loss improved from 7.25120 to 7.22493, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.0740 - accuracy: 0.0054 - val_loss: 7.2249 - val_accuracy: 0.0055\n",
      "1/1 [==============================] - 1s 868ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7678775b-41e3-482f-8734-5f7353808e0f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7678775b-41e3-482f-8734-5f7353808e0f/assets\n",
      "2023/12/04 23:06:12 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmp941x4b09\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
      "d:\\Conda\\envs\\tf310\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"epochs_10\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")\n",
    "\n",
    "\n",
    "experimento(epochs=10)\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n",
      "Epoch 1/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3902 - accuracy: 0.0032\n",
      "Epoch 1: val_loss improved from inf to 7.33352, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 30s 472ms/step - loss: 7.3902 - accuracy: 0.0032 - val_loss: 7.3335 - val_accuracy: 0.0042\n",
      "Epoch 2/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.2053 - accuracy: 0.0032\n",
      "Epoch 2: val_loss improved from 7.33352 to 7.29165, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 451ms/step - loss: 7.2053 - accuracy: 0.0032 - val_loss: 7.2916 - val_accuracy: 0.0044\n",
      "Epoch 3/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1751 - accuracy: 0.0065\n",
      "Epoch 3: val_loss improved from 7.29165 to 7.26353, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 452ms/step - loss: 7.1751 - accuracy: 0.0065 - val_loss: 7.2635 - val_accuracy: 0.0047\n",
      "Epoch 4/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1223 - accuracy: 0.0054\n",
      "Epoch 4: val_loss improved from 7.26353 to 7.25972, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 7.1223 - accuracy: 0.0054 - val_loss: 7.2597 - val_accuracy: 0.0052\n",
      "Epoch 5/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1292 - accuracy: 0.0065\n",
      "Epoch 5: val_loss did not improve from 7.25972\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 7.1292 - accuracy: 0.0065 - val_loss: 7.2615 - val_accuracy: 0.0044\n",
      "Epoch 6/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1528 - accuracy: 0.0054\n",
      "Epoch 6: val_loss did not improve from 7.25972\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 7.1528 - accuracy: 0.0054 - val_loss: 7.2989 - val_accuracy: 0.0047\n",
      "Epoch 7/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1528 - accuracy: 0.0065\n",
      "Epoch 7: val_loss improved from 7.25972 to 7.25650, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 7.1528 - accuracy: 0.0065 - val_loss: 7.2565 - val_accuracy: 0.0055\n",
      "Epoch 8/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1252 - accuracy: 0.0065\n",
      "Epoch 8: val_loss improved from 7.25650 to 7.23971, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 451ms/step - loss: 7.1252 - accuracy: 0.0065 - val_loss: 7.2397 - val_accuracy: 0.0055\n",
      "Epoch 9/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1033 - accuracy: 0.0076\n",
      "Epoch 9: val_loss improved from 7.23971 to 7.23543, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 7.1033 - accuracy: 0.0076 - val_loss: 7.2354 - val_accuracy: 0.0055\n",
      "Epoch 10/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0950 - accuracy: 0.0054\n",
      "Epoch 10: val_loss did not improve from 7.23543\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0950 - accuracy: 0.0054 - val_loss: 7.3122 - val_accuracy: 0.0044\n",
      "Epoch 11/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0560 - accuracy: 0.0065\n",
      "Epoch 11: val_loss did not improve from 7.23543\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 7.0560 - accuracy: 0.0065 - val_loss: 7.3091 - val_accuracy: 0.0047\n",
      "Epoch 12/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0018 - accuracy: 0.0086\n",
      "Epoch 12: val_loss did not improve from 7.23543\n",
      "58/58 [==============================] - 25s 441ms/step - loss: 7.0018 - accuracy: 0.0086 - val_loss: 7.2565 - val_accuracy: 0.0044\n",
      "Epoch 13/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1101 - accuracy: 0.0086\n",
      "Epoch 13: val_loss improved from 7.23543 to 7.21724, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 7.1101 - accuracy: 0.0086 - val_loss: 7.2172 - val_accuracy: 0.0044\n",
      "Epoch 14/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0427 - accuracy: 0.0043\n",
      "Epoch 14: val_loss did not improve from 7.21724\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 7.0427 - accuracy: 0.0043 - val_loss: 7.2571 - val_accuracy: 0.0057\n",
      "Epoch 15/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0758 - accuracy: 0.0075\n",
      "Epoch 15: val_loss improved from 7.21724 to 7.21290, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 7.0758 - accuracy: 0.0075 - val_loss: 7.2129 - val_accuracy: 0.0044\n",
      "Epoch 16/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0472 - accuracy: 0.0032\n",
      "Epoch 16: val_loss improved from 7.21290 to 7.20167, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 455ms/step - loss: 7.0472 - accuracy: 0.0032 - val_loss: 7.2017 - val_accuracy: 0.0044\n",
      "Epoch 17/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0638 - accuracy: 0.0108\n",
      "Epoch 17: val_loss improved from 7.20167 to 7.19958, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 451ms/step - loss: 7.0638 - accuracy: 0.0108 - val_loss: 7.1996 - val_accuracy: 0.0055\n",
      "Epoch 18/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0424 - accuracy: 0.0097\n",
      "Epoch 18: val_loss did not improve from 7.19958\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 7.0424 - accuracy: 0.0097 - val_loss: 7.2189 - val_accuracy: 0.0055\n",
      "Epoch 19/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9574 - accuracy: 0.0086\n",
      "Epoch 19: val_loss did not improve from 7.19958\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9574 - accuracy: 0.0086 - val_loss: 7.3062 - val_accuracy: 0.0055\n",
      "Epoch 20/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0179 - accuracy: 0.0054\n",
      "Epoch 20: val_loss did not improve from 7.19958\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0179 - accuracy: 0.0054 - val_loss: 7.2307 - val_accuracy: 0.0050\n",
      "Epoch 21/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9463 - accuracy: 0.0054\n",
      "Epoch 21: val_loss improved from 7.19958 to 7.19676, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 452ms/step - loss: 6.9463 - accuracy: 0.0054 - val_loss: 7.1968 - val_accuracy: 0.0055\n",
      "Epoch 22/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0463 - accuracy: 0.0097\n",
      "Epoch 22: val_loss improved from 7.19676 to 7.17317, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 7.0463 - accuracy: 0.0097 - val_loss: 7.1732 - val_accuracy: 0.0055\n",
      "Epoch 23/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9982 - accuracy: 0.0086\n",
      "Epoch 23: val_loss improved from 7.17317 to 7.17233, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 454ms/step - loss: 6.9982 - accuracy: 0.0086 - val_loss: 7.1723 - val_accuracy: 0.0055\n",
      "Epoch 24/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9689 - accuracy: 0.0032\n",
      "Epoch 24: val_loss did not improve from 7.17233\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9689 - accuracy: 0.0032 - val_loss: 7.1943 - val_accuracy: 0.0055\n",
      "Epoch 25/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0092 - accuracy: 0.0032\n",
      "Epoch 25: val_loss did not improve from 7.17233\n",
      "58/58 [==============================] - 25s 444ms/step - loss: 7.0092 - accuracy: 0.0032 - val_loss: 7.2408 - val_accuracy: 0.0057\n",
      "Epoch 26/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9707 - accuracy: 0.0086\n",
      "Epoch 26: val_loss improved from 7.17233 to 7.17012, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.9707 - accuracy: 0.0086 - val_loss: 7.1701 - val_accuracy: 0.0055\n",
      "Epoch 27/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0084 - accuracy: 0.0022\n",
      "Epoch 27: val_loss did not improve from 7.17012\n",
      "58/58 [==============================] - 26s 446ms/step - loss: 7.0084 - accuracy: 0.0022 - val_loss: 7.1861 - val_accuracy: 0.0044\n",
      "Epoch 28/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0457 - accuracy: 0.0054\n",
      "Epoch 28: val_loss did not improve from 7.17012\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0457 - accuracy: 0.0054 - val_loss: 7.2430 - val_accuracy: 0.0055\n",
      "Epoch 29/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9830 - accuracy: 0.0075\n",
      "Epoch 29: val_loss improved from 7.17012 to 7.16702, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 6.9830 - accuracy: 0.0075 - val_loss: 7.1670 - val_accuracy: 0.0047\n",
      "Epoch 30/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0079 - accuracy: 0.0054\n",
      "Epoch 30: val_loss improved from 7.16702 to 7.16159, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 452ms/step - loss: 7.0079 - accuracy: 0.0054 - val_loss: 7.1616 - val_accuracy: 0.0044\n",
      "Epoch 31/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0048 - accuracy: 0.0097\n",
      "Epoch 31: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 446ms/step - loss: 7.0048 - accuracy: 0.0097 - val_loss: 7.1690 - val_accuracy: 0.0047\n",
      "Epoch 32/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9472 - accuracy: 0.0065\n",
      "Epoch 32: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9472 - accuracy: 0.0065 - val_loss: 7.1951 - val_accuracy: 0.0047\n",
      "Epoch 33/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9073 - accuracy: 0.0054\n",
      "Epoch 33: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9073 - accuracy: 0.0054 - val_loss: 7.2423 - val_accuracy: 0.0047\n",
      "Epoch 34/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9666 - accuracy: 0.0086\n",
      "Epoch 34: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9666 - accuracy: 0.0086 - val_loss: 7.1704 - val_accuracy: 0.0047\n",
      "Epoch 35/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0133 - accuracy: 0.0032\n",
      "Epoch 35: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0133 - accuracy: 0.0032 - val_loss: 7.2789 - val_accuracy: 0.0047\n",
      "Epoch 36/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0055 - accuracy: 0.0108\n",
      "Epoch 36: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 7.0055 - accuracy: 0.0108 - val_loss: 7.1694 - val_accuracy: 0.0047\n",
      "Epoch 37/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9714 - accuracy: 0.0054\n",
      "Epoch 37: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9714 - accuracy: 0.0054 - val_loss: 7.1754 - val_accuracy: 0.0047\n",
      "Epoch 38/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9358 - accuracy: 0.0054\n",
      "Epoch 38: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9358 - accuracy: 0.0054 - val_loss: 7.2023 - val_accuracy: 0.0047\n",
      "Epoch 39/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9220 - accuracy: 0.0054\n",
      "Epoch 39: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 6.9220 - accuracy: 0.0054 - val_loss: 7.2225 - val_accuracy: 0.0044\n",
      "Epoch 40/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9919 - accuracy: 0.0075\n",
      "Epoch 40: val_loss improved from 7.16159 to 7.15094, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.9919 - accuracy: 0.0075 - val_loss: 7.1509 - val_accuracy: 0.0047\n",
      "Epoch 41/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9441 - accuracy: 0.0075\n",
      "Epoch 41: val_loss did not improve from 7.15094\n",
      "58/58 [==============================] - 26s 446ms/step - loss: 6.9441 - accuracy: 0.0075 - val_loss: 7.2015 - val_accuracy: 0.0047\n",
      "Epoch 42/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9899 - accuracy: 0.0054\n",
      "Epoch 42: val_loss improved from 7.15094 to 7.14564, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 6.9899 - accuracy: 0.0054 - val_loss: 7.1456 - val_accuracy: 0.0047\n",
      "Epoch 43/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9673 - accuracy: 0.0054\n",
      "Epoch 43: val_loss improved from 7.14564 to 7.13779, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 454ms/step - loss: 6.9673 - accuracy: 0.0054 - val_loss: 7.1378 - val_accuracy: 0.0047\n",
      "Epoch 44/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8874 - accuracy: 0.0076\n",
      "Epoch 44: val_loss did not improve from 7.13779\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.8874 - accuracy: 0.0076 - val_loss: 7.1450 - val_accuracy: 0.0050\n",
      "Epoch 45/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9201 - accuracy: 0.0065\n",
      "Epoch 45: val_loss did not improve from 7.13779\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9201 - accuracy: 0.0065 - val_loss: 7.1569 - val_accuracy: 0.0050\n",
      "Epoch 46/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9337 - accuracy: 0.0043\n",
      "Epoch 46: val_loss did not improve from 7.13779\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9337 - accuracy: 0.0043 - val_loss: 7.1397 - val_accuracy: 0.0052\n",
      "Epoch 47/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0147 - accuracy: 0.0043\n",
      "Epoch 47: val_loss improved from 7.13779 to 7.12613, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 449ms/step - loss: 7.0147 - accuracy: 0.0043 - val_loss: 7.1261 - val_accuracy: 0.0050\n",
      "Epoch 48/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9558 - accuracy: 0.0022\n",
      "Epoch 48: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9558 - accuracy: 0.0022 - val_loss: 7.1845 - val_accuracy: 0.0050\n",
      "Epoch 49/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9448 - accuracy: 0.0032\n",
      "Epoch 49: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9448 - accuracy: 0.0032 - val_loss: 7.1280 - val_accuracy: 0.0057\n",
      "Epoch 50/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8911 - accuracy: 0.0065\n",
      "Epoch 50: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.8911 - accuracy: 0.0065 - val_loss: 7.1468 - val_accuracy: 0.0047\n",
      "Epoch 51/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9228 - accuracy: 0.0076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyngrok.process.ngrok:t=2023-12-04T23:28:06-0500 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=73df649ad33c clientid=2ebd534bfd6f26ec87f1c66bd3feffe1\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-04T23:28:06-0500 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"session closed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9228 - accuracy: 0.0076 - val_loss: 7.2213 - val_accuracy: 0.0050\n",
      "Epoch 52/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9114 - accuracy: 0.0097\n",
      "Epoch 52: val_loss improved from 7.12613 to 7.12261, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.9114 - accuracy: 0.0097 - val_loss: 7.1226 - val_accuracy: 0.0052\n",
      "Epoch 53/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8821 - accuracy: 0.0043\n",
      "Epoch 53: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.8821 - accuracy: 0.0043 - val_loss: 7.1263 - val_accuracy: 0.0055\n",
      "Epoch 54/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9485 - accuracy: 0.0054\n",
      "Epoch 54: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9485 - accuracy: 0.0054 - val_loss: 7.1475 - val_accuracy: 0.0052\n",
      "Epoch 55/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9484 - accuracy: 0.0108\n",
      "Epoch 55: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9484 - accuracy: 0.0108 - val_loss: 7.1346 - val_accuracy: 0.0060\n",
      "Epoch 56/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8958 - accuracy: 0.0043\n",
      "Epoch 56: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.8958 - accuracy: 0.0043 - val_loss: 7.1309 - val_accuracy: 0.0060\n",
      "Epoch 57/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9445 - accuracy: 0.0097\n",
      "Epoch 57: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9445 - accuracy: 0.0097 - val_loss: 7.1568 - val_accuracy: 0.0055\n",
      "Epoch 58/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9065 - accuracy: 0.0086\n",
      "Epoch 58: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9065 - accuracy: 0.0086 - val_loss: 7.2659 - val_accuracy: 0.0055\n",
      "Epoch 59/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9095 - accuracy: 0.0054\n",
      "Epoch 59: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 441ms/step - loss: 6.9095 - accuracy: 0.0054 - val_loss: 7.1337 - val_accuracy: 0.0060\n",
      "Epoch 60/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8842 - accuracy: 0.0011\n",
      "Epoch 60: val_loss improved from 7.12261 to 7.11551, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.8842 - accuracy: 0.0011 - val_loss: 7.1155 - val_accuracy: 0.0060\n",
      "Epoch 61/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9358 - accuracy: 0.0032\n",
      "Epoch 61: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9358 - accuracy: 0.0032 - val_loss: 7.1945 - val_accuracy: 0.0063\n",
      "Epoch 62/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9092 - accuracy: 0.0097\n",
      "Epoch 62: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9092 - accuracy: 0.0097 - val_loss: 7.1817 - val_accuracy: 0.0052\n",
      "Epoch 63/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9088 - accuracy: 0.0065\n",
      "Epoch 63: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9088 - accuracy: 0.0065 - val_loss: 7.1643 - val_accuracy: 0.0055\n",
      "Epoch 64/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8839 - accuracy: 0.0043\n",
      "Epoch 64: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.8839 - accuracy: 0.0043 - val_loss: 7.2325 - val_accuracy: 0.0057\n",
      "Epoch 65/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9466 - accuracy: 0.0086\n",
      "Epoch 65: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9466 - accuracy: 0.0086 - val_loss: 7.1346 - val_accuracy: 0.0047\n",
      "Epoch 66/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9298 - accuracy: 0.0119\n",
      "Epoch 66: val_loss improved from 7.11551 to 7.11032, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 6.9298 - accuracy: 0.0119 - val_loss: 7.1103 - val_accuracy: 0.0047\n",
      "Epoch 67/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9358 - accuracy: 0.0032\n",
      "Epoch 67: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9358 - accuracy: 0.0032 - val_loss: 7.1580 - val_accuracy: 0.0047\n",
      "Epoch 68/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9232 - accuracy: 0.0097\n",
      "Epoch 68: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9232 - accuracy: 0.0097 - val_loss: 7.1260 - val_accuracy: 0.0047\n",
      "Epoch 69/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9344 - accuracy: 0.0054\n",
      "Epoch 69: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9344 - accuracy: 0.0054 - val_loss: 7.1142 - val_accuracy: 0.0047\n",
      "Epoch 70/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9281 - accuracy: 0.0097\n",
      "Epoch 70: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9281 - accuracy: 0.0097 - val_loss: 7.1517 - val_accuracy: 0.0047\n",
      "1/1 [==============================] - 1s 872ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3c9f7aae-3e62-44b2-8506-ea5df6a6a91a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3c9f7aae-3e62-44b2-8506-ea5df6a6a91a/assets\n",
      "2023/12/04 23:37:23 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmp_ddtm17e\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T00:38:02-0500 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=96dc9487a7ba clientid=2ebd534bfd6f26ec87f1c66bd3feffe1\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T00:38:02-0500 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"session closed\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:44-0500 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=ee731389ddea clientid=2ebd534bfd6f26ec87f1c66bd3feffe1\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:44-0500 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"session closed\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:45-0500 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:46-0500 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:47-0500 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"epochs_70\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")\n",
    "\n",
    "\n",
    "experimento(epochs=70)\n",
    "\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 70 con reentrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento2(epochs_warm=10,epochs_train=70):\n",
    "        \n",
    "    model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=(250,250,3))\n",
    "\n",
    "    # Congelamos el extractor de características (Transfer Learning)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "    # Creamos una capa de pooling para consolidar los feature maps de salida en 1024 valores\n",
    "    pool = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
    "    # Agregamos una capa densa\n",
    "    dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
    "    # Agregamos dropout para regularización\n",
    "    drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "    # Agregamos una capa de salida\n",
    "    dense2 = tf.keras.layers.Dense(units=train_generator.num_classes, activation=\"softmax\")(drop1)\n",
    "    # Definimos nuestro modelo de transfer learning\n",
    "    ft_model = tf.keras.models.Model(inputs=[model.input], outputs=[dense2])\n",
    "    # Compilamos el modelo\n",
    "    ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "    # ft_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Definimos el callback\n",
    "    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"warming_up_{epochs_warm}_epochs_warm.h5\",\n",
    "                                                    monitor=\"val_loss\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    hist_ft = ft_model.fit(x=train_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs_warm,\n",
    "                        steps_per_epoch=train_generator.samples//128,\n",
    "                        callbacks=[best_callback])\n",
    "\n",
    "    ########################################## re entrenamiento ##########################################\n",
    "\n",
    "    # Hacemos entrenables todas las capas\n",
    "    for layer in ft_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Disminuímos el learning rate\n",
    "    ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    # Cargamos los pesos del calentamiento\n",
    "    ft_model.load_weights(f\"warming_up_{epochs_warm}_epochs_warm.h5\")\n",
    "\n",
    "    # Definimos el callback\n",
    "    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"fine_tuning.h5\",\n",
    "                                                    monitor=\"val_loss\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    hist_ft = ft_model.fit(x=train_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs_train,\n",
    "                        steps_per_epoch=train_generator.samples//16,\n",
    "                        callbacks=[best_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def mapeo_indices(listado):\n",
    "\n",
    "        listado_clases = list(validation_generator.class_indices.keys())\n",
    "\n",
    "        true_index = np.argmax(listado,axis=1)\n",
    "        salida_real = [listado_clases[i] for i in true_index]\n",
    "\n",
    "        return salida_real\n",
    "\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(validation_generator)):\n",
    "        X_val_batch, y_val_batch = validation_generator.next()\n",
    "        y_pred_batch = ft_model.predict(X_val_batch)\n",
    "        y_true += mapeo_indices(y_val_batch)\n",
    "        y_pred += mapeo_indices(y_pred_batch)\n",
    "\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
    "        })\n",
    "\n",
    "\n",
    "    mlflow.sklearn.log_model(ft_model, f\"model_{epochs_warm}_epochs_warm_{epochs_train}_epochs_train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.4088 - accuracy: 0.0043\n",
      "Epoch 1: val_loss improved from inf to 7.37874, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 46s 710ms/step - loss: 7.4088 - accuracy: 0.0043 - val_loss: 7.3787 - val_accuracy: 0.0031\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3056 - accuracy: 0.0065\n",
      "Epoch 2: val_loss improved from 7.37874 to 7.31343, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 28s 483ms/step - loss: 7.3056 - accuracy: 0.0065 - val_loss: 7.3134 - val_accuracy: 0.0026\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.2482 - accuracy: 0.0022\n",
      "Epoch 3: val_loss improved from 7.31343 to 7.28730, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 28s 484ms/step - loss: 7.2482 - accuracy: 0.0022 - val_loss: 7.2873 - val_accuracy: 0.0031\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1803 - accuracy: 0.0043\n",
      "Epoch 4: val_loss improved from 7.28730 to 7.27428, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 477ms/step - loss: 7.1803 - accuracy: 0.0043 - val_loss: 7.2743 - val_accuracy: 0.0050\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1484 - accuracy: 0.0075\n",
      "Epoch 5: val_loss improved from 7.27428 to 7.26903, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 477ms/step - loss: 7.1484 - accuracy: 0.0075 - val_loss: 7.2690 - val_accuracy: 0.0050\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1801 - accuracy: 0.0086\n",
      "Epoch 6: val_loss improved from 7.26903 to 7.26600, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 475ms/step - loss: 7.1801 - accuracy: 0.0086 - val_loss: 7.2660 - val_accuracy: 0.0050\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1533 - accuracy: 0.0054\n",
      "Epoch 7: val_loss improved from 7.26600 to 7.25747, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 473ms/step - loss: 7.1533 - accuracy: 0.0054 - val_loss: 7.2575 - val_accuracy: 0.0050\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1601 - accuracy: 0.0075\n",
      "Epoch 8: val_loss improved from 7.25747 to 7.24848, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 474ms/step - loss: 7.1601 - accuracy: 0.0075 - val_loss: 7.2485 - val_accuracy: 0.0050\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1182 - accuracy: 0.0054\n",
      "Epoch 9: val_loss improved from 7.24848 to 7.24173, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 472ms/step - loss: 7.1182 - accuracy: 0.0054 - val_loss: 7.2417 - val_accuracy: 0.0050\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1505 - accuracy: 0.0054\n",
      "Epoch 10: val_loss improved from 7.24173 to 7.23634, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 470ms/step - loss: 7.1505 - accuracy: 0.0054 - val_loss: 7.2363 - val_accuracy: 0.0050\n",
      "Epoch 1/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 7.0560 - accuracy: 0.0080\n",
      "Epoch 1: val_loss improved from inf to 7.26265, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 132s 265ms/step - loss: 7.0560 - accuracy: 0.0080 - val_loss: 7.2626 - val_accuracy: 0.0042\n",
      "Epoch 2/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 6.6745 - accuracy: 0.0294\n",
      "Epoch 2: val_loss improved from 7.26265 to 6.77277, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 6.6745 - accuracy: 0.0294 - val_loss: 6.7728 - val_accuracy: 0.0326\n",
      "Epoch 3/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 6.2341 - accuracy: 0.0585\n",
      "Epoch 3: val_loss improved from 6.77277 to 6.46005, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 6.2341 - accuracy: 0.0585 - val_loss: 6.4600 - val_accuracy: 0.0575\n",
      "Epoch 4/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.8669 - accuracy: 0.0904\n",
      "Epoch 4: val_loss improved from 6.46005 to 6.02275, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 5.8669 - accuracy: 0.0904 - val_loss: 6.0228 - val_accuracy: 0.1011\n",
      "Epoch 5/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.5304 - accuracy: 0.1260\n",
      "Epoch 5: val_loss improved from 6.02275 to 5.91919, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 5.5304 - accuracy: 0.1260 - val_loss: 5.9192 - val_accuracy: 0.1222\n",
      "Epoch 6/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.1976 - accuracy: 0.1619\n",
      "Epoch 6: val_loss improved from 5.91919 to 5.46014, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 5.1976 - accuracy: 0.1619 - val_loss: 5.4601 - val_accuracy: 0.1468\n",
      "Epoch 7/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.9127 - accuracy: 0.1935\n",
      "Epoch 7: val_loss improved from 5.46014 to 5.23247, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.9127 - accuracy: 0.1935 - val_loss: 5.2325 - val_accuracy: 0.1870\n",
      "Epoch 8/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.6383 - accuracy: 0.2181\n",
      "Epoch 8: val_loss improved from 5.23247 to 4.89236, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.6383 - accuracy: 0.2181 - val_loss: 4.8924 - val_accuracy: 0.2259\n",
      "Epoch 9/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.3870 - accuracy: 0.2430\n",
      "Epoch 9: val_loss improved from 4.89236 to 4.87721, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.3870 - accuracy: 0.2430 - val_loss: 4.8772 - val_accuracy: 0.2162\n",
      "Epoch 10/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.1411 - accuracy: 0.2638\n",
      "Epoch 10: val_loss improved from 4.87721 to 4.50193, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.1411 - accuracy: 0.2638 - val_loss: 4.5019 - val_accuracy: 0.2669\n",
      "Epoch 11/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.9498 - accuracy: 0.2839\n",
      "Epoch 11: val_loss improved from 4.50193 to 4.25786, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 3.9498 - accuracy: 0.2839 - val_loss: 4.2579 - val_accuracy: 0.2847\n",
      "Epoch 12/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.7130 - accuracy: 0.3174\n",
      "Epoch 12: val_loss improved from 4.25786 to 3.92056, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 3.7130 - accuracy: 0.3174 - val_loss: 3.9206 - val_accuracy: 0.3351\n",
      "Epoch 13/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.5086 - accuracy: 0.3365\n",
      "Epoch 13: val_loss did not improve from 3.92056\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 3.5086 - accuracy: 0.3365 - val_loss: 4.1547 - val_accuracy: 0.2977\n",
      "Epoch 14/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.3213 - accuracy: 0.3567\n",
      "Epoch 14: val_loss improved from 3.92056 to 3.57545, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 3.3213 - accuracy: 0.3567 - val_loss: 3.5755 - val_accuracy: 0.3782\n",
      "Epoch 15/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.1490 - accuracy: 0.3792\n",
      "Epoch 15: val_loss did not improve from 3.57545\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 3.1490 - accuracy: 0.3792 - val_loss: 3.7577 - val_accuracy: 0.3633\n",
      "Epoch 16/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.9660 - accuracy: 0.4037\n",
      "Epoch 16: val_loss did not improve from 3.57545\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.9660 - accuracy: 0.4037 - val_loss: 3.7870 - val_accuracy: 0.3565\n",
      "Epoch 17/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.8032 - accuracy: 0.4250\n",
      "Epoch 17: val_loss improved from 3.57545 to 3.24519, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 2.8032 - accuracy: 0.4250 - val_loss: 3.2452 - val_accuracy: 0.4437\n",
      "Epoch 18/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.6625 - accuracy: 0.4440\n",
      "Epoch 18: val_loss improved from 3.24519 to 2.77657, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 2.6625 - accuracy: 0.4440 - val_loss: 2.7766 - val_accuracy: 0.5242\n",
      "Epoch 19/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.5084 - accuracy: 0.4711\n",
      "Epoch 19: val_loss improved from 2.77657 to 2.58006, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.5084 - accuracy: 0.4711 - val_loss: 2.5801 - val_accuracy: 0.5581\n",
      "Epoch 20/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.3802 - accuracy: 0.4908\n",
      "Epoch 20: val_loss improved from 2.58006 to 2.51856, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.3802 - accuracy: 0.4908 - val_loss: 2.5186 - val_accuracy: 0.5787\n",
      "Epoch 21/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.2477 - accuracy: 0.5138\n",
      "Epoch 21: val_loss improved from 2.51856 to 2.51577, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.2477 - accuracy: 0.5138 - val_loss: 2.5158 - val_accuracy: 0.5819\n",
      "Epoch 22/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.1251 - accuracy: 0.5295\n",
      "Epoch 22: val_loss improved from 2.51577 to 2.33377, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.1251 - accuracy: 0.5295 - val_loss: 2.3338 - val_accuracy: 0.6119\n",
      "Epoch 23/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.0302 - accuracy: 0.5462\n",
      "Epoch 23: val_loss improved from 2.33377 to 2.16051, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 2.0302 - accuracy: 0.5462 - val_loss: 2.1605 - val_accuracy: 0.6574\n",
      "Epoch 24/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.9309 - accuracy: 0.5656\n",
      "Epoch 24: val_loss improved from 2.16051 to 1.98830, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.9309 - accuracy: 0.5656 - val_loss: 1.9883 - val_accuracy: 0.6939\n",
      "Epoch 25/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.8184 - accuracy: 0.5823\n",
      "Epoch 25: val_loss did not improve from 1.98830\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.8184 - accuracy: 0.5823 - val_loss: 2.1053 - val_accuracy: 0.6597\n",
      "Epoch 26/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.7530 - accuracy: 0.5912\n",
      "Epoch 26: val_loss improved from 1.98830 to 1.84956, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.7530 - accuracy: 0.5912 - val_loss: 1.8496 - val_accuracy: 0.7229\n",
      "Epoch 27/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.6135 - accuracy: 0.6172\n",
      "Epoch 27: val_loss improved from 1.84956 to 1.84864, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.6135 - accuracy: 0.6172 - val_loss: 1.8486 - val_accuracy: 0.7239\n",
      "Epoch 28/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.5818 - accuracy: 0.6286\n",
      "Epoch 28: val_loss improved from 1.84864 to 1.78297, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.5818 - accuracy: 0.6286 - val_loss: 1.7830 - val_accuracy: 0.7362\n",
      "Epoch 29/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.5219 - accuracy: 0.6330\n",
      "Epoch 29: val_loss improved from 1.78297 to 1.65374, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.5219 - accuracy: 0.6330 - val_loss: 1.6537 - val_accuracy: 0.7728\n",
      "Epoch 30/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.4349 - accuracy: 0.6563\n",
      "Epoch 30: val_loss improved from 1.65374 to 1.63356, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.4349 - accuracy: 0.6563 - val_loss: 1.6336 - val_accuracy: 0.7801\n",
      "Epoch 31/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3975 - accuracy: 0.6604\n",
      "Epoch 31: val_loss did not improve from 1.63356\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.3975 - accuracy: 0.6604 - val_loss: 1.7270 - val_accuracy: 0.7490\n",
      "Epoch 32/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3006 - accuracy: 0.6758\n",
      "Epoch 32: val_loss improved from 1.63356 to 1.59470, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.3006 - accuracy: 0.6758 - val_loss: 1.5947 - val_accuracy: 0.7890\n",
      "Epoch 33/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.2997 - accuracy: 0.6840\n",
      "Epoch 33: val_loss improved from 1.59470 to 1.48568, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.2997 - accuracy: 0.6840 - val_loss: 1.4857 - val_accuracy: 0.8070\n",
      "Epoch 34/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.2088 - accuracy: 0.7039\n",
      "Epoch 34: val_loss improved from 1.48568 to 1.36882, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.2088 - accuracy: 0.7039 - val_loss: 1.3688 - val_accuracy: 0.8245\n",
      "Epoch 35/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1789 - accuracy: 0.7078\n",
      "Epoch 35: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.1789 - accuracy: 0.7078 - val_loss: 1.5068 - val_accuracy: 0.8096\n",
      "Epoch 36/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1282 - accuracy: 0.7143\n",
      "Epoch 36: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.1282 - accuracy: 0.7143 - val_loss: 1.5909 - val_accuracy: 0.7788\n",
      "Epoch 37/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0961 - accuracy: 0.7241\n",
      "Epoch 37: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.0961 - accuracy: 0.7241 - val_loss: 1.6450 - val_accuracy: 0.7764\n",
      "Epoch 38/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0790 - accuracy: 0.7309\n",
      "Epoch 38: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.0790 - accuracy: 0.7309 - val_loss: 1.4289 - val_accuracy: 0.8180\n",
      "Epoch 39/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0183 - accuracy: 0.7392\n",
      "Epoch 39: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.0183 - accuracy: 0.7392 - val_loss: 1.4383 - val_accuracy: 0.8308\n",
      "Epoch 40/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9792 - accuracy: 0.7497\n",
      "Epoch 40: val_loss improved from 1.36882 to 1.35827, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.9792 - accuracy: 0.7497 - val_loss: 1.3583 - val_accuracy: 0.8282\n",
      "Epoch 41/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9917 - accuracy: 0.7509\n",
      "Epoch 41: val_loss improved from 1.35827 to 1.32865, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.9917 - accuracy: 0.7509 - val_loss: 1.3287 - val_accuracy: 0.8438\n",
      "Epoch 42/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9357 - accuracy: 0.7547\n",
      "Epoch 42: val_loss did not improve from 1.32865\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.9357 - accuracy: 0.7547 - val_loss: 1.3427 - val_accuracy: 0.8410\n",
      "Epoch 43/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9421 - accuracy: 0.7566\n",
      "Epoch 43: val_loss did not improve from 1.32865\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.9421 - accuracy: 0.7566 - val_loss: 1.4990 - val_accuracy: 0.8172\n",
      "Epoch 44/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.7674\n",
      "Epoch 44: val_loss did not improve from 1.32865\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.9012 - accuracy: 0.7674 - val_loss: 1.3993 - val_accuracy: 0.8349\n",
      "Epoch 45/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8490 - accuracy: 0.7797\n",
      "Epoch 45: val_loss improved from 1.32865 to 1.29788, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.8490 - accuracy: 0.7797 - val_loss: 1.2979 - val_accuracy: 0.8467\n",
      "Epoch 46/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8598 - accuracy: 0.7781\n",
      "Epoch 46: val_loss improved from 1.29788 to 1.21988, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.8598 - accuracy: 0.7781 - val_loss: 1.2199 - val_accuracy: 0.8540\n",
      "Epoch 47/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.7865\n",
      "Epoch 47: val_loss did not improve from 1.21988\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.8167 - accuracy: 0.7865 - val_loss: 1.3235 - val_accuracy: 0.8446\n",
      "Epoch 48/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.7957\n",
      "Epoch 48: val_loss improved from 1.21988 to 1.14264, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7750 - accuracy: 0.7957 - val_loss: 1.1426 - val_accuracy: 0.8736\n",
      "Epoch 49/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8174 - accuracy: 0.7876\n",
      "Epoch 49: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.8174 - accuracy: 0.7876 - val_loss: 1.3087 - val_accuracy: 0.8425\n",
      "Epoch 50/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.7961\n",
      "Epoch 50: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.7960 - accuracy: 0.7961 - val_loss: 1.6101 - val_accuracy: 0.7999\n",
      "Epoch 51/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7575 - accuracy: 0.8005\n",
      "Epoch 51: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7575 - accuracy: 0.8005 - val_loss: 1.1651 - val_accuracy: 0.8770\n",
      "Epoch 52/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7578 - accuracy: 0.8014\n",
      "Epoch 52: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7578 - accuracy: 0.8014 - val_loss: 1.2461 - val_accuracy: 0.8642\n",
      "Epoch 53/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7231 - accuracy: 0.8073\n",
      "Epoch 53: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7231 - accuracy: 0.8073 - val_loss: 1.2045 - val_accuracy: 0.8645\n",
      "Epoch 54/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6670 - accuracy: 0.8242\n",
      "Epoch 54: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6670 - accuracy: 0.8242 - val_loss: 1.3429 - val_accuracy: 0.8480\n",
      "Epoch 55/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7287 - accuracy: 0.8077\n",
      "Epoch 55: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.7287 - accuracy: 0.8077 - val_loss: 1.3437 - val_accuracy: 0.8558\n",
      "Epoch 56/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.8238\n",
      "Epoch 56: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6741 - accuracy: 0.8238 - val_loss: 1.2437 - val_accuracy: 0.8692\n",
      "Epoch 57/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.8283\n",
      "Epoch 57: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.6406 - accuracy: 0.8283 - val_loss: 1.2179 - val_accuracy: 0.8681\n",
      "Epoch 58/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.8180\n",
      "Epoch 58: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6642 - accuracy: 0.8180 - val_loss: 1.2797 - val_accuracy: 0.8676\n",
      "Epoch 59/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6443 - accuracy: 0.8291\n",
      "Epoch 59: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6443 - accuracy: 0.8291 - val_loss: 1.8228 - val_accuracy: 0.7566\n",
      "Epoch 60/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.8308\n",
      "Epoch 60: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6484 - accuracy: 0.8308 - val_loss: 1.2013 - val_accuracy: 0.8663\n",
      "Epoch 61/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.8320\n",
      "Epoch 61: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6326 - accuracy: 0.8320 - val_loss: 1.2300 - val_accuracy: 0.8699\n",
      "Epoch 62/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.8272\n",
      "Epoch 62: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6424 - accuracy: 0.8272 - val_loss: 1.2851 - val_accuracy: 0.8598\n",
      "Epoch 63/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.8287\n",
      "Epoch 63: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6300 - accuracy: 0.8287 - val_loss: 1.2419 - val_accuracy: 0.8600\n",
      "Epoch 64/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.8347\n",
      "Epoch 64: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.6125 - accuracy: 0.8347 - val_loss: 1.1897 - val_accuracy: 0.8733\n",
      "Epoch 65/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.8416\n",
      "Epoch 65: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.6009 - accuracy: 0.8416 - val_loss: 1.1755 - val_accuracy: 0.8786\n",
      "Epoch 66/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.8396\n",
      "Epoch 66: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.5905 - accuracy: 0.8396 - val_loss: 1.3911 - val_accuracy: 0.8423\n",
      "Epoch 67/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.8397\n",
      "Epoch 67: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.5796 - accuracy: 0.8397 - val_loss: 1.1707 - val_accuracy: 0.8759\n",
      "Epoch 68/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.8547\n",
      "Epoch 68: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.5414 - accuracy: 0.8547 - val_loss: 1.4339 - val_accuracy: 0.8336\n",
      "Epoch 69/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8440\n",
      "Epoch 69: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.5619 - accuracy: 0.8440 - val_loss: 1.3554 - val_accuracy: 0.8694\n",
      "Epoch 70/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.8412\n",
      "Epoch 70: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.5862 - accuracy: 0.8412 - val_loss: 1.2654 - val_accuracy: 0.8611\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://684b2b9c-21bf-4ed5-b39c-61430eb706fa/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://684b2b9c-21bf-4ed5-b39c-61430eb706fa/assets\n",
      "2023/12/05 10:46:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmpgb04_ljx\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"epochs_10_warm_70_train\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")\n",
    "\n",
    "\n",
    "experimento2(10,70)\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mejor opción encontrada es tomar el modelo base, hacer calentamiento y entrenar durante un número extenso de épocas."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABcoAAAFBCAYAAABHMpxFAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAJKYSURBVHhe7d0JfBTl/cfxL3e4SbhJuMIZrKCgoGJQEBBQVFSoR0VE8eCogsUi1LbWgvylglagXkXUaluookQ5BEGJqICoRCHhCGfCKeEm4f7PMzu7md1s7oRrP+/Xa5LZmdnZe3bmu8/8nhJnLAIAAAAAAAAAIESVdP4DAAAAAAAAABCSCMoBAAAAAAAAACGNoBwAAAAAAAAAENIIygEAAAAAAAAAIY2gHAAAAAAAAAAQ0gjKAQAAAAAAAAAhjaAcAAAAAAAAABDSCMoBAAAAAAAAACGNoBwAAAAAAAAAENIIygEAAAAAAAAAIY2gHAAAAAAAAAAQ0kqcsTjjAELEFxv26p/fpujrzft16NhJZyrOlcrlSuuaRtX04FVRur5pdWcqAAAAAAAAzhaCciDETPlqq56dv965hPPNn25spiHXNnAuAQAAAAAA4GwgKAdCiGlJ3u/tH51LOF/NuP8yWpYDAAAAAACcRdQoB0KIKbeC8x+vEwAAAAAAwNlFUA6EEFOTHOc/XicAAAAAAICzi6AcCCF03Hlh4HUCAAAAAAA4uwjKgbPilI7u36+jp5yLAAAAAAAAAM4b5yYo/2iIGrVsrUaPxDkTshOngWa5lkM0y5lycVqpvz35V/V/8j195UxB8dny0SvWc/1XPflRqjOl+B35+j0NfW6yhvx9iX5xpiF//vzYDdr9+9b6tXP5wlZX039/g36+u65zGQAAAAAAAOdSvoPyWY+Y4Dqn4WIPtQvg1H6tXfSBnvnrBA20A3FrGPl/euyv72txmrPMWXFUm76w7sezr2nG+dhXYMo8Pel9fgKGvy1zljnvBX+Oy1WuqLCSJVW+WhWVd6adT359d6x2P3dD1uGxZs4SCI7AGwAAAAAA4GKQ76C8/hVd1KOrM7Sq4ZlYIyZzWtd2qu+ZCuPQT5oydqrGfpqoLftOqkylCqpshgplVDL9gHYcdZY7K/bpm6XW/Th4XOd1BeSy4Yq5pKnauoYWzlvt/Bf8OS596R2aOmG0pj5wmSo6084/hzT1mc9Vyz38Y70z79z78z+s+/N/CfqvcxkAAAAAAAAoKvkOyq8Y9JJenewM/WM8Ey/5Tea0yQN1hWcqTm3V+1PitOzAaVWIulpP//UpvfbsCE3xDmMf0z1RzrLIVKmJ7hl4l55wDb2bOPMAAAAAAAAAoIiVOGNxxvPP1BofFS9dN1abX+vtTJT2Lv2Hnhz7vpZtOaD0U1Kp8jV1xd3PaepT16i6WcB1vZX912vwU+9p2d4T1nINdOOYyXr5zkYqY6/J1Cgfo0WK1aSkKepjT7Ps/VovjxqnV7/eaq2/lMrXaq5ev/0/jfdd7/xwJH66hn6UItXtqPG/66zazvSsTI3yuUpQYz3859ba8MZCLUk9opOlK6r51b007LYWqmovd0g/zflY//omVbuPntAplVTZKnXVpd+duiemsr2Eqb/9TPwB1YztryHlF+hvC3eoUqeH9EKtz9V/xiZ7mUxV1WP4sPMnrDelVyZ9pz0RV+i5MT3U0JnssU8f/22KPthRRlfe9zsNu6yUM136/u3/00sJp9Titic05prD+uLfH+njxL3am2F6zjTvj2jdPeBOXV/bc53M5+gBvXhbpLTsPfu58V22per9sW9pXpr7Ocrl+XfW48+5fqozr0VPvfNwO2feKe345iNN/XyjUvcd00l7fTV0Rffb9NDVtVTaXsZ1P357qyLmxel/G/bpuPVOr92qi0b0v1J1M5+KXJlW4sGY0iuvtDqmqc8s15+daZmaacFzDdRm+1a/FuamZvjgeqYVunWdG9tr97Wyxrep0e9bqVclZ6HDezQsSytwU64kh2W86/owQ3feXlO1rEc758N4bb7Kuj257oPvNvepo7l/nqla9dXn6jbfuc/OtN1r1uhX/97hXHLY1/d8bgzP9ZwLzn28YusaPaem1nNT1pnuuS8DfrBGA65v8z1H/rdvZLf+LPfLYUrfAAAAAAAA4Owols48l7zzulaVvkR3PvYnjR/aW83KpWnZtCEa8u+DzhKO5NfVd+hnOnN5rHp0bKYqx7dqzh/u0ZBPA5Zz2xungT0e1aRvT6rD/b/T+N/fp3Yl1umD3K531h3VVz+k6JTKqG3XTjmE5G5pmvX3T5RQuq5aN6uuCiePaF38R3rnRxP4Gus0f/EulYqM0S23d1efyyNU9nCq5k37QAsDS7ikLNJLn+3Q4dPSqVPW9WtEqe0l9VXbzvvKqHYTU9KksRpVsJe+AISr46U1rf8nlLAqyTPJtlrfJJ2w3sn11KGd9WB2fK+4n44pIuZy3XX7derRvJJO7F6vaf+YYz17hZXL85+v5/iUNn70qkb/L1FbDoWpsSkx06yGyh/dra//90/9X/whZzmvE1r5r/9q9sEq+lVMpGqUPKFdP8/X3+efja5B16vbh3u0u14DLbjRmXR5a91Zz4TG7mC9nO78fVPpM2/pljWao5p6xa8DThMgt1Kvg1t95V2mHgxcxrDW1T1M/7OXcYLpoCpr8HPhWupd13apzbWx+vn3dZT6oXM/vjqkWq2aavrlzlUsdk32a8tZ999Zxnp8da+9IfPxOWq1aqVntMGzjHk8h8uqV3fnvs5f7kzzBPH2Mk6I/+u760hfOet27oO5X+77AAAAAAAAgPNHsQTlnZ6dp+WzX9VzQ+/QXUPHKu65LqqgU1oR/7WzhCPlpDq/G6cZpmTLPz/QwmevtpY7rM/enKHtziKBlrzwvBYditSjM+Zp+lP9ddcDI/Svdx9Wy1yud/bt014766ynSy/Na5PfA1LMvXrht3fpiUcf1pC2JmE9oaQkbyvl5ho4+gmNe/QW9enYXn1+M0gPtCojnd6unxKdRRx7Nu1Rgz6P6c0X/+BpJd3kOj0xsKsut1vxVtDlt5iSJjfrmgh78fNL2nd6xt2Z59h52mJNrhH7K5muJY8lb9Aae0HLOmv8uFSuVTtdb56uiKs05rlheuY3PdSrY6zueeRu3WR+pTi0VT8Gb7ibD7k8//l5jtOW6u2l+3SqbEMN/IN1f02JGes1nzS4rWpan5W1c+bpe2dRj6M60aSP/j7yPmudD+iFu5upnDU1dc1PKrqo3ITO/p15+oLjHxL0PxNCX2ZC4rqa3r2mtGZDQIBdVjt+dIfaOzTgsz3aXammHnLWYwLkNqYWurtl+j+2apVrGQ+zrmCt27Na9VXmcn/+1ro967ra6rpv87fZAfcVLb0dbjbTQ63Karf7/luP7znrjeR5fC6H9+g5X4tv6/H8aH2oK1VVz1wC7//+O97VetyS5T4AAAAAAADgfFIsQXn1agf15eSxevTBX+vaa2LV+okFMg1uT5064VnAq+1v9NtfZRZLqX7nzbrKjKxLCAgJvVZo3pLD1v9UvdqntRq1dIbu/5DdxvjQQRM1n2dKqnSeS2PUU/ceDZySG6V0aVNPO/RDB72PqrIqHU3SrBn/0f9N/LuGPPM3Tf3ZPKendSqwd86G1+ixa6ubyPDCE9iZZ4vang4wK1yu9qYey5GNWp5sJkhrvk/WIZVR68tbeZ63CpWlrcv0/r/+o+f+b6KGjHlDH+8yM07rpLdhfoHl4/nPxZGEDdp02lpjm6t1vat6R+mGV6uTyVKP71RSimeaRwW1advUeW9YyzWPUgMzcuyYjthTikLWzjzdYa8v0H6svnpVOqT/ZSkZclypu51Rrx/26DvrI1s3wjyouurZwHpHbt8XEIAfVqpvGa8g6wrqkJa6A+kf0mXu1Y60HH4VuTFcbaz1f5fkv8x/045JlcLk9Lxg2711j3/ZmN0ZdhAfWcu5nCPTet77o4Or1AwAAAAAAADOO0UflB+cp4dj79BDkz/U6oy66nzPE3ph9E3yVn72U7mKU3s7wKlTCojUHTu1c5/531S//sufND5w+F0P1bOXOx+UcgLyXVqb55of5VQxh1IoR1b+W7+dOEuzVu7R8WqN1LVHd91zWRVnrr+aDRp5wuULUWBnnnderhr2jAq69vIo65k9op9+SrUub9LypKNS2Wh1dGqWb4l7VU/+Y4EWrNuvspGtdFOvW9Qr2p5VaPl5/nPzy35PvB0WVt7+nylc1e1VHtB28xB9yqi8+71RIUyB1yx+69XNlBCpV9mvFXemY9qcbYkUl3oN/FqtBw+R87iuAiurXre774M1BNYbLwRTv333cw1c5Vc8JVoAAAAAAABwfiryoHz7v6fps0NSg4HvKf69l+zyK72bhSndme/n5An/QPzgQdlZUngtBS9QUEd1ws3/0moYe4fu6hcw9GgVPHg/J+rosiYmqj6qbxetKIKW7vu0cHGytbaq6jHMlOow5T8uV2SZfDZlvsBVbNdSTa137Z51SdqVslY/We+1am2vUFt77mp98vU+nSrZWA/+6VH93i6/0ljheWxJnpER9F3qKNrnv0Y1z88YWW9zn/bapfarql7QX5fOpWZacG1l7T4cpERJtiopspJ/C29fPe+AIbtOLYtH1tbzniFv5V5y5Kvf7t8iHwAAAAAAAOevIg/KDxzwdEJYoYK3+esJffvJl0pzLvn59mO9v9cZt5ZLmPIvLbfGKrTvqHaeiQEu05WtTcvhJE3/xwq/kH3v0vf0yQbnwnmieY/2amY9w8fWL9Af3/5eu487M4xT6Uqc84E+8SuvkZOjOnLM/C+jcqY4tXFqg74xLapDSYXW6tDQelJ3bdCsL5K1RxXVtm1jZ2aG0s1zXLq0Xb/bdugHLd/mjGenQll7+UNbt8iu0mIc2qzk/c64rWif/4qXNLTPsji06ht94fnI2E5u+UZLTF5cuZGuiPJMO1/8+bEGamNqdv/fTrsEyzN3B/6cVVkdAzrD9JY58ZRR2aG5W4+rVoOaeQzZi8n8fVoV7L4Wp8tr6gpKrwAAAAAAAJy3ijwob3p5K5mIPOm1hzXghX9o/KDeemCRFLTPyArr9bfre6jf0Cf06L29dcfbqVKphhr4aFdlVi53K6M+Tzyg6FLS7hkPq/0twzX+rX/omXt76KoH/yW7XPT5pHJH/fbeFgoveVr7Eubod2P+T4/9aaKGWMMjo1/U859vl914OE/qqFk986z8ok9f+5fenztPE57/QKvsZzuvwlWrqnnJD2jJe+/qpVf/k4+g/iw6nKz3p/1HL7mGt7+za+5YKuj6KxuqlPZq2SprWkSMOjdxZqmhGpuy7seT9dbEDzRr7gcaPeE77cmtBk2rVroszPq/a5menWA9L9Pe1ajxX2mrtyC4La/Pfx6f45rXa1BsuEod36Jpf31Fz5nH+errGj71e+1RObXueb2aO4ueF25sr8GmlfRnCfqvU4KlVqummh7QqWWba9u7WmR7WqBr+05fp5n//bcnZH/lMdMtq1ddTf+9+3rFbb3eNB13Xhvrf/+tx/hzlvA/Nzu02foQ16rmSsHtOunujjs9nZ/mqaw5AAAAAAAAzokiD8rLdBuj1+6JUSVt1xfT3tQH6bfqvVduti4H0fZ3em9oHW36cpHmrdyl0g07asy/Zup37t70AsX8Vp/Oeka9osvr8LrP9er//UP/WVtOHYeO1v1NnWXOI1Uv66sXR9+hW5tXV8XSp3Tk8FEdOpyhU2Wr69LYTuqc51yulK688zZ1iSwn7duseV8k6ljbfhp2WfCfFIKroOt7X63mlUvp6O4t+n7TAZ3Kc0ejZ9HxfUpcvUHfu4aElMyW26WvaKlLSp7WqdNS5OVXyPTv6VFDt97fWZdWKaX0nYmavWSnGtx6v27PrXB9qUs06EHzvJTQ4Z1btGr9QTW6/RZd5/emzevzn9fnuJSibxukv/Rpprplj2qjeZzr9+p4RDPdP+Jx/a5D0dXLzrvKGuyu2W2G37fWry9vrZ8DAm/N36Y5h8uqV3d3CZZDmvphhu70Xb+B6poyK/9Y78w31qvbM1u1yq9OeStdsXXbWQzKTWAfr2Fr5F+n/FoF6aA0d3/+do92ex+P/QOA94eEVr7HF/kjNcoBAAAAAADOZyXOWJxxABc5U4e7WNzY3g6apxZFjW/YTMgOAAAAAACAs6PIW5QDOH9VLudXTwbnKV4nAAAAAACAs4ugHAgh1zSq5ozhfMbrBAAAAAAAcHYRlAMh5MGropwxnM94nQAAAAAAAM4uapQDIWbKV1v17Hx3B5s4n/zpxmYacm0D5xIAAAAAAADOBoJyIAR9sWGv/vltir7evF+Hjp10puJcMTXJTbkV05L8+qbVnakAAAAAAAA4WwjKAQAAAAAAAAAhjRrlAAAAAAAAAICQRlAOAAAAAAAAAAhpBOUAAAAAAAAAgJCW7xrlTZ781Bk7PyW/eJMzBgAAAAAAAABA7mhRDgAAAAAAAAAIaQTlAAAAAAAAAICQlu/SK9+v3+6MAQAAAAAAAABw7rRtVs8ZK5x8B+WHjmY4YwAAAAAAAAAAnDuVK4Q5Y4VD6RUAAAAAAAAAQEgjKAcAAAAAAAAAhDSCcgAAAAAAAABASCMoBwAAAAAAAACENIJyAAAAAAAAAEBIIygHAAAAAAAAAIQ0gnIAAAAAAAAAQEgjKAcAAAAAAAAAhDSCcgAAAAAAAABASCMoBwAAAAAAAACENIJyAAAAAAAAAEBIIygHAAAAAAAAAIQ0gnIAAAAAAAAAQEgjKAcAAAAAAAAAhDSCcgAAAAAAAOAicvTH/+l3z0zUH+K2OFMA5IagHAAAAAAAACgKK2dqyDN/s4fh76/RSWeyn1Nr9OZznmWGTFyobc7kolVSJUuWUFhYGecygNwQlAMAAAAAAABF7PjaH7U03bngcnT5j1p13LmQL1s0//V/6qnJi3MN1ytcdrteeHa4/tCtnjMFQG4IygEAAAAAAIAiFBFeRSVPb9d33wcm5fu15PvtOm3Nr+5Mybv92rBtn44cP+VcBlCUSpyxOON5cuhohjMGAAAAAAAAwMeUXvloi6p3uFINflihH6pdqT8Nu061nNn65Uv99eUV0nVXKvzLFVoTfplGjeiq+mbeqQP6ef4nev+7XTpw4rRKlwvXJTfeooFX1lRpZ71+7OvW1OxnFljraame9Xfq85/363h0N0351TrP/bj6Hv2ll7dV+Qmlfhunfy7apj3pJ3S6ZBlVbNBBox68SmXWfam3567S+l+O66RKKazGr/TY493U1LkmcD6rXCHMGSscWpQDAAAAAAAARalkM3VuXUHavVqLNjvTLMlLV2tHyXqK7VjNmeJ1WMv+9bb+sWyfwttcrbt7d9TV1Y9o1ex/680f06VGba1pLWVH3hXqq0fvzrq7a0xmq/R9SfriaFv98Y+/05T72zgT3dK14v3XNP7Tjdpbroauuc66fmy0Ik4f05Ff4vXKeyu09ngtXdfdmt79UjU8fkB7nWsCoYKgHAAAAAAAAChiTTpeoro6qmVfO516nlqjxQlHVbbFZepY3l4k0+av9PGG46obe7dG3nq1rm1/te4a0EHROq6flifoaPUm1rT6suP1ctV1Wft2urZ1pCrYVzbq6qZ+bRVeyrkYaPOXmpWYIVW/TMOfuFd3d7Wu37W3Rg26TvVTdmnHaalCw7a6OdaaHttVvx3RR+2cqwKhgqAcAAAAAAAAKGo12iu2YUlfp56eTjwrqMM1rVTaWcRrx9oUHTD/v3xLQ575m2cYF6+NZubhQ7m37g6vraaB4btL2sad9vobXdlRjQLD9Evb6poqJXX4p9ka+dd/avKcBKWeKpXlPgIXO4JyAAAAAAAAoMiVV8cOjVXW7tQzRYu+267TtS5Rl0bObJeTp07b/5t1f1B/GREwPNhRde25BXck47j9v0zZsvZ/P6WidfeIx/T7PpepWcXDWvvNZxo3YaaWHXbmAyGCoBwAAAAAAAAoBqUvbasOlaSNX3yoL3aXVNMr2md27OlSv1EtmQh7y4ZNKhseruruoWr5Qrfu9q5//Q/f6ZBnUqbjJ3S8VHk1aNtVQ4cP1ehrI6SMLfp8+X5nASA0EJQDAAAAAAAAxaKhurQzwfNxpZeso7aXZVMfpVV7XV+9pI5v/ELPvRKnT5ev1FfxC/X61PcUt91ZRpVVzRQl37dG73/8jWZ9/LW2eWbkrlWsbqxTUqe3xesPL87Qh/ErtXDOhxr/xpfa9tPH+oP3Npcv1ccJJiAvq8h6gR2OAhc3gnIAAAAAAACgmNTqeKmallTwTjx96unWwXfotiZVdSptrebELda/F63RNtVQw3BnEUXrlptbqGaZk9r63TdasrukKjpzclddPR69T/e0rqmKR1L0+WeL9fF323U8oraqR9RQRPpGzTO3Gfed1p6prva33K17WzpXBUJEiTMWZzxPDh3NcMYAAAAAAAAAADh3KlcIc8YKhxblAAAAAAAAAICQRlAOAAAAAAAAAAhpBOUAAAAAAAAAgJBGUA4AAAAAAAAACGkE5QAAAAAAAACAkEZQDgAAAAAAAAAIaQTlAAAAAAAAAICQRlAOAAAAAAAAAAhpBOUAAAAAAAAAgJBGUA4AAAAAAAAACGkE5QAAAAAAAACAkEZQDgAAAAAAAAAIaQTlAAAAAAAAAICQRlAOAAAAAAAAAAhpJc5YnPE8Sd170BkDAAAAAAAAAODciaxexRkrHFqUAwAAAAAAAABCWr5blB86muGMAQAAAAAAABePwydKOGMIZZXK5CsuxTlWuUKYM1Y4tCgHAAAAAAAAAIQ0gnIAAAAAAAAAQEgjKAcAAAAAAAAAhDSCcgAAAAAAAABASCMoBwAAAAAAAACENIJyAAAAAAAAAEBIIygHAAAAAAAAAIQ0gnIAAAAAAAAAQEgrccbijOfJoaMZzlhuTuvYoSPaefiEjp92JhVAqdJlFB5RUeFlyPQBAAAAAABQfA6fKOGMIZRVKpOvuBTnWOUKYc5Y4RRbUH780EFtO3hSZ0qVVqXypVWmANuZ06dO6kjGSZ04U1rVa1VRRGlnBgAAAAAAAFDECMphEJRfWM7zoDxdqdvTlV4mTA1qVlBZZ2qBnDyqbbszdKxseTWtUd6ZCAAAAAAAABQtgvJz57tVP2vfgYPOpbxp1/oSRVSr6lwqOgTlF5bzOyjPOKxNe4+rdJUI1a/sTCuEtD1p2nuqrOrVqaSKzrSzLT0jQ+s3pShl526t27TNnhZVt6ai6tRWs0ZRqh5e9B9KAAAAAAAAnD0E5efOa9P/rY1bU5xLefNw/7vUpFF951LRISi/sBCUn0VzFn+tRUtXKv3YMWdKVlddfol6db6GwBwAAAAAAOACRVB+7tgtyvfns0V5G1qUg6D8rNi774Bef/9juxV5WLly6nxNOzVvVF/169VU+bAwrdu4TWkHDuiTRV/bH+Ty1jL33dFDbWKaOWsAAAAopJUzNeSjLap+9T36S696zkQAOF9t18cv/UefHainu5+6S9dSPRM4q7bNeV3jvzmoVrf9TkPaORNzwn5GFgTlMAjKLywE5cXMlFp55m9v2K3IO1x2ifre1NkOx41tO3YrI+OYmjXOPLVj8dff6X9zv7DHH77nVsLyAlr29t/0zgbngq2kylaurvY33qa+baqq2PtzdXYSpCq6/uEH1bd+Kc90R753OgAA59TJA5v1xWdf6su1aUo7dsqeVrpcZbXo/msNbl/Nvnze4wAWhbJdH058X5/vcy7aSimsYhU1bn2l+nRtrchCdSiEcyHoPnP5iqrdrLXu7N5eTav678OeXdsV9/f/6rMjDdX/idt1ZYgG5cd3fKd/z1qphD2HlHHSTLE+d1VbauDveuoSewlc/Nzb3wj1fHygbq5hz/Cze+E0Pftlmj1eFMeZBOWFR1AOg6D8wlJUQXlJ5z8CvPb+x3ZIft/tPdT/jp52SG4C8uenvKPxU9/RS9P+q9/99RWtSlxvL9/5mis0anB/u+X5ux/Ms4N2FFQZ1WzUWK1bWkN0uMoc2aOv/ve23vwx3Zl/NhzUFzM/12ZPpgIAuAAd+ukj/Wni/zQrYY+1Va+iaPO90rK+6oad0P59R52lismmpXrx5ckaN3+7MwE41yqovv0ZMPtXEapw8qASv/lM456fpo+35WOH51y8t/k8ZcO1z9yytsKVodSEpZo08R96deVhZ5nitkXzX/+nnpq8WJ5enIx66v3b4Xrl6dANyU9uW6jnX/1Cy3dlqHJd72tUTzVPH1Mxf/vgvJWmr5eaBlmBtmjRSk9IDkDavnO3Nm7ZluOQtv+As7RH8mbPdKAoEJQH8e0PP2v9pm12S/KrLv+VM1X614fz7DIs11/dTj07XyPz25IJxU2JFqN+3Vp2y3MTsM/8dLE9DQVRXq173qFH7rWGBx7QMzfUtqYd10+r1nlmnwWVKlVQyX0JmjY3RXYDEADAheWXeL3yvw3ar2q65p5BevEPD+pJ871y76816ndDNfrGYm4xlbZdG3/JUAY/uOK8UVOd7c+A2b+6X8/9wfocdKun8ifT9Nn787U2r+/Vc/He5vOUDdc+87336o+jh+ov97RSLWXop9kfa/FZycr3a8O2fTpynBfHbc3S1dp9uowuv3Oo/vyw9zWyvn+euk1XOssglFRR9XDpwJrVWutM8Vnzg5YdLmnNL4JT8YGLQNy8z/Xa2//JcVi5arWztMfr73imA0WB0itBmFbjJhD/y4hBvs45TWty05LchOR9e3W2p5nW5KaG+Z09r7dblHv94cXX7ZrlU577nTMFeeU5jbSKbnjsYd3uzTC85VCadtOU+9tI2xfqj//4UXu9l23e09oaqv9zfdVBqzTlmQVaE36Zht4szZ31k5IPn1LpSnXV85671COgpIqP77SzXrpm0zzF7azkV4Ily2ls6ala/MlCLUjcqwMnTkuly6p2s2v1yK/bqra5im99t6nPkS/0zs/7dVxlFNn2Jg27uaoS/vux/rfWM63mr7rpyb6t5P3ImNM1p8/4Wqt/Oa6TJcuoatQluu/uroqp5CwAAMhWwn9e1mvWPnSLWx7Ub6/MbsPp/e6or+s7n9TK+B06VPkyjRrRVfVPHdDP8z/R+9/tsrfvpcuF65Ibb9HAK2vaZcCOpizTzNnf68ddR3TcbP5d81dmKYmgzNOZc1mvmf9d3Ef636o9OnSylMLqxuihXx3S5AWcEo2CCtxHcjulH96frDcTpXZ3P66BraQ9Py/Ufz9fo/Vm/8MugVdbsbfdqtubVwpS7iPzvZ3T9czt7FoxR28t3qjUQyd02uzXNLlWv+vfThHW3JMH1mn2jM8Vn2I+T9b7vkZD3X73repYq1SOtxnKgu4zO7xlHCI7P6TRXarluH/tey69l9tcqSbbftR3aSfU0uzvtsphX/dHZx/dzdr3HjWilVZkec+dUOq38/TuEuc9YMqQ1IhU5569dLP9HrG49pvv1jK9vcLaJlvbwcoNrtCjA2PV6FxWk8mntR9O0d9/OKboHg/ryY7ZfQfl/pwU6rVrd0r7EhfrrU/XaMsB87n0L/2S0+cORSVz+9vzukOa++UhXd5vqB661PscO9vg5AbqefkezV3mXy7FU75nhd++RrOrrtP9XZv6jhl1fIsWvL9AczZ5jimrNr5Kd9RYpWl+6zLvtTj9c+EW7Tp2SiXLVFTjK7ppUC9nPYHvqVN7tPTDOfrU+dyb5WN63H3hlKwrAoUuvZKeqq/mLtCXa/fqoNl2liqrWk1jdd+dbeX7iJ3YqW8//ETzNx7QUft5DlPj6+7Tw9eY5/mU9q9drH/PW60U1+f37sd76ch7EzQjuYpiBz2im+t6VqUfZuj3cVsU0eFe/d40BvFevrS9Gqb8oFX7TqhZ75Ea2DL3+3XqwFrN/XCxVmw/ZP9AbUoWtuvTUyU+maFvD9dV75G/yex7In2FXn3xC22q3l4jH7tOQSoL5dvs+Yu0Y8cu51Jw7S6/VFe0yWzU+vu/TLD//98fR9r/iwqlVy4slF4pRr/sO2B3zFmhfDlnivX5Tz9m/y8fljmtfDnPi3Ak47j938u0LEdROaW1W/bYY1WrFeCL+cgavTVrp+pdc5VuaFxJpw/vUJy1I5HszM5eNfX49ZWKLJlLCZZ9iVq6tbQuveY63d27o2Ktl35X4iK9/In/QcOhH+bpw2PNdUdstCJKWjsq332qia98oI8PRemmrr9Sg7IntCdhnt78xlNexnu65uqTUereu7Pujo1UiZQfNfVN92mtAIDgUrQ25YRUtqGuzzYkd9umJQk19fDTv9MUE5LrsJb96239Y9k+hbe52t6+X139iFbN/revDNjehNXaULqRut5kbaO7/0r1Suyz5s/U/zZL0ddY0y6rbi9XKfoK6/qddWvrcOtSbus9pYSZb+utlXuUXqmBbuh+lTpW2KrXP2fLj+JSSpc2q2P9P6HNmz1lTTb+mKQDNVqpt9n/uK6BKhzZoc/fm60vrbdo9u/tnK93ck2cJsxeq1/CotXD3q+JVvkj+3TEXPHwKr02ebYW/1JZHbpa87rHqNbhjXr/jVlakcttIrhaLaLsHyBStwUr85CzvatW6fC1AzTpOSdgy2lft1Fba1pL2flthfqe17ZrjDyvltspbZ7ztsZ/ulY7StVV5+6d1efqSFXYv1Vz3/uPPgwo/WP2m9/eVl1du1ymFlXO6NDWZXo9YN/6fNei0+VqUPq0Ns57Q89M/1Kr0qzvJD/5e07yIvC127bgbf35/R+16URlXXJ1R+uzdakal3JKv+TyuUPRq9nxUjW1jgNXLfs+s/xO+vf6Yu0JVW3dVm0Ckhlf+Z49pdSkg3n9LlOz8geV+OVsTZzjPet5r+a98YE+Sj6oMjVaqNdNl6vFoRV6Z4X7dBLve22LTkS3s7ehPaJLaNM31nqClrMy+yL/1vsJB1Q+xrPN7RFTVod+oWhQvuxP1PJtpRVz1fW6vde1usradu5e+7len+tsy9J/0rsvvadZaw+qXO1W6tqri7q0qKxTGZ7nefui6Zrw3x+09XhltehwrW6/obUalDyu/Bb4TfvpRx295gH99Y8jNfBya0Ju92vHF3rZ2jbEp55QtaaX6yZrmasjSyk9o6Eub26are5QgqskbnrCOm05XVKN27YvkpDcuOXGLnpkwN05Du6QHChqBOVBdL6mnV0+5fOvv3emSM2j69v1x7/4eqUSEjfYpVk+mOspr9Impqn931i3cZs9v1mjKGcK8i9dCXM/0GvvWcPr/9TUH46qdM12evjmhs78fDhZXT2H/EZ3xV6t2wfeos52TrFT6/06tMpGjVg9dF0Nldz3s96en00JltqdNerJe62Dgna6tv3VumtAB0Vbkw+kbJW70tzx6u301G866dqut+vpruZn3xPafayRHnmkh7rG9tCTPRpaH8bT2rrFfEGla+n8BO0u3UQDnrhdN7W31t31TvVvU16n967RktxTfgAIcXu121RFq1hJeYvTyuuq3t0V7e3QcPNX+njDcdWNvVsjb73atX0/rp+WJ9gHuHVvvF/PPdzTs422tuPDrjfb9qPauHG/ajazpjXyBPTlaje3rt9O7aLK577e9O+0YPVxqVIrDX6in243310DHtSjbYqmdQQQTOmS/ocD7e5+TGPu7aqurv0Pnd6j5E3K/r1tyel6e7b9Yu3dSLUu7aQb7fm99czDnWU+NcmfL9WajAjdOMjsr1nzvJ+njM1a8n16jreJbJQqqQK3hazfQQ9c6epAP6d93epNrGn1ZTdlKVddl5nXtnWkKpjLbunfa9ay/Trt27a1U9de/fSn25uo7On9WrzoZ7/97JM1r9QYex+5q357b2s7eA/ctz7v1bhGTz5xi26IKqP9ySv0+qRX9NRrX2qjt31VPp+TPHG/dukr9L+v0nQyrKF+M+wBPdzL+s6xns+hwz2lX3L73KEYlG+r61uU0ektP2nRL55Ju5f+pA2nI3RNx8DjXOd48HQFXfObBzXU+/o9caMuL3tau5d9o5Xmt5TkZVq887RK1u+oZ4b1tvZJOun+YQN0W6RnLTbnvVY65kb96R7reNT6nN70m+66qoK1nu9/zFoKRr9o007zRg1X++4dPcv3fVBP3mjKoSLPanXW44//Rrd3bqcOV1ytPr+5SuZVPpS6VSaK2Px5vH62Pmo1OvTTyIE91e2Kdup2+wA91qWe/fmd/bX1+S3XUHc+NlD9b7xaHTp21YPDbtNl9srzIeoq3d2uqnznieR4v9L1zfyV2nWqrFrc9ICG/7qrOlnL3PybQbI2xWp09SUy74ItiQn2d7p5n65cvV2nyzZWxyv4XsbFg6A8iBuuaesLxU3JFa/+d/Sw65K/9v5HdmeepjyLqVXubUFuOvD0hue9One0/6MgTmjP5k1KSLKGbQdVLqanxv+2c8FOt6xaW019jQnrKdJu4nJcR/O4/1eri7XDUcfaiVk2T7ODtuw4rM1L5tqB/u/HT9bw8fHaaCYfO+ZpJeWo3qCh7/S4CmGeJKZSs+ZqYo9ZB6l1w+0w57h1PWmDEredti4k680//01DnvEMf//B3Ol07d9vrgEAyF45lTeb2mPW9t4zIRe11NS7QbbsWJsik7Pv+PIt3zZ4yDhn+374kPba/7dp4Ucf6MWXp+j3417W7+ftMFOV4bQECibX9a7dps3WaMQlrdXC951XSi0a1nTGgaJ31DkzMizME2+e3r1aH874r8b/zXpv/3WSs/9xQum57DvldL267dqoaZh1cL3onxr5wtt6e8lm64C8lEprl37eZD4zaZr7svOZsIaRzufpwIG8tGxAFunHZfYoy1rHM/lVPap+QNCdt33dHK3doo3Wrq3/ts3a/700Wqa50em9e+R5xT3CrfvgKytRr6YdzOTr9s4Tpas21+2PPKYXHumu2KiySk9ZoUlTF3rOVM3nc5IXfq/dplRttdZfKaadOmQ5sYrP3blRSpdf00JVfZ16ejrxLNnwUnXJ0hTXOR6s2lQdm7jeIKVa6dIG1v/T+5S6y9qf2LDd+oRKzS6/IvMzo0pqGuV60Z332vHEOXrcea2HPPOhvjZvgaNHlPXQsraubl9P5a33yeyJkzV6apwWbrL2XkoV5GA8lB3W1qVz9fa0N/WXv72iP7y4xHrFLda2zDqiV3Kq2aJZz3WnqMwQ22tzqlKt16xizBVql+Xzmz8RkfWt19Itp/u1RZt2WjdcoamuuzzIDVdvr6sblJRS1mulHU0kKCHF+q6JbqlWvD1wESEoD6J8WJgeuec2u1X5y//8r91K3GgT00yjh/S3a5KbgHzU4P66ucs19jwTkr/0zxm+8Ny0QEdBmTp8v9OUZwbqrqZldSRxvl7wnV52tlX3lGCxvswWf7DY3uHMlK4V/35bkxas1+7wZurdpat++0RHNXPm5qac9T4L7rROmdup2kqPjXhQfwkY+l/qWQoAkJ2GamySlaMbtHRN/k9fP2lvhK0Dz+5Zt8F/ebCj6qb/pDcn/0+zVu9XjUuuUu9bb9efu7ubbwWX63pPnbG+AaQSAS18geJzWMtWmzqgEWrdupq0fbGef/UzLd5yRs07XKU777pHj7XNjF+yldv1alyp4SMHali3Zqp9Jk3LF/xPf3zFBIanPPs81p5Wn8DPhDWM6EwLxvw7pbWrNluvbBldckle90qzU7h9XRilVD6qte565DENvbyCTu/9UbO/z//3Ur6ln5D5CaxsWe+pUm587s6ZRlfqmlqyO/VM/Ol7LTtcRm06tM16FkYeefcrSpXMIaV09i0i2t6S5bX+y4heMtU4AtW65h6NG95HfX5VTaf3rNWsaf/U2HN2PH4hStePM6br1c/X6ZdqzXTj9V01aMi19tk4Hkflqexb2vqM2hP8ZRy3P79lygSbWRi53a8Mc9PW3Sqj4LdcXu2vbGzN85RfSf9xvbaootp2aJU17C+E16b/26457h4WfPm1M9fju1U/a+bHc5xLWc34aI69DFAQHIllwwTd993uaUH+8lv/tVuQm8DcdO5pOu40AblpSb533wG9++Fc/W7sZDskN519esNzFFLZCMX+5nbdUF3a/c2Hvrqwqh3hqX+1PTXzVLH0PdpRXM1MnBIs2pugWavczanWafla65skvJUG9DWnsbVU4/QjRXBaaEM1MeVCD6Qo8bDpIT3cb6hc1N+XAHDRKa/Ya5taf49r5az/auGOwNqwOavfqJZ9gLBlwyaVDdgGV69aXqXXJOmnDKn65T11vylHcEl9HT10yHPlHOS63sgaMueo7U1er8zz2U7ph7XBaogChWQ6lv3Q1EI+rfItrrRbNG6zDnp3n5ZadrnLLgVxZdMaOnIw9x2s3K538vgJnbT261p26q1Rvx+qB0z5gb2rtWhtPTWtV8ZaYrfWbCjj/5mwhmrlaaKWPyeUuuS/euOHoypZvbVudDoMrF3d/GhxUJs2Ze7HbtuVl1bDRbSv27C2XWYnbd0av35/Tq7bpq3W/7J16tnzLyYr5s/RzwfcgXgphVfxNJI5cdx6TvP4nBT4tYuuI/PzbdrqBK3NksvzuTt3qqvLFfVU8uh6/XP2Jh2v1EKdfR17utWXtctgHQ9u1gr3Wc2nNsjeJShb024QUL9ehB3obFi7xhVi79Wq5IPOuMX7Xti0WYeq+L/W1cMrBglET+n48VPWoXgTde17v8Y/faNam3IvK7/TGmcJ5Gatvl9vfc6rXaK7b79OHa6IUcMMd+v9eoquaz6DqfpuubuevKNxXfvzvz9xlZKzfH6lWhGe7cLWzZnbhe2787JNz+1+NTKbJmvVG7RsY5AbtpS6pJVirDfNtrXf6POEHVLNSxRbgAq5hZGRkaG4eYv03arVQcNyE5KvTFhtLwMURKk/W5zxPDl+Ig+/I548rv3pp1SyXHlVzf8Zf1mkH01X+plSqlypbDa/bBWPqLq11KpZY+3as1frN6do2Y+rNWfx19b4Nn37w2r979PFWvDVcqXs3KPwalXU//Ye6nKN00U1CiR11ddalVZO0Ve2U4zZ/pesopgGGfru+xRt2rhbNdpeoqiwstqz+kdt2L9Xazcf0ZnDG/XJh8u0LuOMzpyppjZdrGW0SysWb9Se8nV07dXRqupZfdb1B9qxRnOSDqhC/UvVuVnmAhUb11dJ64tq9X7PF0bNlteofb39+nnpOu1I3689GaV0YmeC/rtgnfXeP6mT3tsNtr5g0w5t1OLvdird2iG56bJGqld2u75K3KUNq9Zo09HjOr5vt35asVjv/1hC1/+KVh4AkJuSNZupeUaSvt28R4krVmjhd+us7+9k/fjTD5ozZ4mWHolSbFMp8ZuftCnD+93hqFlFx6wd7HXbN+ub1b/oeInD2r1xjeZ9+o22R7ZWi1PrtfDnNB1N26dj5Y4p9btF+vjnQzp64lTmtj19i5b8uEcHftmlQyV+0c+bw/SrKyNzXm/dKspITNC6Xala/vNenSiRpp/nz9GiHSWVcfxElu8mIG8OOe/zE0rbsVWrf0rUypUr9OEnS/VVarrC6nXQ0AFXqmZJ6dS2n7R481Ht27tXZaz3Z+KXczV/60kdP3na2fexVhfkvR1TanOO14v86W1NXLpXp4/v0/aNP+vLhJ06cLqG2ndro2si0+39vK3rftaPv2So5JFftG7Vt5rxxW41bNfIsw8X7PPUKLQ/C5592tM6/Euq1iZar6nZts1erHnrDup0laYa+EhPtXAOnCod26ZFq/fpl5QN+uX4GaWunKsZPxzSiTNnMrcrQfeB87Cvq6NavyxR2w6laeuh09qzdpsqtaym7e5ta/koVd37o37YkmodT6Uo/VSGUld/pbfnJ+tAyVq6+e6uamaa1Aa9D8H36c93KUs/1j/nrNSK9dbrk5Sob776wvqesF6bsIbqfWsbRVXN23NS4NfOPOe7v9fK1J1asXyt9hyz3isbV2nmx6tV5uqWahuRh88dioB3+5u5n1GmXilt/3qtth07o6qtr1Nf6/NiHFy/Ul+lHHO2tWFqVGmv4tds1/ofnePBnWs1+4N4/XiopBp0vkV3NrbeINaGe+e365SyM9na37GOi0+kKP6Dz/VjehkdO37Ss67oWgrbvko/pmzXcue9tnfXBi39dIGWl2qrtqaBlt976Ig+nvJffb7Xcwy67ocELd92RKdrt9StVwaW8bh4HT9d4J4eLPut132ddlnbzr0ZpXVy5yp9/Lm17cywtp1hdXVVh2hF1zmh1Su3KSX5B600z+/xXfop/nN9vruWroyJUZXdK7Vq+0798N1a67N/Wkc2r1LcJ9bnt0NLNT2+VfGJ+5SWul57T5zRjh/m6ONVnu2COYPl2qbWdmDnai1cdyDzcp7uVyv9quIeLV2zU1t//kE/29uGLVo+b4G+L9VOre0YooYq71+lFeus+374jGq1vVE3mPdiEbriskvV7bqOfkOTRpkVG0qXLq0WTRtr1c9J2pq6Q/v3H9D2XXvsefv2HbBDclNK+ZH771LlSqYD0oIry++GF5RyZXw9rBQKLcpzYVqNP/HgXXr8gV+rw2WX2IG46cjTDKa1eeuWTe2W53998mG7NAuKQb3OGnhNNZXM2KL/zPze2t0w5VC6qE1EKR3c9KM+XrxZYdd3U6di3aNzSrD4fWJaqu+vf6X6ZU5o7deLNfPrvbr0zmvU2JlbGBUu66Mxd16i+uXTtfabpfp33JdauCZDNRrYRdYBALkqpUa9HtBf7rlSMTXKSkf2KMn0fbHuFx0uXVlN6+fUzWc93Tr4Dt3WpKpOpa3VnLjF+veiNdpmHRw0NFdr1VUPX1FTZY+naGHcEi3c3VAD7I6aXZp00l2tq1nL7Fb8wp+1vaQ5iMhlvea75v5bdEODCjqxx5r/6XIlhLXXsC7UKEdROKpt5jNghq0HVKpGI93Q5zca91isrx+YiOtu0p1NKqrk3rWa9elXWlHmGt1/eUAsEuS9ndv1KtaLUJmUnzTLvOc/S9TuSg3U897b1d287639vKcfuEYxEae1O2G5tc+zWHHfWwe8tWtbnwhH0M8T/Pr1Mdu2yrV0bbc79ZcRt8ldXrb0pd308NV1VfnkPi3/coni9zfUAzfkpUxkXvZ1o3XLzS1Us8xJbf3uGy3ZXVJZY4lSat13gAZf10DV01P1+WeLNWvZDp2q2UL3PHavumepz3zha3W19d1jHascSPW8Put+Oa3qTa7UY0Nvd2qG5+05KfhrZ63/roEaaq2/5qn91nWtz97S9dpfqZZMLpqnzx2KR6lW6tzabMOCdeKZqXSr3nrmXut9VC1D683x4GcJSj4Vofa3/EZPXue8SqVaqv+gzOPiWXN/1t6mPXTfpaa1sld5XXn3/XqgXU1VPJJiv9f+/ekP+iG9ipqaVutZVFD9mqW0aYU5BrXelz/uVxXrvTv4vmuse4y8aalb7viVIq1t54ZvF2n2t2lq2aej3WmmT/VYDXvkBl1eJ0yHN/+gT+d8rW+3nlSNuuaLsZRa9X1QD17bQDVO79cP8Yv04dfrrM9vTfvMw1KXdNd9HeqpkrVd+CH+S317oJHu7py3bXpu96tUzG0aeXd7NQ8/rV9+Xq4P56zUqkMVFOnaFbU79Tx9WqdL1tPVV5+bLUa9OrXsINwE4qZluZc7JDfLAAVR4ozFGc+TQ0cznLEcZBzWpr3HVbpyNdWvUvgsPm1PmvaeKmu90SsF2fECAAAAAAAACu/wicK0KL/I7f1SE6cs176WN+nP/Yq2Pnl+bd+5W6+9/R9lHLOLvhd5SF6pTL7iUpxjlStk1w9g/hRPi/KwUiprbVeOZaTbnRAUysmjOnJCKlG6FCE5AAAAAAAAcA5s/y5Ju1RWMZe2OKchueFuWU5LchSV4mlRbjl+6KC2HTypM6VKKax0QX+NO6MTx0/ppEqreq0qiiiacjMAAAAAAABAFrQozyrpq7nasHuHVq7Zq5MNb9Dv72srV5Wvc8q0LDeKOiSnRfmFpahalBdbUC6d1rFDR7Tz8AkdP+1MKoBSpcsoPKKiwstQTh0AAAAAAADFh6A8q5XvTdCM5FKqVP9KDegfq/oh0NElQfmF5QIIygEAAAAAAIALB0E5DILyC8v5XaMcAAAAAAAAAIALBEE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCklThjccbz5NDRDGcMAAAAAAAAAIBzp3KFMGescPIdlKfuPeiMAQAAAAAAAABw7kRWr+KMFU6+g3IAAAAAAADgYnTwuDOCkFalrDOCkEKNcgAAAAAAAABASCMoBwAAAAAAAACENIJyAAAAAAAAAEBIIygHAAAAAAAAAIQ0gnIAAAAAAAAAQEgjKAcAAAAAAAAAhDSCcgAAAAAAAABASCMoBwAAAAAAAACENIJyAAAAAAAAAEBIK3HG4owXvVNHlLL9sH45dkqnC3grJUqUUoWqldSkZkWVdqYBAAAAAAAARe3gcWcEIa1KWWcEIaX4gvLTR7V5y36lnSqhchXCFF6uII3XTyvjyDEdOHZaZSpXU0ydCirlzAEAAMirZZtW6fPEpVpu/U/amWxPi6xWW+0bt7GGy9Tn8u72NAAAAIQ2gnIYBOWhqdiC8iO7d2ntgTMKr1tTjSsVJt4+pf0792jjoRKqWb+26oc5kwEAAHJxMOOwhr7/JzsgN+pVraXI8Dr2uJm3dudGe9yE5pPveVYxdZvalwEAABCaCMphEJSHpmILyndu267tJ8PUpHGEqjrTCuxImn7anqGy1eupRYQz7Sw7mp6hpOQt2rJ9lxI3bLGnNYysrQb16qhlkwaqGVHNnoYitHmatG2GtG+FdDzNM62s9QYIv1Kq309qNNAzDQCAIBJ3bFD/aU/qUMYRXdmotUb3GpwlCE/dt1Mf/jBfUxa/a18e12ekbm97oz0OAACA0ENQDoOgPDQRlOfBrM+WaP6Xy3Q045gzJavYK1vrtu6dCMyLwv4fpe8fkdKWOxOyEdFeavuaVO0yZwIAAB6mtfgNL95rh+R5Cb9NqH7fP5/U4WNH9PbAF9WhcRtnDgAAQMF8++23mjNnjtLSnIZfAZo1a6Y77rhDUVFRzhScDwjKYRCUh6aCFA4PGXvS9usPL76hWfOXyPyaYILwpx+7T6/+9Xd658U/2OOD7uqt6uFVFb8iQc9Yy678ea3nyigYE5IvuSH3kNwwy5hlzXUuULPeWKimb2x1LiHU2e+H59co0bkMoOCGvPfHPIfkhmlp/u6DL6pSuYoa+v4f7aC9+CVpfNfW6vRCknMZAM6NWY+0VqNH4pxLAIrKv/71r2xDcmPbtm16+eWXlZKS4kwppOUr1XTkSs1yLgbi+BMAckZQng1TasUE31u379K1V7TWpD8M0+03dlJM04bak3bALsNixmOvbGPPu/fWbnaL85ffmklYXhimJbm3zEpemGXNdZCtxNlfZbMztFUPjbR2lLzDxbrDZO8suh6ne/ALpUPk+QBCgOm4c8XmBHVpeU2+yqiYsHxYl/52wP721x84U/MhcaI6tWytRr6hp8bzyxeA4vDRENe2xgxDsg3GDDsI91ueH+kueFm+c6yh60T/Bhf2Mtm9N3L6sfbc/JCb+ELPkPvB5oknnpA5yb9Iw3KcH+KGqHXr1r5haC5v7U+GZC7buvUQfeJM95M0Ub1c62w9JPhKzbp6TQz2+Y3T0FyvH7BMz4nyS7hyfFxJmtTTdd0s8y2Bj6F1T00KvKsBtxH8sQDFg6A8Gy+9NdMOvk2L8YfvvkUVyodpS+ouu4X5MxPf0Lip7+rRMRN8ofiNnTrouRGDVD6snN7492w7aEc+mZrkeWlJHshcx1wXfuzWAiMXqnd8sPeiCYXXKTm2nTZM6GoNzXX9unXqPHufM/8i0t77GN2D9XitWdff0Eox9kKFeT48AftDBXjrBuozyLrtp733CUBBfZ74lf3fhN65+fD7+XbZFa/7r7nDblX+eeLXzpS8m/VSsoYnJWizM8wdKL3aJ+fwCgDyL0nj5zbRXNf2ZtJ18RoeGJIGaDBwhm95Myx5qqUzB0UtIyNDn376qd544408D2Z5c728sH/46OP/nbM5aYYeVbL17nCJGaHh1ntj0gXyo0jMU4PV5cupIfUjsym5Qlh+ETJB75iNemBGghISrGFsrJaMCRIIO0ywPXrjAM00y1rDuNh4jQ4WUPdboG7edZphSm9nppEZUo+Odyb5MQH4GG0cMMO5/lh1ih/jH0LbIfYYaazrNuaOUAtndu6Pa72sO5h5XXu+f1j+ySvJGuydbw0zB0hv9XP9MBB4GzOsBab3y/WHBqCoEJQHEb9ild1i3LQkNy3Gvd74z2y7hXn32PZ2GRZTjsWE4qZEi2E69/zNbd3tgP1fH31mT0M+mI47C6ow170YLV+pkevCNWFCV01o7kxzSZy9VV9E1NPUW8KdKQ30Zt9wbYvfGBKBjvfxP9ne/3KoPh/Axca0KK9XtVaWjjsDmZB89KwJdpkWt64xHZW0Mznf5Vf6vDZFfZxxwz7gV7ziPnImAECRaKlRr43w+2G9zxMD1CBlgT4OGjAmaW2yM4qzYubMmVq9erVzKW/M8uZ6uTGtrocnD9DcJP/vHPt9sTBwmvXe6BmrrZ/NuUBK+/VW7+tSNSfuwgj2iwph+cUmSZOmxitqwCQN9/4e2XuKxsWm6q1XgrfgnhcvdRqcGUjfPGyAolIXaI7voxCnoXZ4PDdznQHWThyut2TC9hl6INKZ6LJ24lQtiRygSSN8d0qTx8YqZforTkht3e/h0xU9NkGT3fm7T14eV28N963f0nuYfV82rs/8TN88ZYpudsaNFiMGq5O1vzzPXkWQ22g5QpMGRGrJ1IAfDoBiQlAexPwlnqahfW7sZP83TGtyb0huwnBThsW0NDeh+PeuUismWDc1y7/6LsGZgjzbt8IZKYACXNcuSeIqteHXIthX282/HEfWFsb7NP75zPlZ1uMIvK3Ash7+8wNqygWWDslLSRC7FXW7LDvKHvv08eoM1b+krn/L5XrlVV/pWpvTflmQmnee++4/zbRm9z5XgY/d7zlMWaPOI7/S+OXmv5nvWY+3dp63VXzm9fyf77y1+A60VS/GZ7hakxf2+VinL6zRL2Z67pP9+mfzuLKUdwnyvGW+vp7Hah6j+3mghjmQu7U7NyoyvI5zKZMJ0L28IblpPT7l3r84Uz0iw2vb/xN3nL1kya8sQpZWoZ5T4N2n1w/0C9/jNDBImRf/msPe0+jjfOuy1xF46n6OLVLN7biWDXI6v33avG++dZ8+MusPvG+B66FEDXA2NG2eTbri4v8Z5oyYgjItw9u2batBgwbleTDL596iPE4TpqWqy1D/H0oCeV5H5/W7rYe6pEzXhEL+aBtYvsfveyjwu8RdPsUuE2TdF2+5IO/3TGD5IOc6drA/7ZWL8r3XoUMH9ezZU02bZv0hPzAsz43f8YE1ZHt2q31ckvMxhP+6rGMYv+OfnI5fvMcrW33HaPb98B4zem/bGbK9jxebpDlakBqpbr38t7nNmtiJcYHCXk/I3U0Bq/TTYsRc/9bffpI0Z0Gqorr18p/frImitFF2jh33it5KjVWPoCG5pRgeV3aim/nfRote3RSVmmzaqwPFjqA8CNNCvEJYOVUsH+ZM8dQsN0wJFq8KYZ7xI+nH7P9eDSOzHpwjD/JTmzxQPq9rwtveqyMU5y3F8Xg9Jc8M/PLep5Ej9+gm1zKKX+kKZ81Ow0q9WaO5U64jcz3uANfsePSOL2+37vaV9XDm2dat02Bd4sxrp4cirNv1hqVm52Jmuh56PJvrFsghbbCeriZ1vK2nHVGV1EQZ2rDduRxM+5rW7e/Tp77nyRMym/+Z07bq03Xe9W/Vi7saOPfdGpzn0P95ztCbn0tTncfvC/et5+XTNs717Nbdnh2uDTe4p60O2JHLXWBr8sI9H+YHCaeMS1/P/XrTt96sjytx9h419b2WprW/9VrnEnybx+17HsxtpW3X4AL9QACEtnFzpuj+aU/aAbk7JDcdeObW8rzAPpqnRYpV79ucy9nYOq2f4np6T50fawcaj/hOkzehcj+92mRs5un1swZow6iC1Y7dOm2q9IpnPdNus9bdZ7qajvfe9gw92sRZMIjEF+ap+SzvsllLPZhgpue0aE1y5m9OGqx1o6Zb3wQudpgyRhvcJSDGR+vVPoHhP4D8SIxboK1R3XRr0PR0vdZZ+0uLrO2GN5QMuv34cowe0STns2ltD6KszzgdfJ5f8vi94q+3Rg6M1KK5BX8tM1uxe7fbsc4ci9mu91mgXr7vB+u9kzwmoNa49V6a28Mzf+EIxZjrjNqoR33Xsb77nCV12zD7vXexnY31+OOP67777tNNN91kB+ImNDeef/55Oxg3wwcffKAKFSooPT3dnpcdcyw78pd6mcey1nFRcNbx6svWwYwpL5lNeUdzrDpSruPZvuX15suZYXlejl+2xVvf9Pd55mceC1nLves9FuqquNgwfTHTv5HQRWt9slIUrYCsVy2aRUtBw97eGmZaTI/JLD/yySvTleIKxtcne0Lu9XmpYx7UelmryBJAq2Uz656mKtm6U2vXb7TeKz3UbGJP1224yqrk+3E5rdxTYzXY3co8UNw8LbG2a+6A3t0C3WbfthPoA8WMoDwIU2/ctBSf57QsN0zHnab++GdLltktyE1plvc+9pRXaferzN/kEjdssee3jG7gTEGelY1wRgogX9c1LYqlh+5z7SxEtdJvm0tfrHIfzofpocddwa21zFTrC37b6h32joE3cI0b5HqtA5YxQfff1wWsx5T1cF/Hr+RHuEbdYI2v2+PZidierm0qrxZR9kxLwHXPuga6yXqeknc6QW3KDs1PC9f1ftMOK9l6HDfZO0kB9zeqrm60Xirfso7M1t0uzZtn7mi1j9ZD5iXOMi1D87/PT2jsCfaD3l4xCLydmFvaaZTvtZT6tLFe67R05fh9737M1vP5pPv9BSAoU3bFdMjp1ufyG+1g3ATkuYXkh5ySK1HVPC3LC8QOAeLVYOCwbM7ucblurKb5Qg9PoOE9TT7xhalaFDVAc19zHT3EjNBrrmXy5brBGuXdMCWu1wZFqrlvNyZrOQe3mKemZF7XYlr9KcVbD9dp5Tjefdp/b02bNcDacmWa9dJ0bbUer19t5NumaNJ1KlSIA4S0j4aoZ46tjK3PojfgNIP1udS0flnDcmtb85rvs2ltD4Zan/Ev54VGsHUhiWpivTo5i3lqrvVaZ26PY3p3U4NCvJZJJmFr0izz/WVtt73fW2a7roGTXN8Pwd47kXr0Cdf32Npk64gsWi1817Heo77vuZa6tXvhgv3zjWlB3qxZM7ukyuuvv25P69Wrl/0/NTVV69ev9w1pabk3AEvalSHVqJT5erRv5zpe8DKNutbpC+tYYrHvWDOAfawargnu47WAY6w8Hb80b+C3jId1DOw63o65pUFAgyu4mdbgMwds1GgnoLbrlftahyfJZNgp0/tpXo/M2t5B65gXggnjFT9GwzUp+/rheeHqiLNv8mBrPf6lVvyYmuhjTKmVYc4yLdWrW6T1WK174XuTmbIzQYuuA8WCoDyIHp3a+0JxU3LFy5RaMXXJTUefpjNPU4rF1Co3tckN0+rcG573udE64kP+hF/pjBRAfq5rB7kZevPlzNPAzGDtRwRwB9QeMXXK+3YMzA5KlnIdlpi2Earv3XkwQXdEhG7NsuPg4t7JCWTvqJiW7QUtM1L0WtZ2BbXm8TWvqTetHSbfDwjfp9nTMoMS9+l6K/Vmln2/MDWt54zmon7tys5YAS3faN2+N8QvbsEfl18pmpm5v6aFfsxACGrf+DK7xnjqvp3OFGvbXLepHYybgDy3luQL1yy1w/Zg5Vvywj7l3WmpnZfO8ho0aeaMZWXCiQbde2X9rjGhhy+kzju/27I7eUu1W3P7t/zLnl9ZhlGugxY7dM+tlaOnTnKXnlnP6bVDd9OcCUXO7zUzAy2ELyp2OQynZW7mD265sD77S8YHKW/hDkJx/irAtr+wnXraNfC/HGNtQwJLZXm26+bMKL/tjPv7weYOxS1Oq/Hh1rLBzm64WDv13Lt3r9atW2f/Ny3HC6pPz3qqb60na5kUr3T9/XlPSL4hp0ZWdqMsz7Gm7/gkyPFabscvwY9Xsh5LIzueTjjdAXXC4GT1dbfmNmLH+tUO99Qxn66gZc8Lyq+GuQnwPfXDp7o7/MxN7ymZj6PHPDsw9+sw1LHWtFzv56mJPsfvNs2PBiag97Zqn6ceM6zHGqQ1O1AcCMqDMOVVnnign92q/Pmp79itxA3TcvyvTw7Svbd2swPy50YMsmuVGyYkd4fnpgU68ql+P2ekAPJ9XU9Hl95TyHzDOW2tHUy4Rj1t7ltzNbFLjxRFYF5ZTYO06vb8gJB7aO35ISBNH1s7ZbNW7dP1baznzJRksac5LbbNNItnp2qd5JQl8ZSWsWedE+b+yi/ENwr3fOSd5wcDv5I/2Z4mCaAw2jf2dMT94Q/z7f9e3rA8p5Dc1DHffmC3HbYXhAmtvOVH8hxanUN9XjMtTJ3T5HMMUT11xXt+1i34qfc4b3laljqvmRncZyfgAubpd8BTDmOu39keuIi1aKIG2qi1BQiQC9Wpp/lxxd7ue0plBQbmXXwlvNxD1o5FM5mOR80yY9XUCdn9A/OLq1NP02rchONt2rTR3/72N1WvXl1Hjx5VmFPKNd+iWmmxfSxhyqSYADubwPyXw7m/3uYMae+xiWvwtELn+KVA3HW/XezSJpFNlKV5hKkNLv+A2r+TzJYy1U2ysMum5FUzeUqJB9yppPXWPY00v5N6ao1n4bmeZzSfj8uwHsfMAZFKWTDHr+X7J0Naq+/0aI1LCN5xqF1v3Ru2mxbppvRKdrcBFDGC8myYoHvQXb3tFuTP/+NdOwQ3gXnNiGp2aRYTkJuW5Kae+Rv/ma1H//A3X2ef3vAc+dRooPVFXYCmvuY65rp5ZdeeLthpX+6g1a9ltYvdojqivPV1ZjEdQjqhcuE00JvWjomp7Vb4shvhalFD2rbrkHPZkZfW74ZdPsXU7ja1yDNLrNzU3Ewz9b6905xOMmODnQp4Lnhqp3tD/EyFfD7yavkefWF+oMmmPiCAonN72xvtFuFTFlvf3Ts2OFM9TECeXUh+MOOwRn/4gj0+rEt/+39+mJDcE1rlFA7kT0vr6CRYsOGpR+w+/T5V6/zOvfW08ssbJ7AwwXd2p+Y7dXEnmdqyzqSsgtSUtU+v92ppMp6gp9PPmhtvt2YFkBcmJHf6LsjxM5m9xHUb81TCA+eZmF7qFZWqV18qQBNS04pb7k49PdvkoOF54hzNSYlUr94B75DbpshTv94bYme/Xc8bT1mguUHKiZlW7LpIOvU0NcfHjx+vZcuW+UqsmLDc1Co3YbkpzVKg0NzuL8k0RAosR1lev33amq7t6p1Tf0i5Haty/FIwrrrfbt4644GdbdpBcy6CdpjpCrlz5wnbU7LeKV8t9OC1xl21zfP5uLJjQnK7tExOJVn8JGnS1Ph83QZQGATlOYi9so1GD+5v1xs3NclNYN7/yb/aobkZHh0zQU+Onaz4FQmqHl5Vjz/QV7+5rbtzbRRI29fyV2/cLGuuky+eOtuBnYkkzl4Z8Et8QEcly1fa5Vm8QatdZy3N2vnwdrxppKzR4HhXDWy79rkp8+K+ra16yH2dnFi3WRy9g3tP18tct3WfZu7LY+3ucN16iemMxZzOl9k62/xwYE/z/kgQJIBOnL06SOmVs8TZ0QtWdqVwz0c2LdIDmR1RpWut7z3muQ0AxWPKvX+x//ef9mSWsDwYE5L3/+eTSt2/S0M631eAsitxivsyUo++UrDQKjv26ecp09XT3dI7caIe8atHbFreSYsmuzvWHK5Xc/uR1lrPwLyehp+lFWOcBrpPrbdP67fuw6ghru+7gGUs3tP3/VoOfjREw81z565fCyB7dogZq0k5nB1gl2TxdrYb+Fl3tiHByjrhfGd+2ByrLqYMiqszZQ/zA4pnG+wpueTeHhtZa3/b22S/DqQNaz3DTH8SmX1azHokcF2Z7NJZ1v3x65A5t+8Xa7ufawfO9o8CmT/ABj6mwMt+7/nzkAnL3333XV/Hnd9++62ioqLsFuYmMDf/vR185mbWG3npFNOcmZxLWO40gHrzXf/5vvVz/FJAWTvnNHW7R8dndmpplx1xOuO0y5ukTtdwd3kSe3mpk9PDZdZlkjRp+HSlxA7W8IDfs7Jjl2qJH6Ohvk2Ap/Z3p8FOLfTew/RAZLxGD8ncRnwyZIyWRA7QMPtu5OVxDfEvF5M0UcOnu4P0OM2Lj9QDk7z114OIm+i3jk+G9AtocW/db79yLgGXTd1z63Lm4wTyh6A8F6bV+Ogh/fX0Y/fp2ita24G4Cc3NYFqbt72kud3yfNIfhvl16okCqnaZ9W3wed5alptlzLLmOvnUZ5DTY7e31po5pWxXzYBOSMI14YZ09fYuY4LTvu6evE0r7+a63q4P5yzzcppufNy9TLDbWidladWcvS9meq/nnPZWFL/om9P1Hq+nZN+6PeVR8try2y6/Yv13t872TnPXbe8zyP/5GWw9Z+eq9EriznTrPeMN8QMU6vnwdMC6zSmNk+0PG3ZHr3LVxt+jmzh1ESg2ptX4uD4j7U49TVj+zjcfOnOyMuVW+kx5xK5rfttl3TWsy/3OnHywa3Q79b5NCRP3UKgDd9PazglFvOvrs0C9AuoR93nNWsYE6s4yj2iS3UFmbja468qOknVgkk1r+BjTgahcj2+eegeUXjFlXCZd56k561smoDNP+/R9pyNB/9uldASQZ/aZGu7PmmvIpnyS32c9H30ooOA2bdqkzz//PM+DWT5vPK2wJzXJ3OZ7hn6a0z3nDqSzdOppl1TJLH+SuZ4ZAWWa3O83z3zf+8e0Mre+DxaN8s63hj7J6p3L+8u9vF3WK8vZERdfp55u//rXv+xyLMaqVavsIP2+++5TREReDpbcx5YrNf+Sdtl02GnCcut4zDTuClqexRWm+9a3UH+vHe15H3H8UmCBnXO2HiONy7YFdW9NThir6On9nJrcZvl4dRrrLksSuEw/vRU9VglTsv/BNIuWIzRnxgBtHOPchrlTfrfRUsPnztADG8c48wM7Fc39cbVo5q4tbg2BNcjtVvCp/st4B1/HpMl+80fLepyu+wAUtxJnLM54kdq5bbu2nwxTdOMIVXOmFdiRNP20PUNlq9dTi3MUsuEc2DxN2jbD2g9YIR13miGbFuSm405Tkzw/5Vbya/lKNZ0pTZjQrshOnwcAnH0mBB/y3h91+NgRVQ6rqK4xHX2txQ+mH9bniUvtVuTG0z0f0/3X3GGPo4gkTlQnO9gnCAcQOnbs2KGZM2dq587MTqVzU6dOHfXt21d169Z1phQP0/J6UhNX0H1eM/1jTFXzC/g7ZOjQoc5YVpMnT7bD8j/96U/q2bOnbrrpJr300kvasGGDPQ/nzsHjzghCWpWyzghCSrEF5Ud279LaA2dUrXZNRVcp5UwtiFPav3OPNh4qoZr1a6t+Afu7APLlPA/KTSeZveMznEtuYXro8WsDWsbnU8oadX55u7Y5F91MvfHgrRXOLZ4PADkxZVXe/voDLUxcqrU7/etAmlrmpuNOU5M8/+VWkBtzanzwloIAcHFauHChSpQo4VwqmBtuuMEZKwYfDcn5DKLzjN3/h8ZesB0R5xSUDxo0yO7gMyUlxa5Zbjz//PNKS0sjKD/HCMphEJSHpmILynX6qDZv2a+0UyVUrlwZlSngvsKZkyd09MQZlalcTTF1KqgwkTuQZ7QoB4CLkgnNE3d4ermMqlabcLwIzXqkp9Y+4Wr1Z4cx8eoy3r9MDABczEaPHu2MZVWtWjWFh4dr37592r9/vzM1q3HjxjljuNDlFJSbMisPP/ywXavcW8M8ISHBnkdQfm4RlMMgKA9NxReUG6eOKGX7Yf1y7JROF/BWSpQopQpVK6lJzYoq7UwDih1BOQAA+WK3Hp+W6lzyICQHEGo2btxoDwUVHR1tD7g45BSU54Sg/NwiKIdBUB6aijcoBwAAAAAACEHffvut5syZY5dTyQvTyrxXr1666qqrnCk4FwjKYRCUhyaCcgAAAAAAAMBCUA6DoDw0lXT+AwAAAAAAAAAQkgjKAQAAAAAAAAAhjaAcAAAAAAAAABDSCMoBAAAAAAAAACGNoBwAAAAAAAAAENIIygEAAAAAAAAAIY2gHAAAAAAAAAAQ0gjKAQAAAAAAAAAhrcQZizOeJ6l7DzpjAAAAAAAAAHBxiaxexRlDKMl3UA4AAAAAAABcjA4ed0YQ0qqUdUYQUii9AgAAAAAAAAAIaQTlAAAAAAAAAICQRlAOAAAAAAAAAAhpBOUAAAAAAAAAgJBGUA4AAAAAAAAACGkE5QAAAAAAAACAkEZQDgAAAAAAAAAIaQTlAAAAAAAAAICQRlAOAAAAAAAAAAhpJc5YnPGid+qIUrYf1i/HTul0AW+lRIlSqlC1kprUrKjSzjQAAAAAAACgqB087owgpFUp64wgpBRfUH76qDZv2a+0UyVUrkKYwssVpPH6aWUcOaYDx06rTOVqiqlTQaWcOQAAAHlxMOOw3vnmQy1cs1RJO5OdqR6R1WqrfeM2Gtq5vyLD6zhTAQAAEKoIymEQlIemYgvKj+zepbUHzii8bk01rlSYePuU9u/co42HSqhm/dqqH+ZMxkXvp59+0jfffKN169Zp79699rTq1aurefPmuvrqq3XppZfa0wAAyM7CxKV6+sMXdCjjiCqVq6iuMR0VGV7bnnco47Adnm8/sNu+PLRLfzswBwAAQOgiKIdBUB6aii0o37ltu7afDFOTxhGq6kwrsCNp+ml7hspWr6cWEc60syxxwxZ9v3qt/X/r9l32tBrhVdUgso7a/aqFYq9sbU9D4aWnp2v69OlatmyZMyW4Dh06aMCAASpfvrwzBQCATB9+P1+jZ02wA/JhXfrr/mvucOb4W7ZplZ7+4P/swLzP5d31/O1POXMAAAAKzhzbJiQk+Bp+BWrWrJk94PxCUA6DoDw0EZTnYkvqLr338WdKSt7iTJHq16ulimFhStq41ZniCc3vva27HZqj4MyOxAsvvKBNmzY5U3LWuHFjPfXUU4TlAAA/Jvy+f9qTdkj+7oMvKqZuU2dOcKY8y5D3/qgVmxM0pPN9GtblfmcOAABAwTz//PNKTU11LgVnGoDdd999ziWcDwjKYRCUh6aCFA4PGSt/Xqvnp75jh+Qtoxvo6cfu0zsv/kFjn3xYo4f0t8dfHDNU117RWr/sO6CX35pph+ooONOSPK8huWGWNde5UM16Y6GavpH5gwsAoGiYFuJGXkJyo0pYJU259y+qV7WWpix+V6n7djpzzrGPhqhRyyGa5VwEgPNG4kR1atlaAz9yLgPIIi8huTmT+t1333WmFNLylWo6cmW2+w0cfwJAzgjKs2Fakpvg2zS3f/yBvnYwHtO0ofak7df8Jcs167Ml9jI1I6rp4btv0XMjBtktzc28D+cv8awE+WJqkudWbiUYcx1zXQSXOPurbHaGtuqhkdaOkndghwnARcKUXDFlVEzL8LyE5F4mLH/+jt/b468sesf+X1CJL/RUo0finEvBxGlgy54an+hcBID8sH9Ea+0/dJ2o7DcpZpvjWjbH7RMuCM4PFTm+B+xlsvuxNUnju7ZWpxeSnMtuOc0rPrl/d158TEvyIg/LcX6IG6LWrVv7hqG5vLXXTuzpWr6nJmX5+MVpqGt9rYcErDDg9uyh50StdWYH+mSIWWaIPnEue+RyG36cZXNYxvOYgj2WwMcb+Pzk534ARYugPIij6Rl6+a0ZKh9WTqMH9/eVUzEtzJ958Q271fis+Uv0zMQ3fKF4w8jaGmMta8Lyjz5bYtcyR/6YjjsLqjDXvVjZrQVGLlTv+AxnipsJydcpObadNkzoag3Ndf26deo8e58zHwAuXMs3rbL/3375jfb/nIz64AU7WPfq0LiN3ap8xWbPOvJr1iOesKLntGxakPmCjTFa5EwCgAKJGqC5SQna7B0WjlCMM8ufCcnHaMPAGc6yY9XlyzGFD0FjRmiJtb5ptzmX4ZORkaFPP/1Ub7zxRp4Hs7y5Xl7Y3zV9kjXc/fonzdCjSpbfq2q9RsOvi9eksxx4F1TMU4Ot9+bUkPsRmbD8ImRC6zEb9cCMBLtGfcLYWC0ZEzwwNkxo3Hd6tMaZZa1h5gDprX7uENsEx2O0ccAMz/oSxqpT/Bj1mhiwwsgBmumswx7mjlDQ4sBJEzU13hn3yeNtONZOnKqcm4jG6ZXpwfeHTUjfd0E3v/s6ubczM5/3AyhqBOVBzFuy3C6lcmOnDnYA7vX6v2f7WpibMizeUNy0LDcqlA/Tb271HJSbFufIn3Xr1jlj+VeY616Ulq/UyHXhmjChqyY0d6a5JM7eqi8i6mnqLeHOlAZ6s2+4tsVv5PR+ABe85Zt+VIs60YoMr+NMCc6E5B/9+Jne/uYDZ4pH11Ydlbp/V/7Lr3w0RMO/jLUOghI06Tpnmp8kjR82XTJh1awB1pYXAAomcd1GZywPPpqnRYrV8KdaOhN6a+TASG39bE4OLdBRGDNnztTq1audS3ljljfXy41pdT082fxIMkV9nGkeLTVqYeA0qU/P2Avote6t3telak5c6AVihOUXkyRNmhqvqAGTNNy32Z2icbGpeuuVYC2jPYFyp7FTdLMzpcWISXogMl5TnXDYDqUjB2jSiMzt+OSxsUqZ/oovTF+7Pq/fC9b9Gz5d0bGxzmVH3Dwtsb4rBrtuY9iASKUsmJO1VXrSRA2fHq1OAatw+2TIGG20biPKuewTN0SjNw7QzGxC/Lw8VqA4EZQHEb9ild2avEen9s4Ua4dkwxalZxyzw3PTwtyUYfGG4qaluZeZbuqZuzv/RN5k1xN4XhTkunZJElfpkYeWOzMMX203//IkWVtc79P45zPnZ1mPI/C2Asuc+M8PqCln35fsrxtUe9NSvF2WHWWPffp4dYbqX1LXv9VRvfKqr3StTXEuA8AFypRdMWVU3ExnnZMXZ5ZT8YbkJlB/98GJzlSPys51U/Z7fgjPs9umaHOW4MLNhBgJWuILq/Ih4BT7LDWBA8swBJx+H/R0dnudmeVf7BaK1jLeVvHe1qbey94hp3rE9u24ls3aYjVr+Qfv7WbynPLvXg81kIEgmjTLpgV5/gV+/s3g+fz6fx79PtMB2xBfnwq5ba9CgGkZ3rZtWw0aNCjPg1k+9xblcZowLVVdhmZ39oCHZ1vslFy5rYe6pEzXhEK+Djl+FwS85n7bdO/7wvs95f1+Cvzecq5jB/vTXrnoGu9ceumlevbZZzV58mQ9/vjjCgsLs6ebFrPr16+3h6uuukqRkZF5KkfqPXvYOwQ7BrWlrFFns8zza7L9scR/XV9pvN/xYEC5Tr9jVc+xcOfZW33HxPb98B5Le2/bGbK9jxebpDlakBqpbr389/eaNYmUNq7PGjo7AXUPX4tqo6WaRVsvX/J6azxJcxakKqpbL/9guVkTRWmj1rt3taKbBW9B7rJ24nC9FT1Wk3s4E/LNCdrHTlG2q3DC8EnDmjgTvJwfEQIfi08+HitQTAjKg6hQvpwdih9Jz9xRMa3FDVOWxeuosyNT0Vrebc++A84YzlcmmO69OkJxdtkRa3i8npJnBn5579PIkXt0k2sZxa90heVmp2Gl3qzR3Clfkrked6Budjx6x5e3W3f7ypw482zr1mmwLnHmtdNDEdbtesNws3MxM10PPZ7NdQvkkDakWcdWdbytyR1RldREGdqw3bkMABeRcZ9O1eRF7+jpD1/IEpIHhurnn3gNHya95pxeP3dgpBaNyqw5awciozbq0VmZp+BPajJdPQvSCeiXYxTX07MOE+hntlx01j0+h6ZDJsBJHuy7D6bVvKb1c4UpgeUfrMfSZKqGf+nMtpll+unVJmP91rNh1NmvlQucz5KSU+3Pqy9kzKk++W3D9GiUtR3xhZeesLVB917+Yavr828+61utz28j6/O4bqh72vBcymLkvL1CITlnB/TOV7kbzxkEi+a6f5DMnxy/C0xI3meBevm+g2bo0WTrven3A6j1vpjbwzPflAgy1/H73hqrLs6S3vdr3EX0A0tERIT69++v6tWrKyUlRc2aNbNbkBuvv/66Xn75Zd+QW+efhjmWHflLvcxj2b4Bx3U+1vHqy9bBnSm3+XSroD+umGPVkXIdz/YtrzdfzgzLE2fvUVPfsag5W9k6Vg0I3bfFW8eu93nmv+lra2gt96401bleXGyYvpiZfSejF5X1yUpRtJoFtItoYZLv1GSZ6NvNbgke2UTNnMtemcH6eplNfnTgCls2s24lVXaWbllvFoofk1nXO1h98rghnhIvU/xSeY/ew+xW7KN99cA9Ld0DQ+tPhvTzBO1BVmFLmqhepuzMpGAtxs1jMT8irM+mBnneHitQnAjKg+jRqYP939Qh9zIlWEyplc/il3tqlH+2RG/8e7bd8rytU8PcMK3R9+47oGuvaO1MQV6ZHYeCyt91t+rFeOmh+1w7C1Gt9Nvm0her3K21w/TQ465W2dYyU60v+G2rd9g7Bt7yJXGDXCfPByxjgu6/rwtYjylz4r6OXwmUcI26wRpft8ezE7E9XdtUXi185ysFXBcAkIUJwLcHtAYffdNge/qsHz7LNSRP3ee5bkzdwFYw50qkHn0ls/WgXcNV3hDBaV04fq5GuY6A+7xmQocCBA1RAzTSFcDYYZy71eptU3KoR9xb015zHTXF9FIv6/trwzpPwJ34wlQtstb/mqtFfcxTc/3K1HiXmeu3nhF6jTIRgJ8+r3kDRjOY2tTT1TPbsNyczeKEl3aw7vnBKsvZLdeNzfx822Gl9T/LtNzKYuS0vUKRiGpivaI5M9tW9xlOMb27qcGX8wocUub0XTDrJVNSbJLrO8h6vw2Nlfxuz3pfPOHarq9Nto7IotXCdx3390dL3dq9cMH++cYE5eXLl9fixYs1fvx4+2zoNm3aOHPzL2lXhlSjUubr0b6dK6D28vRJ9UXz5lrsO9YMYB+rhmuC+/iyfbQeisjQ/O89Db9ibmmnUa7aGX3aWOtKS5ffVqB5A79lPKxjYNfxdswtDXS99unTUGlVfg7cPCWz3ndCwgw9YH0v9HWH5d4Ae0ZmiRd/LTV8rnW9jd6w3VMnfI6vBIppjd7TUzYlWNBui9PQfqa1+dzMsjNuSeu1Ual6q9889fDdV2qQ4/xCUB5E7JVt7PIpX32XYAffXg/fdasdls9fstwO0U298ofvvkU1I6rZ802t8vc++swOz/vc2Mmehrxrbn2JF1S+rptyWMnK0JsvZ54GZgZrPyKAO6D2iKlT3rdjYHZQspQvscS0jVB9786DCbqtHaNbs+w4uLh3cgLZOyqmZXuwsi8AgGBMh5ymxnjijg3OFNmBuAnGTUCeW0vyhYlL7WXOn5bm7jAhQOJ6bQjautDUec0MqfMsoJRDnycGqIHdatVVXiFH7tIq/fSq6/RtE7RkacEaILtl7JAnJaCTOuRJYDkc/1aeuDiYIHxs9uU17LIYw6VXMsP14cn9cm6F7mhgbRPyJ4ftFYpGQbaFhezUM/vvgiSTeTtnH7i2M6MCewkMeF94z3Kwlg12ttDF1qlnenq6/d+UVrnjjjt8LcsLqk/Peqq/bp11DBtYJsUrXX9/3hOSb8ipkZXdKMtzrJl5XLxSb6Y58x1+ZUJnZj0mrV+7sjPmlvVYGmeTCb3HqlPqdHnKoucSYBsmSG9tfVdMygzcB1vfFb6W6d7W6Nl1EGptDyb19ITr2bY2d7jrsZt9VrsWOjXIcZ4gKM/GEwP72YH3G/+Js1uPm5IrplX52Ccf1nMjBtmdeb42dqRdr9wwgfrzU9/R0YxjfuE58u7qq692xvIv/9f1dHTpPYXMN5x3rbXDNeppc9+aq0m8p1Z54QPzymoaISXvDFiP/QNCmJrWcy4DwAXqhphr7f/Pz/2H/d/LG5bnFJK//fUHOnzsiLrGdHSmhLiYEVpigrXx0Xq1jwlAsg/MPYHsGGm8q5UrB8nnnKdlaWZAutndWh8hIWuLX9MiPYdgHeevFk3UQBu1tgABcqE69czlu6CLb7vvHnLvs8OUXGnqhOz+gfnF1amnKafywQcf2K3KO3fubE+LioqyO+8skKhWWmyOXe0yKSbAziYw/+Vw7q+3OUM68JjYGjyt0D31yf1KlmZb5gU+2dTTzq7ESnYlWexSKnbN8WbyVGEJWKHdOjvStHHIld1BpvV/yRhXuZMx5geteI22xk1r7k9esb4r3B2QWm6e4g3bPbXFvct71zHanmRaoPfUpNde0VvWXU6Z3i/zNvpNV4rdgtwaN+VV7BIqWdnPga3wjxUoLILybJia5KMH97dbkJvW4yP++oodmJtOOmtGVLU77TQdfMavSLDnmUDdtDB//IG+vvAc+WM6OCnIzoK5jrluntm1uAt22tesVfuk5jXtnb6WtV0lVlwSv0/Ttojy1u6fxXSQmZamjwveYMDRQG9aOyamtluw28yfcLWoIW3bdci57MhL63cAuACYFuVXNmqt5ZtW6cPv5ztTPUxAnl1Iblqgmw4/K5WrqPuvucOZep6LaaamQcsaxCnuS6lpc9fRTvJ6/+8P+9T3PLI7KjXBd3bBRZI+/ixVDQbOyKE0i3XInSWk8bRG9GppHR0FC3IS4xZoax7KDQAhyz67JFLNsxyG+H/GcIGzS1ql6tWXCnBmiGnFLfePIy1N7h48PE+cozkpkerVO2Crm+W7wLOOgpdJ6a1pSZ5a9oH3w7Ri10XUqacpu/LHP/7RV4vctDI3dcpNC3PTuaf57+3gM8/at5Onj6vMUike5fXbp63p2q7eOXTimeux6vI9+sI0MMumvjmykU09bRN8B+3EMmiwniSTq3eye/h0d+zpYmqhR3ZTQJ+hmVzhcosRc32txH3DWNPXQKzGWeNzRlirs24ve6aFesD1rWGcvYqx1vhcDX9kSpb5CTMGWI8tUg/MsMbtci3Bg/DMHxEK+FiBIkRQngPTgnzM4P66rXsnOwQ3gfm4qe/q0T/8Tf2f/Kue/8e7euM/s/WLU5P8r08OIiQvpAEDBqhx48bOpdyZZc118qeBbjL1yAM6E0mcvTLgl/iAjkqWr7TLs1zfxtPq3K6zlmbtfHg73jRS1mhwfIauv8HZmbBrn5syL+7b2qqH3NfJiXWbxdE7uPd0vcx1W/dp5r7M+w0AF7gp9/7FDrxHz5qQJSwPxoTk/ac9qUMZR+zrnj9lV3LjdNI2yr9136xHxti1vr31xj2lS9wBSZwGZjk1PqtZj+S1Ez4ncHEd2CS+MNyv9Ip96r51Hx5xtRoMXMY+3d5apqe7PEjiRD1i6rAPzax7DIS2JI1/xF02xbo8bLq2RnXTrc6HZNYjrZ3SKp560YGdcNrbiHx3ColzzymzY8qgZCmdY70Punq22Z4zfAK331lrfwfbLvveT9cN9p2FkNN3gWmpbjqCzey42WJttwfmVObloyH+ywdj/yiQ+UNw4GMKvJz5nj9/paWlaf369fbw0ksv2WG5aWFuOvc0/5944glnyZzNeiMvnWKaM5NzCcuj6urGCOtY9V3/+b71myBd6Vrr+572HDMiN55SIkvGDMksJRI3RKPjYzXYqfdtan23bu3MbzlCg2NT9dbwzHriaycO11saoGHOSWA3DxugqPgxGur7+MZp6Jh4dRrsLYOSpElD3J13WpeHT89HuNxSwwfHKmX6cE1yfXQ/GTJGS6zvCjuvLxJBbidpooZPT/U9ltwfq3W5tacVvEfAZbuETGvX9YH8ISjPhWlZfvuNnTTpD8M06K7e6h7b3q5fbgYTjt97aze9OGYo5VaKiDkd7amnnspTy3KzjFnWXCe/+gxyeuz21lozp5TtqhnQCUm4JtyQrt7eZUyQ3Nfdk7dp5d1c19v14ZxlXk7TjY+7lwl2W+skJ2zPiy9meq/nnPZWFL/om9P1Hq+nZN+6rfvk99gA4MLmKbPyoi8sNyG4u2a5V+q+nRo3Z6r6TH3UDsnH9Rlpt0i/kJjSGnMHyjkd3jMM11htXugKlp0OMReN8i4zT71nDbC+yXLjqSHruU4/zekepANAh13Kwa5h61n+EQ32L71iTt23blOuWraPaJJfZ56e1oX+62nUZ4F6zUrIsaU6EHKSp6un9zNi+gNoEvCZdzOtgH0lMzzD8C9jNSnH0hgorE2bNunzzz/P82CWzxtPK+xJTdzvAc/7YE73YTm+plk69bRLqmSWP8lcz4yAMk05fBfY769Y1/eLNfRJVu9sviu83Mv3/Kyb5mZ5/158nXq6mXIshgnLn3/+ea1atcoux9K0aVN7es7cx5YrNf+Sdtl02GnCcut41TTuClqexRWm+9a3UH+vHe15H1nHjFNj5erba49uovRKnpgW3DMHbMwsUzJGGpeQXSeano44x0VPV19n+b4LummmuxZ4yxGaM2OANvpKp1grHJvgXwt8Y+b1W7fup7eixyoh23riQfSeooSx0Z4yKc56TLif0/0ukMDbsWunux5LXh4rUIxKnLE440Vq57bt2n4yTNGNI1To+PhImn7anqGy1eupRYQzDRe9n376Sd98843WrVtn9wxumI5PTMedpiZ5vsqt5NfylWo6U5owoR0HEABwATuYcVhD3vujVmxOsC9HVqutyPA69vjB9MNK2umpSVCvai27JXlM3bwcoKIomVaAdrBP7WwAF4kdO3Zo5syZ2rlzpzMld3Xq1FHfvn1Vt25dZ0rxMNvcSU2y/9Hz/GI6iJ6q5rPm+tXYv5AMHTrUGcvq2WefVYUKFfTOO+/YHX22adPGbmm+YcMGTZ482VkK58LB484IQlqVss4IQkqxBeVHdu/S2gNnVK12TUVXKeVMLYhT2r9zjzYeKqGa9Wurfj7LdgEFcp4H5abn8d7xGc4ltzA99Pi1AS3jAQCmNfmsH+Zb/5N9oXmLOtGKqdNUXVt1pPPOc8aEIJ4OQGkxDuBisHDhQpUoUcK5VDA33HCDM1YMPhqiRqN0wZxRcKH/mJpTUH799dfrzjvvdC7JLsli6pcbBOXnFkE5DILy0FRsQblOH9XmLfuVdqqEypUrozIF3Fc4c/KEjp44ozKVq1kHsxVUmMgdyDNalAMAULQSJ6rTS820xBd2mHq6/fSqBgQ55R4ALkyjR492xrKqVq2awsPDtW/fPu3fv9+ZmtW4ceOcMVzocgrKDVNawpRcMSVYTIefXgTl5xZBOQyC8tBUfEG5ceqIUrYf1i/HTul0AW+lRIlSqlC1kprUrKjSzjSg2BGUAwBQxDytxxc5l2xRhOQALi4bN260h4KKjo62B1wcTP1xbz3yvIqMjNTTTz/tXMK5QFAOg6A8NBVvUA4AAAAAABCCTEtx01Gnt8+t3Jg+uUyt8vLlyztTcC4QlMMgKA9NBOUAAAAAAACAhaAcBkF5aCrp/AcAAAAAAAAAICQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKSVOGNxxvMkde9BZwwAAAAAAAAALi6R1as4Ywgl+Q7KAQAAAAAAgIvRwePOCEJalbLOCEIKpVcAAAAAAAAAACGNoBwAAAAAAAAAENIIygEAAAAAAAAAIY2gHAAAAAAAAAAQ0gjKAQAAAAAAAAAhjaAcAAAAAAAAABDSCMoBAAAAAAAAACGNoBwAAAAAAAAAENIIygEAAAAAAAAAIa3EGYszXvROHVHK9sP65dgpnS7grZQoUUoVqlZSk5oVVdqZBgAAAAAAABS1g8edEYS0KmWdEYSU4gvKTx/V5i37lXaqhMpVCFN4uYI0Xj+tjCPHdODYaZWpXE0xdSqolDMHAAAAAAAAKEoE5TAIykNTsQXlR3bv0toDZxRet6YaVypMvH1K+3fu0cZDJVSzfm3VD3MmAwAA5MGyTas0ZfE7Wm79DyayWm0N7XK/+lze3ZkCAACAUEVQDoOgPDQVW1C+c9t2bT8ZpiaNI1TVmVZgR9L00/YMla1eTy0inGnnSMax49q9d589XrVyRWuoZI+j6C1Yn6r3f9ygpZt3KeXAEXtaVNWK6tiotu65rKm6NYu0pwEAkJ1xc6bonW9m2eNdWl6jrjEdFRlex758KOOwFq5ZqoWJS3X42BG1b9xGk+95VlXC+G4HAAAIVQTlMAjKQxNBeR4cTc/Q/Pjlil++Sr/sO+BM9agQVk7tLm2h27p3Us2Ias5UFMbBY8c1ZNZSzfxpkzMluL6XNtaUPh1VpRxbLwBAVqM+eEEf/fiZWtSJ1vjbn1JM3abOHH8HMw7byy5K+lot6zTROw++SFgOAAAKLT09XQkJCdq7d68zxV+zZs3sAecXgnIYBOWhiaA8F/ErVum9jz7T0YxjKh9WTg3r1VaDyDqqUD5MSRs2a8++A9rrhOc3dmqve2/ltO3CMCF5r3/O03epvzhTcnZFZA3NebDHBRuWz3pjoUaquTYMauBMAQAUhbe//kDPz/2HrmzUWlPu/Uuegm9v6/MbYjpqyj3POlPPsY+GqNEoaVLSFPVxJgHAhWLWI601PHmA5i4coRhnGhBKnn/+eaWmpjqXguvQoYPuu+8+51IhLV+ppjOlCRPaBd1v4PgzbwjKYRCUh6aC9LAZMt74z2xriJP5JcG0GJ/0h2EaPaS/fnNbd91+Yyd73EwbdFdvVQ+vqvlLlusPL75ht0BHwZiW5HkNyQ2zrLkOXMzO0ciFwYfn1yjRWUzaqofc897Y6kwHgAubaSE+efE7qlSuYp5DcmN0ryF2sP554lK7rnlhJL7QU40eiXMuBROngS17anzmRhkA8szexrRsnTnktL1JnKhOLYfIU4QqG/YyrvWxfTr/ZXnNrKHrRNe+viXH1z5J47u2VqcXkpzLbjnNKz65f3deePISki9btkzvvvuuMwUXjbghat26tW8Ymstbe+3Enq7le2pSlo9fnIa61td6SA4rTJqoXtYyvSZ6VxJwXb/BfVtJmtTTNa/nRK115vjk9ric2/Ytk8399H+8BV8PUNQIyrNhWpLHr0hQ/Xq19NcnB9nBuGlFvvLntXrprZkaN/VdOxg3Yq9sYwfm117RWlu379Lr/+EDXBCmJnlu5VaCMdcx14WjfTttmNA1YGiu661Z19/QymnNY0LydUqO9S5rzV+3Tp1ne+rvA8CFbNb383Uo44hG9xqca0huQnU3U6LFMC3SC8K0njRhRc9p2Xwv+YKNMVrkTAKA/EpSN81NStBmexirLl+OyRowmjNSzPamz3Rrzy9ns15K1nDf+hI0d6D0ap9cwvU86POatb4QbE2ekZGhTz/9VG+88UaeB7O8uV5e2N81ffxfs81JM/Sokq33hkvMCA2/Ll6TznLgXVAxTw223stTQ+pHGtOSnLD8ImTC5DEb9cCMBLv0TsLYWC0ZEyz89jChcd/p0RpnlrWGmQOkt/oN0SfOfE/QPUYbB8zwrC9hrDrFj3EF4f4+eWW6Upxxj96a7KzbPYyLtWbFDtbwlmYZE5L301vRY535M/SApquvO6DO7XGZcLvfdEWP9d5G8Pv5yZDW6rugm2Y698MMk3s7Mw1zO/0WqJv3dswwxb0AUHwIyoPYk7bfLrdiWomPGdzfV3v8w/lL9PJbM/X9z2uVlLxF7338mR2Yez189y1qe0lze74J2pE/puPOgirMdUNB4uyt+iKinp5s73956i3hnglqoDf7hmtb/MZCHxABwLnmbQ3etVVH+392TEje/59P6ukPX3CmyO7o09Q0X7G5AN/jHw3R8C9jrYOFBE26zpnmJ0njh02XBs7Q5lkDrC0vABRMn6fc4XNvjRwYKSWvd7UmjtPAUfHqMj5Bm8ebJCRnfV7zL+9kB5aKV9xHzgTky8yZM7V69WrnUt6Y5c31cmNaXdvlbLKU5GqpUQuzlunq0zNWWz+b49/S/LzVW72vS9WcuAsj2C8qhOUXmyRNmhqvqAGTnADa0nuKxsWm6q1XgjWqjNMr01PVaewU3exMaTFikh6IjNdUJ2BeO3GqlkQO0KQRvhVq8thYpUx/xRWmO+KGaPTGWHWyvhZylDRRU+Mj9cAwJ4COe0VvpcZqnC+QbqnhkwYoKn6qE4Tn/rjsgD52rCv0DnI/7fs3QDPnjlALZ5K/OA21w/i5mbcDnEUE5UHMmr/ErknubUVumHIqH322xG5h/upff6d3XvyD3YLcBOamlbnXvbd5apSbdSB/lm7e5YzlX0Gumzj7K7+yJA95ThDwsMuXrNSsgPIkWVtc79P45zPnZ1mPI/C2Asuc+M83t+sSWEol3yVSturF+AxXa/J9+nh1hupfUte/dU+98qqvdK31/+kZAC44yzetskuo5NSa3BuSJ+1MVmBvLV1jOtot0hN35PNH2NumaHOOtcRNiJGgJU8VYK8/4BT7gYHhlbflqHcIOP0+6Ons9jozyyvYLRStZbyt4r2n3Hsve4cst+0SWA4i62n7puSMa33e2/O7b55T/t3ryek2AQTTW9OSEjTtNudiYXm3Fx+5t0VOi3O/7Y9/K3T/z3dmOQ+/7UpguZCLgGkZ3rZtWw0aNCjPg1k+9xblcZowLVVdhubcSt+zLXZei9t6qEvKdE0o5HY0x++CgO8ov226/f6w7ov3feJ9vQO/t5zr2MH+tFf8j4dCQH7DclNr3H2MGOwY1JayRp3NMn4lOP35r+srjfc7Hgwo1+l3rLrPPhbuPHur75jYvh/eY2nvbTtDtvfxYpM0RwtSI9Wtl//+XrMmkdLG9UFKmczTEsWqh1+D6ZZqFm29fMnrrfEkzVmQqqhuvfyD5WZNFKWNWu+3q+WEzJOGqYkzJTueUNvbmty6PC9eiu3hC+ttLZspWqnmd9g8P66oJgGd0/buoU6+++mE7YGPxcXzo0A3BdwMcNYQlAdhWoRXCCtnl1Tx2pLqCWLb/aqlLzz3zvfOM0zrc9Oq/Beng0/kXcqBI85Y/uX3uiaY7r06QnHe0iSP11PyzMAv730aOXKPbnIto/iVrrDc7DSs1Js1mjvlSzLX4w7UzY5H7/jymuBdximD4rNunQbrEmdeOz0UYd2uNww3Oxcz0/XQ49lcNw8CW5NLh7QhTWpSx9ua3BFVyfoyzdCG7c5lALhAHT6W9TvBhN79pz1pB+TukPy2y7pr/B2eciuBDmYU/HupaMVr+DDpNef0+rkDI7VoVGYYZQciozbq0VmZp+BPajJdPbOtS5uDL8corqdnHSbQz2y56Kw7x5apcZqQPNh3H0yreU3r5wpTTEg+RhtMi3pnmblNpmr4l85sm1mmn15tMtZvPRtGnf1aucAFI3GiHslDeJovH83TIsWqt1/QnqpXJ3u3RTP0aJS1bTIB5+QmzjbCmeb3w1dWW63tgnc7Y5eNSZmuR/h8503Q1yU3njMOFs3N+XXJSY7fBSYk77NAvXzfQdb7IDmwFJD1vpjbwzPflOIx1/H73rLeB86Sum2Y/T662M5muPTSS/Xss89q8uTJevzxxxUW5skUTEmJ9evX28NVV12lyMhIOyzPiTmWHflLvcxj2b4Bx3U+1vHqy9bBnSm3+bS30ZQ/X+eevnWV15svZ4blibP3qKnvWLSrJjS3jlUDQvdt8dax632e+W/6jjmt5d6VpjrXi4sN0xczAxqEXazWJytF0WoWEPS2MMl3arJM5uy2dv1GKbKJAuJlVwC9XsmpUnTgCt0hts2UTjHlWVwtvrMT2Jrcuq65G1lCbuteee6GtY3O4+PyhPsuSeu10Xc/zWMxYfv6bOutr7cerAnS1w9xzW/tLkMDFC+C8iBaNmlotyhP3LDFmSLViKhq/zc1yL22bt9p/6/pzDNMy3PTyrx8WDlnCs4/poW19NB9rp2FqFb6bXPpi1Xu1tpheuhxV2/h1jJTrS/4bat32DsG3gA6zt1jeMAyJuj++7qA9ZgyJ+7r+JVACdeoG6zxdXs8OxHb07VN5dUiyp5pCbhurjytxzNbkwNAaJr1w3y7pbkJyPMSkp9fIvXoK5kBmH9JBKd14fi5GuXa0Pd5zYQOBQgaogZopCuASTJHZtZBk2/Vt03JoYVqb017zdUcKqaXelnfXxvWeQKwxBemapG1/tdcLepjnprrV6bGu8xcv/WM0GsDIy+g0gHA2eA6O8OpVV2krcdHxavBwGEBZ8i4t0UtNWqoCUuDTPtyXs5h2HVjXffVE+Ly+c6HqCbWM50zs211n+EU07ubGuT2uuQgp++CWS+ZkmKTXN9Bwd4H1vvkCdd2fW2ydUQWrRa+67i/P1rq1u6FC/bPNxEREerfv7+qV6+ulJQUNWvWzG5Bbrz++ut6+eWXfUNunX8aSbsypBqVMl+P9u1cAbWXp0+qL5o312LfsWYA+1g1XBPcx5fto/VQRIbmf+9p+BVzSzuN8h2LWvsXbax1paXL76et5g38lvGwjoFdx9sxtzTQ9dax6aeh0qr8HPhkiKe++BxfeZbsrZ2zwK81eVG5uYf12Y8f4+qYM0mThrvqpTuh+Vv95qmHt/a4Xx1zT2CfMr2f5vXIrE8+LjZeo4N1LAoUA4LyIG7s1MH+P+uzzPIp3pbiprX5H158w65N/t7HC+w65u1+lXnSyLwly+2Q3bsO5F1U1YrOWP7l67oph5WsDL35cuZpYGaw9iMCuANqj5g65X07BmYHJUv5EktM2wjV9+48mKDb2jG6NcuOg4t7JyeQvaNiWrYHK/uSB8s36s20cN2UZccJAC5epuyKCcLdRvcaYgfjZnpuIXniDs91OzTOPLPs3HKHCQES12tD0NaFps5rZkidZ+4gxNLniQFqYDoJdJVoyZm7tEo/veo6fdsELQ2698r+O8+S3TJ2yJMS0Ekd8iSwHI5/K09cuDylVTytcXsozry2RVDCxH6/9JmupuPzWiYqh+1TNhpkabGIfCnItrCQnXpm/12QZDJv+ywBv+3MqHhnvlfA+8RpNW7OSAh2ttDF1qmnCcrLly+vxYsXa/z48dq7d6/atCn4PkafnvVUf9066xg2sEyKV7r+/rwnJN+QUyMru1GW51gz87h4pXX86Mx3+JUJnZn1mLR+7crOmFvWY2kUH9MZqF33O08dXjo10f1rvRSN3lOcDj69LcGHS5PGqpMizS6mj7seu/k+GzYg0r+OuV+dc+nmYQMUlTpdQUu8A0WMoDyImKYN1T22vd0y/I3/zLZbiRums04z3XT2aeaZ4Hz04Pt8pVhMB57eOuY9OpFM5lfHRrWdsfzL/3XDXaVQXEO+WmufDeEa9bS5b83VJN5Tqzw/gfmsVdayzWsGtAaqrKYRUvLOgPXYPyCEqWk95zIAXKDaN25j1xhfmLjUmeJhgnETkOdcbuWwFiV9bYftsMSM0BITxI2P1qt9TACSfWDuCWTHWE+0N7wzZRicmThnPC1Lva+JNbhb6+Mi0VvTTAfBKQv0cSGCRVODuue0aLtD4iJrnY6i1aKJGmij1hbgdS5Up565fBfYnca6tzP2kHufHabkSlMnZPcPzC+uTj3T09Pt/6a0yh133OFrWV5gUa202By72mVSTICdTWD+y+HcX29zhnTgMbE1eFqhe+qT+5UszbbMC3yC1g7PvsRKdiVZTAkSRTdTC3f5Eze7dbYJoD3Bt1Knq6+vVEk/vWVNMi2zs5QtybUmupur7EteH5cJy32txedquLWOjd6SLXa5mKzs58DmuR9ZZHM9oDgQlGfjN7d1twPv+BUJdutxU4bFBOJm+mtjR9qdeT4xsJ/d0twE5y+9NVNv/CfOLrny8F23+sJz5N09lzV1xvIvX9e1a3EX7LQvd/DcsrarxIpL4vdp2hZR3trEW0wHmWlp+rgQ+0EeDfSmtWNiarsFu83gturTddL1bQLD/3C1qCFt23XIuezIS+t3ALgA3H75jfb/5+dMtf+7mYA8p3Ir4z71XKePs47zXkwzNQ1aYiVOcV9KTZu7WoRaBz9+3x/2qe95ZHdUaoLv7IKLJH38WaoaDJyRY8CWNaTxtEb0amkdCQYLchLjFmhrHsoNACgYE5J7alDnFG7inLNLWqXq1ZcK0KzStOKWu1PPliZ3Dx6eJ87RnJRI9eodsNXN8l3gWUfBy6R4zowwfW8E3g/Til0XSaeeppzKBx98YLcq79y5sz0tKirK7ryzUNq3k6ePq8xSKR7l9dunrenart45dOKZ67Hq8j36wjQwy6a+ObKRpXa4h7f2dpZOLIMG0J4SJJ5W39mE2KZmuN3pZW9N9gXT3mGGHoi03mcDZljj7tbb2XTaaQna2agdxjuhen4fl8PTaaj39oKH/u6wPfv74d8qHSguBOU5GPvkw7r2itZ2XfLn//GuHZibciym/IppUW7GTYvzJ8dOtqeZYP2vTw5Sw8iCt4wOZd2aRarvpY2dS3lnrmOum3cNdJOpRx7QmUji7JUBv8QHdFSyfKVdnsUbPNt11tKsnQ9vx5tGyhoNjnfVBLdrn5syL+7b2qqH3NfJiXWbBe4d3NmxCVZ2xXu6Xua6rfs0cx+1zAFcFCLD66j/1X2Uun+Xnv7wBWdq7j78fr4++vEztagTrdvbXiBBuZxO2kb5t+6b9cgYu9a3t964p3SJOyCJ08Asp8ZnNeuRvHYI6gQurqOnxBeG+5VesU/dD+i0L3AZ+3R7a5me7vIgxdFRIXAhsz4TA/1a3yZp/LDp2hrVTbfm8UPiOQPE+/k2P6z594WA85VpiT1WXUwZlCyldqz3QVfPa+r/+nplrf0dbLvsez9dN9hXdzyn7wLTUt10BJ3ZcbMly3s0wEdD/JcPxv5RIPOH4MDHFHjZ/NhTFOWHiospu/LHP/7RV4vctDI3dcpNC3PTuaf57+3gMzez3shLp5jmzORcwvKouroxwjpWfdd/vm/9JkhXutb6vqc9x4zIjaeUyJIxrpbccUM0Oj5Wg5364aZUiq+ld8sRGhybqreGZ9bgXjtxuN7SAHn72rRLj/jV/o7T0DHx6jR4RLYBdXBxMjl5sLIrLUYMVqfU6Rpu1wo3PPXFNWCYE3Ln/rhMJ6GTXLtwdkkYa/44X0mYlho+OFYp04drku9mJmq4KQXjPJbs7kdmTXXrsbdu7dQ0D3LZWl8v63LmcwXkD0F5Lky5lacfu08toxt4wvH5S+zW43Zobo2bFuemTvmgu3rbwbppYY6Cm9Kno66IrOFcyp1Z1lwnv/oMcnrs9tZaM6eU7aoZ0AlJuCbckK7e3mVMkNzX3ZO3aeXdXNfb9eGcZV5O042Pu5cJdlvrpCytvLP3xUzv9ZzT3vL4i37iznTJ27I9kDld7/F6Svat27pPfo8NAC5spia5Cbxn/fCZHZabkio5efvrDzR61gRVKldRU+/5izP1wmBKa8wdKOd0eM8wXGO1eaEr9HI6xFw0yrvMPPU2pRqc2dnz1JD1XKef5nSfkW3dYrsDUbuGrWf5RzTYv/SKOXXfuk25atk+okl+nXl6Whf6r6dRnwXqNYtSEIBPTDO/z5HdH0CTgM98fth9HaT6bUN8w3kcPJ7vNm3apM8//zzPg1k+bzytsCc1ma6efq+X2UYHdsDqL0unnnZJlczyJ5nrmRFQpimH7wLTynx8rOv7xRr6JKt3LjXu3cv3/Kyb5mZ5/158nXqmpaVp/fr19vDSSy/ZYblpYW469zT/n3jiCWfJ3LiPLVdq/iXtsumw04Tl1vGqadwVtDyLK0z3rW+h/l472vM+so4Zp8bK1bfXHt1E6ZU8aTFirmYO2KjR3lIoY6RxAS273W6ekqBx0ZmlU/ou6KaZc10heMsRmjNjgDb6an9bKxyb4FfHO09ybJltWqaPVbRdrsXcRj8t6DbDr3PQvDyuBb776DyOwMdt1zGP1lv9nOX6TVe032PJej9MJ6UJeaq/DhReiTMWZ7xI7dy2XdtPhim6cYQKHR0fSdNP2zNUtno9tYhwpp0DplZ5YvIWbUndZV+uGVFVDerVoQV5ETt47LiGzFqqmT/lvLNoWpKbkLxKubLOlCK0fKWazpQmTGjH6acAcIEy4fiQ9/6oFZsTVDmsooZ1ud+uPR5T11OuK3XfTi235pmQ3HTwWa9qLU259y+++Tg77JIPJtindjaAi8SOHTs0c+ZM7dy505mSuzp16qhv376qW7euM6V4mG3upCbZ/+h5fjEdRE9V81lzfa3bLzRDhw51xrKaMGGC/d+E5r169bI7+DTjGzZs0OTJk+15ODcOHndGENKqFEPUhPNfsQXlR3bv0toDZ1Stdk1FVynlTC2IU9q/c482HiqhmvVrqz6lv0PGgvWpev/HDVq6eZdSDhyxp0VVrWh33Glqkuev3Eo+nedBuel5vHe8p5NZf2F66PFrA1rGA0BoM0H4K4ve0eFjnu+SYEyplqFd7leVsErOFJwdJgTxdABKi3EAF4OFCxeqRIkSzqWCueGGG5yxYvDREDUaJU26QOrRX+g/puYUlD/77LOqUKGC3nnnHbujT4Ly8wdBOQyC8tBUbEG5Th/V5i37lXaqhMqVK6MyBdxXOHPyhI6eOKMylasppk4FFSZyB/KMFuUAcNFZmLhUiTs2aPmmVfblmLpN1LJOU3Vt1ZGA/GxInKhOLzXTEl/YYerp9tOrGhDklHsAuDCNHj3aGcuqWrVqCg8P1759+7R//35nalbjxo1zxnChyykov/7663XnnXc6l2SXZDH1yw2C8nOLoBwGQXloKr6g3Dh1RCnbD+uXY6d0uoC3UqJEKVWoWklNalZUaWcaUOwIygEAKGKe1uOLnEu2KEJyABeXjRs32kNBRUdH2wMuDjkF5YapwRwVFWXXKzcdfnoRlJ9bBOUwCMpDU/EG5QAAAAAAACHo+eefV2pqqnMpbyIjI/X00087l3AuEJTDICgPTQTlAAAAAAAARcy0FF+1apX27t3rTMlZ9erV7Vrl5cuXd6bgXCAoh0FQHpoIygEAAAAAAAALQTkMgvLQVNL5DwAAAAAAAABASCIoBwAAAAAAAACENIJyAAAAAAAAAEBIIygHAAAAAAAAAIQ0gnIAAAAAAAAAQEgjKAcAAAAAAAAAhDSCcgAAAAAAAABASCtxxuKM50nq3oPOGAAAAAAAAABcXCKrV3HGEEryHZQDAAAAAAAAAHAxofQKAAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQhpBOQAAAAAAAAAgpBGUAwAAAAAAAABCGkE5AAAAAAAAACCkEZQDAAAAAAAAAEIaQTkAAAAAAAAAIKQRlAMAAAAAAAAAQpj0/2JpcEYq7fZFAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_id: 348474030925203123\n"
     ]
    }
   ],
   "source": [
    "exp_id = mlflow.create_experiment(name=\"Modelo final\", artifact_location=\"mlruns/\")\n",
    "print(f\"exp_id: {exp_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"Modelo Final\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3665 - accuracy: 0.0065\n",
      "Epoch 1: val_loss improved from inf to 7.33583, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 37s 495ms/step - loss: 7.3665 - accuracy: 0.0065 - val_loss: 7.3358 - val_accuracy: 0.0050\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1977 - accuracy: 0.0065\n",
      "Epoch 2: val_loss improved from 7.33583 to 7.27770, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 474ms/step - loss: 7.1977 - accuracy: 0.0065 - val_loss: 7.2777 - val_accuracy: 0.0050\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1524 - accuracy: 0.0054\n",
      "Epoch 3: val_loss did not improve from 7.27770\n",
      "58/58 [==============================] - 27s 462ms/step - loss: 7.1524 - accuracy: 0.0054 - val_loss: 7.3393 - val_accuracy: 0.0050\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1420 - accuracy: 0.0032\n",
      "Epoch 4: val_loss did not improve from 7.27770\n",
      "58/58 [==============================] - 26s 460ms/step - loss: 7.1420 - accuracy: 0.0032 - val_loss: 7.2807 - val_accuracy: 0.0050\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1403 - accuracy: 0.0022\n",
      "Epoch 5: val_loss improved from 7.27770 to 7.25858, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 465ms/step - loss: 7.1403 - accuracy: 0.0022 - val_loss: 7.2586 - val_accuracy: 0.0050\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1493 - accuracy: 0.0032\n",
      "Epoch 6: val_loss improved from 7.25858 to 7.24772, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 467ms/step - loss: 7.1493 - accuracy: 0.0032 - val_loss: 7.2477 - val_accuracy: 0.0050\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1352 - accuracy: 0.0011\n",
      "Epoch 7: val_loss improved from 7.24772 to 7.24254, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 465ms/step - loss: 7.1352 - accuracy: 0.0011 - val_loss: 7.2425 - val_accuracy: 0.0050\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0812 - accuracy: 0.0022\n",
      "Epoch 8: val_loss did not improve from 7.24254\n",
      "58/58 [==============================] - 26s 457ms/step - loss: 7.0812 - accuracy: 0.0022 - val_loss: 7.2631 - val_accuracy: 0.0050\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0246 - accuracy: 0.0043\n",
      "Epoch 9: val_loss did not improve from 7.24254\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.0246 - accuracy: 0.0043 - val_loss: 7.3090 - val_accuracy: 0.0050\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0952 - accuracy: 0.0054\n",
      "Epoch 10: val_loss did not improve from 7.24254\n",
      "58/58 [==============================] - 26s 459ms/step - loss: 7.0952 - accuracy: 0.0054 - val_loss: 7.2572 - val_accuracy: 0.0050\n",
      "Epoch 1/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 7.0237 - accuracy: 0.0085\n",
      "Epoch 1: val_loss improved from inf to 7.23104, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 137s 269ms/step - loss: 7.0237 - accuracy: 0.0085 - val_loss: 7.2310 - val_accuracy: 0.0052\n",
      "Epoch 2/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 6.7367 - accuracy: 0.0194\n",
      "Epoch 2: val_loss improved from 7.23104 to 7.08260, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 6.7367 - accuracy: 0.0194 - val_loss: 7.0826 - val_accuracy: 0.0206\n",
      "Epoch 3/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 6.3943 - accuracy: 0.0356\n",
      "Epoch 3: val_loss improved from 7.08260 to 6.40562, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 6.3943 - accuracy: 0.0356 - val_loss: 6.4056 - val_accuracy: 0.0538\n",
      "Epoch 4/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.9938 - accuracy: 0.0728\n",
      "Epoch 4: val_loss improved from 6.40562 to 6.11355, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 5.9938 - accuracy: 0.0728 - val_loss: 6.1136 - val_accuracy: 0.0883\n",
      "Epoch 5/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.6641 - accuracy: 0.1083\n",
      "Epoch 5: val_loss improved from 6.11355 to 6.03189, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 5.6641 - accuracy: 0.1083 - val_loss: 6.0319 - val_accuracy: 0.0969\n",
      "Epoch 6/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.3816 - accuracy: 0.1358\n",
      "Epoch 6: val_loss improved from 6.03189 to 5.65971, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 5.3816 - accuracy: 0.1358 - val_loss: 5.6597 - val_accuracy: 0.1358\n",
      "Epoch 7/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.0982 - accuracy: 0.1648\n",
      "Epoch 7: val_loss did not improve from 5.65971\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 5.0982 - accuracy: 0.1648 - val_loss: 6.1108 - val_accuracy: 0.1288\n",
      "Epoch 8/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.8652 - accuracy: 0.1945\n",
      "Epoch 8: val_loss improved from 5.65971 to 5.09872, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 4.8652 - accuracy: 0.1945 - val_loss: 5.0987 - val_accuracy: 0.1922\n",
      "Epoch 9/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.6618 - accuracy: 0.2156\n",
      "Epoch 9: val_loss did not improve from 5.09872\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 4.6618 - accuracy: 0.2156 - val_loss: 5.3274 - val_accuracy: 0.1818\n",
      "Epoch 10/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.4256 - accuracy: 0.2402\n",
      "Epoch 10: val_loss improved from 5.09872 to 4.97595, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 4.4256 - accuracy: 0.2402 - val_loss: 4.9760 - val_accuracy: 0.2155\n",
      "Epoch 11/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.2566 - accuracy: 0.2587\n",
      "Epoch 11: val_loss improved from 4.97595 to 4.63907, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 4.2566 - accuracy: 0.2587 - val_loss: 4.6391 - val_accuracy: 0.2497\n",
      "Epoch 12/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.0411 - accuracy: 0.2789\n",
      "Epoch 12: val_loss improved from 4.63907 to 4.50783, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 4.0411 - accuracy: 0.2789 - val_loss: 4.5078 - val_accuracy: 0.2682\n",
      "Epoch 13/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.8754 - accuracy: 0.3019\n",
      "Epoch 13: val_loss improved from 4.50783 to 4.33640, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 3.8754 - accuracy: 0.3019 - val_loss: 4.3364 - val_accuracy: 0.3022\n",
      "Epoch 14/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.7198 - accuracy: 0.3122\n",
      "Epoch 14: val_loss improved from 4.33640 to 4.17111, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 3.7198 - accuracy: 0.3122 - val_loss: 4.1711 - val_accuracy: 0.3077\n",
      "Epoch 15/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.5650 - accuracy: 0.3319\n",
      "Epoch 15: val_loss improved from 4.17111 to 3.91591, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 3.5650 - accuracy: 0.3319 - val_loss: 3.9159 - val_accuracy: 0.3429\n",
      "Epoch 16/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.4139 - accuracy: 0.3497\n",
      "Epoch 16: val_loss did not improve from 3.91591\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 3.4139 - accuracy: 0.3497 - val_loss: 3.9616 - val_accuracy: 0.3426\n",
      "Epoch 17/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.2575 - accuracy: 0.3727\n",
      "Epoch 17: val_loss improved from 3.91591 to 3.80656, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 3.2575 - accuracy: 0.3727 - val_loss: 3.8066 - val_accuracy: 0.3586\n",
      "Epoch 18/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.1093 - accuracy: 0.3871\n",
      "Epoch 18: val_loss improved from 3.80656 to 3.48625, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 3.1093 - accuracy: 0.3871 - val_loss: 3.4863 - val_accuracy: 0.3985\n",
      "Epoch 19/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.9772 - accuracy: 0.4089\n",
      "Epoch 19: val_loss improved from 3.48625 to 3.24510, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 2.9772 - accuracy: 0.4089 - val_loss: 3.2451 - val_accuracy: 0.4516\n",
      "Epoch 20/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.8611 - accuracy: 0.4170\n",
      "Epoch 20: val_loss did not improve from 3.24510\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 2.8611 - accuracy: 0.4170 - val_loss: 3.3301 - val_accuracy: 0.4395\n",
      "Epoch 21/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.7378 - accuracy: 0.4395\n",
      "Epoch 21: val_loss improved from 3.24510 to 3.04559, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 2.7378 - accuracy: 0.4395 - val_loss: 3.0456 - val_accuracy: 0.4824\n",
      "Epoch 22/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.5948 - accuracy: 0.4605\n",
      "Epoch 22: val_loss did not improve from 3.04559\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 2.5948 - accuracy: 0.4605 - val_loss: 3.0792 - val_accuracy: 0.4756\n",
      "Epoch 23/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.4876 - accuracy: 0.4781\n",
      "Epoch 23: val_loss did not improve from 3.04559\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 2.4876 - accuracy: 0.4781 - val_loss: 3.2362 - val_accuracy: 0.4453\n",
      "Epoch 24/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.3721 - accuracy: 0.4975\n",
      "Epoch 24: val_loss improved from 3.04559 to 2.70165, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 2.3721 - accuracy: 0.4975 - val_loss: 2.7017 - val_accuracy: 0.5398\n",
      "Epoch 25/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.2791 - accuracy: 0.5122\n",
      "Epoch 25: val_loss improved from 2.70165 to 2.47244, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 2.2791 - accuracy: 0.5122 - val_loss: 2.4724 - val_accuracy: 0.5915\n",
      "Epoch 26/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.1655 - accuracy: 0.5214\n",
      "Epoch 26: val_loss did not improve from 2.47244\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 2.1655 - accuracy: 0.5214 - val_loss: 2.5454 - val_accuracy: 0.5921\n",
      "Epoch 27/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.0497 - accuracy: 0.5439\n",
      "Epoch 27: val_loss improved from 2.47244 to 2.30631, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 2.0497 - accuracy: 0.5439 - val_loss: 2.3063 - val_accuracy: 0.6341\n",
      "Epoch 28/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.9562 - accuracy: 0.5613\n",
      "Epoch 28: val_loss did not improve from 2.30631\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.9562 - accuracy: 0.5613 - val_loss: 2.4234 - val_accuracy: 0.6119\n",
      "Epoch 29/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.8525 - accuracy: 0.5877\n",
      "Epoch 29: val_loss did not improve from 2.30631\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.8525 - accuracy: 0.5877 - val_loss: 2.3109 - val_accuracy: 0.6145\n",
      "Epoch 30/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.7888 - accuracy: 0.5898\n",
      "Epoch 30: val_loss improved from 2.30631 to 1.97522, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 1.7888 - accuracy: 0.5898 - val_loss: 1.9752 - val_accuracy: 0.7216\n",
      "Epoch 31/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.7413 - accuracy: 0.6004\n",
      "Epoch 31: val_loss did not improve from 1.97522\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.7413 - accuracy: 0.6004 - val_loss: 2.3109 - val_accuracy: 0.6425\n",
      "Epoch 32/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.6525 - accuracy: 0.6182\n",
      "Epoch 32: val_loss improved from 1.97522 to 1.86733, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 1.6525 - accuracy: 0.6182 - val_loss: 1.8673 - val_accuracy: 0.7362\n",
      "Epoch 33/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.6394 - accuracy: 0.6131\n",
      "Epoch 33: val_loss did not improve from 1.86733\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.6394 - accuracy: 0.6131 - val_loss: 1.9755 - val_accuracy: 0.7245\n",
      "Epoch 34/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.5476 - accuracy: 0.6329\n",
      "Epoch 34: val_loss did not improve from 1.86733\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.5476 - accuracy: 0.6329 - val_loss: 2.0538 - val_accuracy: 0.7054\n",
      "Epoch 35/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.4879 - accuracy: 0.6514\n",
      "Epoch 35: val_loss did not improve from 1.86733\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.4879 - accuracy: 0.6514 - val_loss: 2.1014 - val_accuracy: 0.7083\n",
      "Epoch 36/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.4375 - accuracy: 0.6543\n",
      "Epoch 36: val_loss improved from 1.86733 to 1.83149, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 1.4375 - accuracy: 0.6543 - val_loss: 1.8315 - val_accuracy: 0.7508\n",
      "Epoch 37/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3593 - accuracy: 0.6731\n",
      "Epoch 37: val_loss did not improve from 1.83149\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.3593 - accuracy: 0.6731 - val_loss: 1.8872 - val_accuracy: 0.7514\n",
      "Epoch 38/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3601 - accuracy: 0.6780\n",
      "Epoch 38: val_loss improved from 1.83149 to 1.76708, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 1.3601 - accuracy: 0.6780 - val_loss: 1.7671 - val_accuracy: 0.7652\n",
      "Epoch 39/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3005 - accuracy: 0.6847\n",
      "Epoch 39: val_loss improved from 1.76708 to 1.59395, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 1.3005 - accuracy: 0.6847 - val_loss: 1.5939 - val_accuracy: 0.8070\n",
      "Epoch 40/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.2296 - accuracy: 0.6970\n",
      "Epoch 40: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.2296 - accuracy: 0.6970 - val_loss: 1.6536 - val_accuracy: 0.7981\n",
      "Epoch 41/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.7075\n",
      "Epoch 41: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.1972 - accuracy: 0.7075 - val_loss: 1.5951 - val_accuracy: 0.7900\n",
      "Epoch 42/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1730 - accuracy: 0.7147\n",
      "Epoch 42: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.1730 - accuracy: 0.7147 - val_loss: 1.8562 - val_accuracy: 0.7412\n",
      "Epoch 43/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1792 - accuracy: 0.7072\n",
      "Epoch 43: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.1792 - accuracy: 0.7072 - val_loss: 1.5990 - val_accuracy: 0.8033\n",
      "Epoch 44/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1303 - accuracy: 0.7181\n",
      "Epoch 44: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.1303 - accuracy: 0.7181 - val_loss: 1.6387 - val_accuracy: 0.7976\n",
      "Epoch 45/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1038 - accuracy: 0.7248\n",
      "Epoch 45: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.1038 - accuracy: 0.7248 - val_loss: 1.7283 - val_accuracy: 0.7916\n",
      "Epoch 46/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0628 - accuracy: 0.7296\n",
      "Epoch 46: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.0628 - accuracy: 0.7296 - val_loss: 1.6310 - val_accuracy: 0.8015\n",
      "Epoch 47/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0255 - accuracy: 0.7390\n",
      "Epoch 47: val_loss did not improve from 1.59395\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.0255 - accuracy: 0.7390 - val_loss: 1.6069 - val_accuracy: 0.8117\n",
      "Epoch 48/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0165 - accuracy: 0.7413\n",
      "Epoch 48: val_loss improved from 1.59395 to 1.51265, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 1.0165 - accuracy: 0.7413 - val_loss: 1.5126 - val_accuracy: 0.8389\n",
      "Epoch 49/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0085 - accuracy: 0.7445\n",
      "Epoch 49: val_loss did not improve from 1.51265\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 1.0085 - accuracy: 0.7445 - val_loss: 1.7242 - val_accuracy: 0.7845\n",
      "Epoch 50/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9520 - accuracy: 0.7587\n",
      "Epoch 50: val_loss did not improve from 1.51265\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.9520 - accuracy: 0.7587 - val_loss: 1.5393 - val_accuracy: 0.8208\n",
      "Epoch 51/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9661 - accuracy: 0.7557\n",
      "Epoch 51: val_loss improved from 1.51265 to 1.43071, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 0.9661 - accuracy: 0.7557 - val_loss: 1.4307 - val_accuracy: 0.8485\n",
      "Epoch 52/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.7668\n",
      "Epoch 52: val_loss did not improve from 1.43071\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.9260 - accuracy: 0.7668 - val_loss: 1.6062 - val_accuracy: 0.8073\n",
      "Epoch 53/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9005 - accuracy: 0.7736\n",
      "Epoch 53: val_loss did not improve from 1.43071\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.9005 - accuracy: 0.7736 - val_loss: 1.7584 - val_accuracy: 0.7952\n",
      "Epoch 54/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8652 - accuracy: 0.7840\n",
      "Epoch 54: val_loss did not improve from 1.43071\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.8652 - accuracy: 0.7840 - val_loss: 1.6501 - val_accuracy: 0.8164\n",
      "Epoch 55/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.7695\n",
      "Epoch 55: val_loss improved from 1.43071 to 1.39554, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 0.9048 - accuracy: 0.7695 - val_loss: 1.3955 - val_accuracy: 0.8472\n",
      "Epoch 56/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8361 - accuracy: 0.7870\n",
      "Epoch 56: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 265ms/step - loss: 0.8361 - accuracy: 0.7870 - val_loss: 1.4948 - val_accuracy: 0.8308\n",
      "Epoch 57/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8647 - accuracy: 0.7743\n",
      "Epoch 57: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 265ms/step - loss: 0.8647 - accuracy: 0.7743 - val_loss: 1.8339 - val_accuracy: 0.7673\n",
      "Epoch 58/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8402 - accuracy: 0.7850\n",
      "Epoch 58: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.8402 - accuracy: 0.7850 - val_loss: 1.4371 - val_accuracy: 0.8574\n",
      "Epoch 59/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8163 - accuracy: 0.7925\n",
      "Epoch 59: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.8163 - accuracy: 0.7925 - val_loss: 1.6592 - val_accuracy: 0.8148\n",
      "Epoch 60/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7584 - accuracy: 0.8010\n",
      "Epoch 60: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7584 - accuracy: 0.8010 - val_loss: 1.4008 - val_accuracy: 0.8595\n",
      "Epoch 61/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7728 - accuracy: 0.7990\n",
      "Epoch 61: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7728 - accuracy: 0.7990 - val_loss: 1.4807 - val_accuracy: 0.8569\n",
      "Epoch 62/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7904 - accuracy: 0.7941\n",
      "Epoch 62: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7904 - accuracy: 0.7941 - val_loss: 1.4481 - val_accuracy: 0.8624\n",
      "Epoch 63/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.7961\n",
      "Epoch 63: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7824 - accuracy: 0.7961 - val_loss: 1.6276 - val_accuracy: 0.8279\n",
      "Epoch 64/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7585 - accuracy: 0.8042\n",
      "Epoch 64: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7585 - accuracy: 0.8042 - val_loss: 1.5008 - val_accuracy: 0.8449\n",
      "Epoch 65/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7264 - accuracy: 0.8106\n",
      "Epoch 65: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7264 - accuracy: 0.8106 - val_loss: 1.4911 - val_accuracy: 0.8517\n",
      "Epoch 66/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7187 - accuracy: 0.8171\n",
      "Epoch 66: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7187 - accuracy: 0.8171 - val_loss: 1.5134 - val_accuracy: 0.8582\n",
      "Epoch 67/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7235 - accuracy: 0.8065\n",
      "Epoch 67: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7235 - accuracy: 0.8065 - val_loss: 1.4162 - val_accuracy: 0.8631\n",
      "Epoch 68/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7397 - accuracy: 0.8030\n",
      "Epoch 68: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7397 - accuracy: 0.8030 - val_loss: 1.4288 - val_accuracy: 0.8558\n",
      "Epoch 69/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.8125\n",
      "Epoch 69: val_loss did not improve from 1.39554\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.7226 - accuracy: 0.8125 - val_loss: 1.4756 - val_accuracy: 0.8663\n",
      "Epoch 70/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6726 - accuracy: 0.8263\n",
      "Epoch 70: val_loss improved from 1.39554 to 1.36810, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 0.6726 - accuracy: 0.8263 - val_loss: 1.3681 - val_accuracy: 0.8739\n",
      "Epoch 71/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6858 - accuracy: 0.8171\n",
      "Epoch 71: val_loss did not improve from 1.36810\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6858 - accuracy: 0.8171 - val_loss: 1.4340 - val_accuracy: 0.8723\n",
      "Epoch 72/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6861 - accuracy: 0.8216\n",
      "Epoch 72: val_loss did not improve from 1.36810\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6861 - accuracy: 0.8216 - val_loss: 1.4182 - val_accuracy: 0.8410\n",
      "Epoch 73/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6744 - accuracy: 0.8240\n",
      "Epoch 73: val_loss did not improve from 1.36810\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6744 - accuracy: 0.8240 - val_loss: 1.3684 - val_accuracy: 0.8757\n",
      "Epoch 74/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6589 - accuracy: 0.8276\n",
      "Epoch 74: val_loss did not improve from 1.36810\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6589 - accuracy: 0.8276 - val_loss: 1.4454 - val_accuracy: 0.8710\n",
      "Epoch 75/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6855 - accuracy: 0.8207\n",
      "Epoch 75: val_loss improved from 1.36810 to 1.34236, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 0.6855 - accuracy: 0.8207 - val_loss: 1.3424 - val_accuracy: 0.8689\n",
      "Epoch 76/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.8315\n",
      "Epoch 76: val_loss improved from 1.34236 to 1.31068, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 125s 265ms/step - loss: 0.6435 - accuracy: 0.8315 - val_loss: 1.3107 - val_accuracy: 0.8741\n",
      "Epoch 77/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6260 - accuracy: 0.8319\n",
      "Epoch 77: val_loss did not improve from 1.31068\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6260 - accuracy: 0.8319 - val_loss: 1.3590 - val_accuracy: 0.8723\n",
      "Epoch 78/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.8317\n",
      "Epoch 78: val_loss did not improve from 1.31068\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6169 - accuracy: 0.8317 - val_loss: 1.4430 - val_accuracy: 0.8686\n",
      "Epoch 79/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.8364\n",
      "Epoch 79: val_loss did not improve from 1.31068\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6162 - accuracy: 0.8364 - val_loss: 1.4169 - val_accuracy: 0.8731\n",
      "Epoch 80/80\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5888 - accuracy: 0.8461\n",
      "Epoch 80: val_loss did not improve from 1.31068\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.5888 - accuracy: 0.8461 - val_loss: 1.4021 - val_accuracy: 0.8652\n",
      "1/1 [==============================] - 1s 936ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://edacfdbd-e8bb-4507-ac92-73d77ae83951/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://edacfdbd-e8bb-4507-ac92-73d77ae83951/assets\n",
      "2023/12/06 16:05:34 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmpo53vviz0\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
      "d:\\Conda\\envs\\tf310\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x23419f11810>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs_warm = 10\n",
    "epochs_train = 80\n",
    "\n",
    "model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                weights='imagenet',\n",
    "                                                input_shape=(250,250,3))\n",
    "\n",
    "# Congelamos el extractor de características (Transfer Learning)\n",
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "# Creamos una capa de pooling para consolidar los feature maps de salida en 1024 valores\n",
    "pool = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
    "# Agregamos una capa densa\n",
    "dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
    "# Agregamos dropout para regularización\n",
    "drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "# Agregamos una capa de salida\n",
    "dense2 = tf.keras.layers.Dense(units=train_generator.num_classes, activation=\"softmax\")(drop1)\n",
    "# Definimos nuestro modelo de transfer learning\n",
    "ft_model = tf.keras.models.Model(inputs=[model.input], outputs=[dense2])\n",
    "# Compilamos el modelo\n",
    "ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "# ft_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# Definimos el callback\n",
    "best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"warming_up_{epochs_warm}_epochs_warm.h5\",\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                verbose=True,\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,\n",
    "                                                mode=\"min\")\n",
    "\n",
    "# Entrenamos el modelo\n",
    "hist_ft = ft_model.fit(x=train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=epochs_warm,\n",
    "                    steps_per_epoch=train_generator.samples//128,\n",
    "                    callbacks=[best_callback])\n",
    "\n",
    "########################################## re entrenamiento ##########################################\n",
    "\n",
    "# Hacemos entrenables todas las capas\n",
    "for layer in ft_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Disminuímos el learning rate\n",
    "ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "# Cargamos los pesos del calentamiento\n",
    "ft_model.load_weights(f\"warming_up_{epochs_warm}_epochs_warm.h5\")\n",
    "\n",
    "# Definimos el callback\n",
    "best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"fine_tuning.h5\",\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                verbose=True,\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,\n",
    "                                                mode=\"min\")\n",
    "\n",
    "# Entrenamos el modelo\n",
    "hist_ft = ft_model.fit(x=train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=epochs_train,\n",
    "                    steps_per_epoch=train_generator.samples//16,\n",
    "                    callbacks=[best_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mapeo_indices(listado):\n",
    "\n",
    "    listado_clases = list(validation_generator.class_indices.keys())\n",
    "\n",
    "    true_index = np.argmax(listado,axis=1)\n",
    "    salida_real = [listado_clases[i] for i in true_index]\n",
    "\n",
    "    return salida_real\n",
    "\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(validation_generator)):\n",
    "    X_val_batch, y_val_batch = validation_generator.next()\n",
    "    y_pred_batch = ft_model.predict(X_val_batch)\n",
    "    y_true += mapeo_indices(y_val_batch)\n",
    "    y_pred += mapeo_indices(y_pred_batch)\n",
    "\n",
    "\n",
    "mlflow.log_metrics({\n",
    "    \"accuracy\": accuracy_score(y_true, y_pred)\n",
    "    })\n",
    "\n",
    "\n",
    "mlflow.sklearn.log_model(ft_model, f\"model_{epochs_warm}_epochs_warm_{epochs_train}_epochs_train\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c913c554-ecc1-45b6-b8f7-f487870bf7a0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c913c554-ecc1-45b6-b8f7-f487870bf7a0/assets\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0f1e312c-bcb1-4f77-9d26-130573e7add4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0f1e312c-bcb1-4f77-9d26-130573e7add4/assets\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas del modelo final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2344de6c2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(hist_ft.history[\"loss\"], \"k\", label=\"fine tuning - entrenamiento\")\n",
    "plt.plot(hist_ft.history[\"val_loss\"], \"k--\", label=\"fine tuning - validación\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.title(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(hist_ft.history[\"accuracy\"], \"k\", label=\"fine tuning - entrenamiento\")\n",
    "plt.plot(hist_ft.history[\"val_accuracy\"], \"k--\", label=\"fine tuning - validación\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardado del modelo final y el histórico de entrenamiento, ya que el modelo es muy pesado se subió a google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "######### Modelo final ###########  \n",
    "ft_model.save('Modelo_final/ft_model.h5')\n",
    "\n",
    "######### Historia entrenamiento ###########  \n",
    "with open('Modelo_final/hist_ft', 'wb') as file_pi:\n",
    "    pickle.dump(hist_ft.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# código de carga \n",
    "\n",
    "with open('Modelo_final/hist_ft', \"rb\") as file_pi:\n",
    "    history = pickle.load(file_pi)\n",
    "\n",
    "from keras.models import load_model\n",
    "ft_model_1 = load_model('Modelo_final/ft_model.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 250, 250, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 256, 256, 3)  0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 125, 125, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 125, 125, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 125, 125, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 127, 127, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 63, 63, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 63, 63, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 63, 63, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 63, 63, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 63, 63, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 63, 63, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 63, 63, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 63, 63, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 8, 8, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           65568       ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 32)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1680)         55440       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,708,720\n",
      "Trainable params: 23,655,600\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ft_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23433a15630>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history[\"loss\"], \"k\", label=\"fine tuning - entrenamiento\")\n",
    "plt.plot(history[\"val_loss\"], \"k--\", label=\"fine tuning - validación\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.title(\"Pérdida\")\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.plot(history[\"accuracy\"], \"k\", label=\"fine tuning - entrenamiento\")\n",
    "plt.plot(history[\"val_accuracy\"], \"k--\", label=\"fine tuning - validación\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
