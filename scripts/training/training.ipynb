{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generadores de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7534 images belonging to 1680 classes.\n",
      "Found 3829 images belonging to 1680 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_folder = \"../data_acquisition/train\"\n",
    "test_folder = \"../data_acquisition/test\"\n",
    "\n",
    "\n",
    "image_size = (250, 250)\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                rotation_range=5,\n",
    "                                                                width_shift_range=0.1,\n",
    "                                                                height_shift_range=0.1,\n",
    "                                                                shear_range=0.01,\n",
    "                                                                zoom_range=0.01,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                fill_mode='constant')\n",
    "\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "# Creación de los generadores\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory = train_folder + \"/\",\n",
    "                                                    target_size = image_size,\n",
    "                                                    batch_size = batch_size,\n",
    "                                                    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(directory = test_folder + \"/\",\n",
    "                                                       target_size = image_size,\n",
    "                                                       batch_size = batch_size,\n",
    "                                                       class_mode = 'categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se experimentará con un modelo preentrenado y con el mismo modelo agregando fine tunning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ya existe el subdirectorio o el archivo mlruns.\n"
     ]
    }
   ],
   "source": [
    "!mkdir mlruns\n",
    "\n",
    "# command = \"\"\"mlflow server --backend-store-uri sqlite:///tracking.db --default-artifact-root file:mlruns -p 5000 \"\"\"\n",
    "\n",
    "token = \"2YBABEo5mjbSd6lnmoNbFAGJbsf_5Rc66NjJRLbinDmw12UTr\" \n",
    "os.environ[\"NGROK_TOKEN\"] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: C:\\Users\\andre\\AppData\\Local/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2YBABEo5mjbSd6lnmoNbFAGJbsf_5Rc66NjJRLbinDmw12UTr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-12-04T22:55:11-0500 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=C:\\\\Users\\\\andre\\\\AppData\\\\Local/ngrok/ngrok.yml legacy_path=C:\\\\Users\\\\andre\\\\.ngrok2\\\\ngrok.yml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<NgrokTunnel: \"https://c281-2800-484-6173-2700-94d0-a25f-966a-4e52.ngrok-free.app\" -> \"http://localhost:5000\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2023-12-04T22:55:26-0500 lvl=warn msg=\"failed to open private leg\" id=cd3caa9ffd32 privaddr=localhost:5000 err=\"dial tcp [::1]:5000: connectex: No connection could be made because the target machine actively refused it.\"\n",
      "t=2023-12-04T22:55:27-0500 lvl=warn msg=\"failed to open private leg\" id=9e2da9eda364 privaddr=localhost:5000 err=\"dial tcp [::1]:5000: connectex: No connection could be made because the target machine actively refused it.\"\n"
     ]
    }
   ],
   "source": [
    "# taskkill /f /im ngrok.exe\n",
    "from pyngrok import ngrok\n",
    "ngrok.connect(5000, \"http\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo preentrenado + fine tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 250, 250, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 256, 256, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 125, 125, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 125, 125, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 125, 125, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 127, 127, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 63, 63, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 63, 63, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 63, 63, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 63, 63, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 63, 63, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 63, 63, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 63, 63, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 63, 63, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 63, 63, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 63, 63, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 63, 63, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 63, 63, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 63, 63, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 63, 63, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 8, 8, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                weights='imagenet',\n",
    "                                                input_shape=(250,250,3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento(epochs=10):\n",
    "        \n",
    "    model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=(250,250,3))\n",
    "\n",
    "    # Congelamos el extractor de características (Transfer Learning)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "    # Creamos una capa de pooling para consolidar los feature maps de salida en 1024 valores\n",
    "    pool = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
    "    # Agregamos una capa densa\n",
    "    dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
    "    # Agregamos dropout para regularización\n",
    "    drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "    # Agregamos una capa de salida\n",
    "    dense2 = tf.keras.layers.Dense(units=train_generator.num_classes, activation=\"softmax\")(drop1)\n",
    "    # Definimos nuestro modelo de transfer learning\n",
    "    ft_model = tf.keras.models.Model(inputs=[model.input], outputs=[dense2])\n",
    "    # Compilamos el modelo\n",
    "    ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "    # ft_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Definimos el callback\n",
    "    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"warming_up_{epochs}_epochs.h5\",\n",
    "                                                    monitor=\"val_loss\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    hist_ft = ft_model.fit(x=train_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=train_generator.samples//128,\n",
    "                        callbacks=[best_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def mapeo_indices(listado):\n",
    "\n",
    "        listado_clases = list(validation_generator.class_indices.keys())\n",
    "\n",
    "        true_index = np.argmax(listado,axis=1)\n",
    "        salida_real = [listado_clases[i] for i in true_index]\n",
    "\n",
    "        return salida_real\n",
    "\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(validation_generator)):\n",
    "        X_val_batch, y_val_batch = validation_generator.next()\n",
    "        y_pred_batch = ft_model.predict(X_val_batch)\n",
    "        y_true += mapeo_indices(y_val_batch)\n",
    "        y_pred += mapeo_indices(y_pred_batch)\n",
    "\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
    "        })\n",
    "\n",
    "\n",
    "    mlflow.sklearn.log_model(ft_model, f\"model_{epochs}_epochs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición del experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_id: 955390070555724847\n"
     ]
    }
   ],
   "source": [
    "# mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "exp_id = mlflow.create_experiment(name=\"modelo_base_con_fine\", artifact_location=\"mlruns/\")\n",
    "print(f\"exp_id: {exp_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3932 - accuracy: 0.0032\n",
      "Epoch 1: val_loss improved from inf to 7.33877, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 36s 509ms/step - loss: 7.3932 - accuracy: 0.0032 - val_loss: 7.3388 - val_accuracy: 0.0034\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3048 - accuracy: 0.0043\n",
      "Epoch 2: val_loss improved from 7.33877 to 7.29551, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 27s 462ms/step - loss: 7.3048 - accuracy: 0.0043 - val_loss: 7.2955 - val_accuracy: 0.0034\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1731 - accuracy: 0.0032\n",
      "Epoch 3: val_loss did not improve from 7.29551\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.1731 - accuracy: 0.0032 - val_loss: 7.3232 - val_accuracy: 0.0026\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1718 - accuracy: 0.0032\n",
      "Epoch 4: val_loss improved from 7.29551 to 7.28912, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 459ms/step - loss: 7.1718 - accuracy: 0.0032 - val_loss: 7.2891 - val_accuracy: 0.0026\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1165 - accuracy: 0.0022\n",
      "Epoch 5: val_loss did not improve from 7.28912\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 7.1165 - accuracy: 0.0022 - val_loss: 7.3192 - val_accuracy: 0.0042\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1078 - accuracy: 0.0054\n",
      "Epoch 6: val_loss did not improve from 7.28912\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 7.1078 - accuracy: 0.0054 - val_loss: 7.3279 - val_accuracy: 0.0050\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1008 - accuracy: 0.0086\n",
      "Epoch 7: val_loss improved from 7.28912 to 7.25179, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 455ms/step - loss: 7.1008 - accuracy: 0.0086 - val_loss: 7.2518 - val_accuracy: 0.0047\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0615 - accuracy: 0.0043\n",
      "Epoch 8: val_loss improved from 7.25179 to 7.25120, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.0615 - accuracy: 0.0043 - val_loss: 7.2512 - val_accuracy: 0.0047\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0688 - accuracy: 0.0097\n",
      "Epoch 9: val_loss did not improve from 7.25120\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 7.0688 - accuracy: 0.0097 - val_loss: 7.2807 - val_accuracy: 0.0050\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0740 - accuracy: 0.0054\n",
      "Epoch 10: val_loss improved from 7.25120 to 7.22493, saving model to warming_up_10_epochs.h5\n",
      "58/58 [==============================] - 26s 456ms/step - loss: 7.0740 - accuracy: 0.0054 - val_loss: 7.2249 - val_accuracy: 0.0055\n",
      "1/1 [==============================] - 1s 868ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7678775b-41e3-482f-8734-5f7353808e0f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://7678775b-41e3-482f-8734-5f7353808e0f/assets\n",
      "2023/12/04 23:06:12 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmp941x4b09\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n",
      "d:\\Conda\\envs\\tf310\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"epochs_10\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")\n",
    "\n",
    "\n",
    "experimento(epochs=10)\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n",
      "Epoch 1/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3902 - accuracy: 0.0032\n",
      "Epoch 1: val_loss improved from inf to 7.33352, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 30s 472ms/step - loss: 7.3902 - accuracy: 0.0032 - val_loss: 7.3335 - val_accuracy: 0.0042\n",
      "Epoch 2/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.2053 - accuracy: 0.0032\n",
      "Epoch 2: val_loss improved from 7.33352 to 7.29165, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 451ms/step - loss: 7.2053 - accuracy: 0.0032 - val_loss: 7.2916 - val_accuracy: 0.0044\n",
      "Epoch 3/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1751 - accuracy: 0.0065\n",
      "Epoch 3: val_loss improved from 7.29165 to 7.26353, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 452ms/step - loss: 7.1751 - accuracy: 0.0065 - val_loss: 7.2635 - val_accuracy: 0.0047\n",
      "Epoch 4/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1223 - accuracy: 0.0054\n",
      "Epoch 4: val_loss improved from 7.26353 to 7.25972, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 7.1223 - accuracy: 0.0054 - val_loss: 7.2597 - val_accuracy: 0.0052\n",
      "Epoch 5/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1292 - accuracy: 0.0065\n",
      "Epoch 5: val_loss did not improve from 7.25972\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 7.1292 - accuracy: 0.0065 - val_loss: 7.2615 - val_accuracy: 0.0044\n",
      "Epoch 6/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1528 - accuracy: 0.0054\n",
      "Epoch 6: val_loss did not improve from 7.25972\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 7.1528 - accuracy: 0.0054 - val_loss: 7.2989 - val_accuracy: 0.0047\n",
      "Epoch 7/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1528 - accuracy: 0.0065\n",
      "Epoch 7: val_loss improved from 7.25972 to 7.25650, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 7.1528 - accuracy: 0.0065 - val_loss: 7.2565 - val_accuracy: 0.0055\n",
      "Epoch 8/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1252 - accuracy: 0.0065\n",
      "Epoch 8: val_loss improved from 7.25650 to 7.23971, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 451ms/step - loss: 7.1252 - accuracy: 0.0065 - val_loss: 7.2397 - val_accuracy: 0.0055\n",
      "Epoch 9/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1033 - accuracy: 0.0076\n",
      "Epoch 9: val_loss improved from 7.23971 to 7.23543, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 7.1033 - accuracy: 0.0076 - val_loss: 7.2354 - val_accuracy: 0.0055\n",
      "Epoch 10/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0950 - accuracy: 0.0054\n",
      "Epoch 10: val_loss did not improve from 7.23543\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0950 - accuracy: 0.0054 - val_loss: 7.3122 - val_accuracy: 0.0044\n",
      "Epoch 11/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0560 - accuracy: 0.0065\n",
      "Epoch 11: val_loss did not improve from 7.23543\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 7.0560 - accuracy: 0.0065 - val_loss: 7.3091 - val_accuracy: 0.0047\n",
      "Epoch 12/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0018 - accuracy: 0.0086\n",
      "Epoch 12: val_loss did not improve from 7.23543\n",
      "58/58 [==============================] - 25s 441ms/step - loss: 7.0018 - accuracy: 0.0086 - val_loss: 7.2565 - val_accuracy: 0.0044\n",
      "Epoch 13/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1101 - accuracy: 0.0086\n",
      "Epoch 13: val_loss improved from 7.23543 to 7.21724, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 7.1101 - accuracy: 0.0086 - val_loss: 7.2172 - val_accuracy: 0.0044\n",
      "Epoch 14/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0427 - accuracy: 0.0043\n",
      "Epoch 14: val_loss did not improve from 7.21724\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 7.0427 - accuracy: 0.0043 - val_loss: 7.2571 - val_accuracy: 0.0057\n",
      "Epoch 15/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0758 - accuracy: 0.0075\n",
      "Epoch 15: val_loss improved from 7.21724 to 7.21290, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 7.0758 - accuracy: 0.0075 - val_loss: 7.2129 - val_accuracy: 0.0044\n",
      "Epoch 16/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0472 - accuracy: 0.0032\n",
      "Epoch 16: val_loss improved from 7.21290 to 7.20167, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 455ms/step - loss: 7.0472 - accuracy: 0.0032 - val_loss: 7.2017 - val_accuracy: 0.0044\n",
      "Epoch 17/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0638 - accuracy: 0.0108\n",
      "Epoch 17: val_loss improved from 7.20167 to 7.19958, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 451ms/step - loss: 7.0638 - accuracy: 0.0108 - val_loss: 7.1996 - val_accuracy: 0.0055\n",
      "Epoch 18/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0424 - accuracy: 0.0097\n",
      "Epoch 18: val_loss did not improve from 7.19958\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 7.0424 - accuracy: 0.0097 - val_loss: 7.2189 - val_accuracy: 0.0055\n",
      "Epoch 19/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9574 - accuracy: 0.0086\n",
      "Epoch 19: val_loss did not improve from 7.19958\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9574 - accuracy: 0.0086 - val_loss: 7.3062 - val_accuracy: 0.0055\n",
      "Epoch 20/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0179 - accuracy: 0.0054\n",
      "Epoch 20: val_loss did not improve from 7.19958\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0179 - accuracy: 0.0054 - val_loss: 7.2307 - val_accuracy: 0.0050\n",
      "Epoch 21/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9463 - accuracy: 0.0054\n",
      "Epoch 21: val_loss improved from 7.19958 to 7.19676, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 452ms/step - loss: 6.9463 - accuracy: 0.0054 - val_loss: 7.1968 - val_accuracy: 0.0055\n",
      "Epoch 22/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0463 - accuracy: 0.0097\n",
      "Epoch 22: val_loss improved from 7.19676 to 7.17317, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 7.0463 - accuracy: 0.0097 - val_loss: 7.1732 - val_accuracy: 0.0055\n",
      "Epoch 23/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9982 - accuracy: 0.0086\n",
      "Epoch 23: val_loss improved from 7.17317 to 7.17233, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 454ms/step - loss: 6.9982 - accuracy: 0.0086 - val_loss: 7.1723 - val_accuracy: 0.0055\n",
      "Epoch 24/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9689 - accuracy: 0.0032\n",
      "Epoch 24: val_loss did not improve from 7.17233\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9689 - accuracy: 0.0032 - val_loss: 7.1943 - val_accuracy: 0.0055\n",
      "Epoch 25/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0092 - accuracy: 0.0032\n",
      "Epoch 25: val_loss did not improve from 7.17233\n",
      "58/58 [==============================] - 25s 444ms/step - loss: 7.0092 - accuracy: 0.0032 - val_loss: 7.2408 - val_accuracy: 0.0057\n",
      "Epoch 26/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9707 - accuracy: 0.0086\n",
      "Epoch 26: val_loss improved from 7.17233 to 7.17012, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.9707 - accuracy: 0.0086 - val_loss: 7.1701 - val_accuracy: 0.0055\n",
      "Epoch 27/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0084 - accuracy: 0.0022\n",
      "Epoch 27: val_loss did not improve from 7.17012\n",
      "58/58 [==============================] - 26s 446ms/step - loss: 7.0084 - accuracy: 0.0022 - val_loss: 7.1861 - val_accuracy: 0.0044\n",
      "Epoch 28/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0457 - accuracy: 0.0054\n",
      "Epoch 28: val_loss did not improve from 7.17012\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0457 - accuracy: 0.0054 - val_loss: 7.2430 - val_accuracy: 0.0055\n",
      "Epoch 29/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9830 - accuracy: 0.0075\n",
      "Epoch 29: val_loss improved from 7.17012 to 7.16702, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 6.9830 - accuracy: 0.0075 - val_loss: 7.1670 - val_accuracy: 0.0047\n",
      "Epoch 30/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0079 - accuracy: 0.0054\n",
      "Epoch 30: val_loss improved from 7.16702 to 7.16159, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 452ms/step - loss: 7.0079 - accuracy: 0.0054 - val_loss: 7.1616 - val_accuracy: 0.0044\n",
      "Epoch 31/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0048 - accuracy: 0.0097\n",
      "Epoch 31: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 446ms/step - loss: 7.0048 - accuracy: 0.0097 - val_loss: 7.1690 - val_accuracy: 0.0047\n",
      "Epoch 32/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9472 - accuracy: 0.0065\n",
      "Epoch 32: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9472 - accuracy: 0.0065 - val_loss: 7.1951 - val_accuracy: 0.0047\n",
      "Epoch 33/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9073 - accuracy: 0.0054\n",
      "Epoch 33: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9073 - accuracy: 0.0054 - val_loss: 7.2423 - val_accuracy: 0.0047\n",
      "Epoch 34/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9666 - accuracy: 0.0086\n",
      "Epoch 34: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9666 - accuracy: 0.0086 - val_loss: 7.1704 - val_accuracy: 0.0047\n",
      "Epoch 35/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0133 - accuracy: 0.0032\n",
      "Epoch 35: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 7.0133 - accuracy: 0.0032 - val_loss: 7.2789 - val_accuracy: 0.0047\n",
      "Epoch 36/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0055 - accuracy: 0.0108\n",
      "Epoch 36: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 7.0055 - accuracy: 0.0108 - val_loss: 7.1694 - val_accuracy: 0.0047\n",
      "Epoch 37/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9714 - accuracy: 0.0054\n",
      "Epoch 37: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9714 - accuracy: 0.0054 - val_loss: 7.1754 - val_accuracy: 0.0047\n",
      "Epoch 38/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9358 - accuracy: 0.0054\n",
      "Epoch 38: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9358 - accuracy: 0.0054 - val_loss: 7.2023 - val_accuracy: 0.0047\n",
      "Epoch 39/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9220 - accuracy: 0.0054\n",
      "Epoch 39: val_loss did not improve from 7.16159\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 6.9220 - accuracy: 0.0054 - val_loss: 7.2225 - val_accuracy: 0.0044\n",
      "Epoch 40/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9919 - accuracy: 0.0075\n",
      "Epoch 40: val_loss improved from 7.16159 to 7.15094, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.9919 - accuracy: 0.0075 - val_loss: 7.1509 - val_accuracy: 0.0047\n",
      "Epoch 41/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9441 - accuracy: 0.0075\n",
      "Epoch 41: val_loss did not improve from 7.15094\n",
      "58/58 [==============================] - 26s 446ms/step - loss: 6.9441 - accuracy: 0.0075 - val_loss: 7.2015 - val_accuracy: 0.0047\n",
      "Epoch 42/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9899 - accuracy: 0.0054\n",
      "Epoch 42: val_loss improved from 7.15094 to 7.14564, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 453ms/step - loss: 6.9899 - accuracy: 0.0054 - val_loss: 7.1456 - val_accuracy: 0.0047\n",
      "Epoch 43/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9673 - accuracy: 0.0054\n",
      "Epoch 43: val_loss improved from 7.14564 to 7.13779, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 454ms/step - loss: 6.9673 - accuracy: 0.0054 - val_loss: 7.1378 - val_accuracy: 0.0047\n",
      "Epoch 44/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8874 - accuracy: 0.0076\n",
      "Epoch 44: val_loss did not improve from 7.13779\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.8874 - accuracy: 0.0076 - val_loss: 7.1450 - val_accuracy: 0.0050\n",
      "Epoch 45/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9201 - accuracy: 0.0065\n",
      "Epoch 45: val_loss did not improve from 7.13779\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.9201 - accuracy: 0.0065 - val_loss: 7.1569 - val_accuracy: 0.0050\n",
      "Epoch 46/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9337 - accuracy: 0.0043\n",
      "Epoch 46: val_loss did not improve from 7.13779\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9337 - accuracy: 0.0043 - val_loss: 7.1397 - val_accuracy: 0.0052\n",
      "Epoch 47/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.0147 - accuracy: 0.0043\n",
      "Epoch 47: val_loss improved from 7.13779 to 7.12613, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 449ms/step - loss: 7.0147 - accuracy: 0.0043 - val_loss: 7.1261 - val_accuracy: 0.0050\n",
      "Epoch 48/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9558 - accuracy: 0.0022\n",
      "Epoch 48: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9558 - accuracy: 0.0022 - val_loss: 7.1845 - val_accuracy: 0.0050\n",
      "Epoch 49/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9448 - accuracy: 0.0032\n",
      "Epoch 49: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9448 - accuracy: 0.0032 - val_loss: 7.1280 - val_accuracy: 0.0057\n",
      "Epoch 50/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8911 - accuracy: 0.0065\n",
      "Epoch 50: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.8911 - accuracy: 0.0065 - val_loss: 7.1468 - val_accuracy: 0.0047\n",
      "Epoch 51/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9228 - accuracy: 0.0076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyngrok.process.ngrok:t=2023-12-04T23:28:06-0500 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=73df649ad33c clientid=2ebd534bfd6f26ec87f1c66bd3feffe1\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-04T23:28:06-0500 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"session closed\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: val_loss did not improve from 7.12613\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9228 - accuracy: 0.0076 - val_loss: 7.2213 - val_accuracy: 0.0050\n",
      "Epoch 52/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9114 - accuracy: 0.0097\n",
      "Epoch 52: val_loss improved from 7.12613 to 7.12261, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.9114 - accuracy: 0.0097 - val_loss: 7.1226 - val_accuracy: 0.0052\n",
      "Epoch 53/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8821 - accuracy: 0.0043\n",
      "Epoch 53: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 26s 445ms/step - loss: 6.8821 - accuracy: 0.0043 - val_loss: 7.1263 - val_accuracy: 0.0055\n",
      "Epoch 54/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9485 - accuracy: 0.0054\n",
      "Epoch 54: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9485 - accuracy: 0.0054 - val_loss: 7.1475 - val_accuracy: 0.0052\n",
      "Epoch 55/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9484 - accuracy: 0.0108\n",
      "Epoch 55: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9484 - accuracy: 0.0108 - val_loss: 7.1346 - val_accuracy: 0.0060\n",
      "Epoch 56/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8958 - accuracy: 0.0043\n",
      "Epoch 56: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.8958 - accuracy: 0.0043 - val_loss: 7.1309 - val_accuracy: 0.0060\n",
      "Epoch 57/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9445 - accuracy: 0.0097\n",
      "Epoch 57: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9445 - accuracy: 0.0097 - val_loss: 7.1568 - val_accuracy: 0.0055\n",
      "Epoch 58/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9065 - accuracy: 0.0086\n",
      "Epoch 58: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9065 - accuracy: 0.0086 - val_loss: 7.2659 - val_accuracy: 0.0055\n",
      "Epoch 59/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9095 - accuracy: 0.0054\n",
      "Epoch 59: val_loss did not improve from 7.12261\n",
      "58/58 [==============================] - 25s 441ms/step - loss: 6.9095 - accuracy: 0.0054 - val_loss: 7.1337 - val_accuracy: 0.0060\n",
      "Epoch 60/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8842 - accuracy: 0.0011\n",
      "Epoch 60: val_loss improved from 7.12261 to 7.11551, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 450ms/step - loss: 6.8842 - accuracy: 0.0011 - val_loss: 7.1155 - val_accuracy: 0.0060\n",
      "Epoch 61/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9358 - accuracy: 0.0032\n",
      "Epoch 61: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 443ms/step - loss: 6.9358 - accuracy: 0.0032 - val_loss: 7.1945 - val_accuracy: 0.0063\n",
      "Epoch 62/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9092 - accuracy: 0.0097\n",
      "Epoch 62: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 26s 444ms/step - loss: 6.9092 - accuracy: 0.0097 - val_loss: 7.1817 - val_accuracy: 0.0052\n",
      "Epoch 63/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9088 - accuracy: 0.0065\n",
      "Epoch 63: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9088 - accuracy: 0.0065 - val_loss: 7.1643 - val_accuracy: 0.0055\n",
      "Epoch 64/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.8839 - accuracy: 0.0043\n",
      "Epoch 64: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.8839 - accuracy: 0.0043 - val_loss: 7.2325 - val_accuracy: 0.0057\n",
      "Epoch 65/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9466 - accuracy: 0.0086\n",
      "Epoch 65: val_loss did not improve from 7.11551\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9466 - accuracy: 0.0086 - val_loss: 7.1346 - val_accuracy: 0.0047\n",
      "Epoch 66/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9298 - accuracy: 0.0119\n",
      "Epoch 66: val_loss improved from 7.11551 to 7.11032, saving model to warming_up_70_epochs.h5\n",
      "58/58 [==============================] - 26s 448ms/step - loss: 6.9298 - accuracy: 0.0119 - val_loss: 7.1103 - val_accuracy: 0.0047\n",
      "Epoch 67/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9358 - accuracy: 0.0032\n",
      "Epoch 67: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9358 - accuracy: 0.0032 - val_loss: 7.1580 - val_accuracy: 0.0047\n",
      "Epoch 68/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9232 - accuracy: 0.0097\n",
      "Epoch 68: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9232 - accuracy: 0.0097 - val_loss: 7.1260 - val_accuracy: 0.0047\n",
      "Epoch 69/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9344 - accuracy: 0.0054\n",
      "Epoch 69: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9344 - accuracy: 0.0054 - val_loss: 7.1142 - val_accuracy: 0.0047\n",
      "Epoch 70/70\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.9281 - accuracy: 0.0097\n",
      "Epoch 70: val_loss did not improve from 7.11032\n",
      "58/58 [==============================] - 25s 442ms/step - loss: 6.9281 - accuracy: 0.0097 - val_loss: 7.1517 - val_accuracy: 0.0047\n",
      "1/1 [==============================] - 1s 872ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3c9f7aae-3e62-44b2-8506-ea5df6a6a91a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://3c9f7aae-3e62-44b2-8506-ea5df6a6a91a/assets\n",
      "2023/12/04 23:37:23 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmp_ddtm17e\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T00:38:02-0500 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=96dc9487a7ba clientid=2ebd534bfd6f26ec87f1c66bd3feffe1\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T00:38:02-0500 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"session closed\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:44-0500 lvl=eror msg=\"heartbeat timeout, terminating session\" obj=tunnels.session obj=csess id=ee731389ddea clientid=2ebd534bfd6f26ec87f1c66bd3feffe1\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:44-0500 lvl=eror msg=\"session closed, starting reconnect loop\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"session closed\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:45-0500 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:46-0500 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n",
      "ERROR:pyngrok.process.ngrok:t=2023-12-05T08:05:47-0500 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session obj=csess id=f6ff9fa04fc5 err=\"failed to dial ngrok server with address \\\"connect.us.ngrok-agent.com:443\\\": dial tcp: lookup connect.us.ngrok-agent.com: no such host\"\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"epochs_70\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")\n",
    "\n",
    "\n",
    "experimento(epochs=70)\n",
    "\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba 70 con reentrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimento2(epochs_warm=10,epochs_train=70):\n",
    "        \n",
    "    model = tf.keras.applications.resnet50.ResNet50(include_top=False,\n",
    "                                                    weights='imagenet',\n",
    "                                                    input_shape=(250,250,3))\n",
    "\n",
    "    # Congelamos el extractor de características (Transfer Learning)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "\n",
    "    # Creamos una capa de pooling para consolidar los feature maps de salida en 1024 valores\n",
    "    pool = tf.keras.layers.GlobalAveragePooling2D()(model.output)\n",
    "    # Agregamos una capa densa\n",
    "    dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
    "    # Agregamos dropout para regularización\n",
    "    drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
    "    # Agregamos una capa de salida\n",
    "    dense2 = tf.keras.layers.Dense(units=train_generator.num_classes, activation=\"softmax\")(drop1)\n",
    "    # Definimos nuestro modelo de transfer learning\n",
    "    ft_model = tf.keras.models.Model(inputs=[model.input], outputs=[dense2])\n",
    "    # Compilamos el modelo\n",
    "    ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "                    metrics=[\"accuracy\"])\n",
    "    # ft_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    # Definimos el callback\n",
    "    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f\"warming_up_{epochs_warm}_epochs_warm.h5\",\n",
    "                                                    monitor=\"val_loss\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    hist_ft = ft_model.fit(x=train_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs_warm,\n",
    "                        steps_per_epoch=train_generator.samples//128,\n",
    "                        callbacks=[best_callback])\n",
    "\n",
    "    ########################################## re entrenamiento ##########################################\n",
    "\n",
    "    # Hacemos entrenables todas las capas\n",
    "    for layer in ft_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Disminuímos el learning rate\n",
    "    ft_model.compile(loss=\"categorical_crossentropy\",\n",
    "                    optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
    "                    metrics=[\"accuracy\"])\n",
    "\n",
    "    # Cargamos los pesos del calentamiento\n",
    "    ft_model.load_weights(f\"warming_up_{epochs_warm}_epochs_warm.h5\")\n",
    "\n",
    "    # Definimos el callback\n",
    "    best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"fine_tuning.h5\",\n",
    "                                                    monitor=\"val_loss\",\n",
    "                                                    verbose=True,\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    mode=\"min\")\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    hist_ft = ft_model.fit(x=train_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=epochs_train,\n",
    "                        steps_per_epoch=train_generator.samples//16,\n",
    "                        callbacks=[best_callback])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def mapeo_indices(listado):\n",
    "\n",
    "        listado_clases = list(validation_generator.class_indices.keys())\n",
    "\n",
    "        true_index = np.argmax(listado,axis=1)\n",
    "        salida_real = [listado_clases[i] for i in true_index]\n",
    "\n",
    "        return salida_real\n",
    "\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(len(validation_generator)):\n",
    "        X_val_batch, y_val_batch = validation_generator.next()\n",
    "        y_pred_batch = ft_model.predict(X_val_batch)\n",
    "        y_true += mapeo_indices(y_val_batch)\n",
    "        y_pred += mapeo_indices(y_pred_batch)\n",
    "\n",
    "\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
    "        })\n",
    "\n",
    "\n",
    "    mlflow.sklearn.log_model(ft_model, f\"model_{epochs_warm}_epochs_warm_{epochs_train}_epochs_train\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run:<ActiveRun: >\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.4088 - accuracy: 0.0043\n",
      "Epoch 1: val_loss improved from inf to 7.37874, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 46s 710ms/step - loss: 7.4088 - accuracy: 0.0043 - val_loss: 7.3787 - val_accuracy: 0.0031\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.3056 - accuracy: 0.0065\n",
      "Epoch 2: val_loss improved from 7.37874 to 7.31343, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 28s 483ms/step - loss: 7.3056 - accuracy: 0.0065 - val_loss: 7.3134 - val_accuracy: 0.0026\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.2482 - accuracy: 0.0022\n",
      "Epoch 3: val_loss improved from 7.31343 to 7.28730, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 28s 484ms/step - loss: 7.2482 - accuracy: 0.0022 - val_loss: 7.2873 - val_accuracy: 0.0031\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1803 - accuracy: 0.0043\n",
      "Epoch 4: val_loss improved from 7.28730 to 7.27428, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 477ms/step - loss: 7.1803 - accuracy: 0.0043 - val_loss: 7.2743 - val_accuracy: 0.0050\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1484 - accuracy: 0.0075\n",
      "Epoch 5: val_loss improved from 7.27428 to 7.26903, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 477ms/step - loss: 7.1484 - accuracy: 0.0075 - val_loss: 7.2690 - val_accuracy: 0.0050\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1801 - accuracy: 0.0086\n",
      "Epoch 6: val_loss improved from 7.26903 to 7.26600, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 475ms/step - loss: 7.1801 - accuracy: 0.0086 - val_loss: 7.2660 - val_accuracy: 0.0050\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1533 - accuracy: 0.0054\n",
      "Epoch 7: val_loss improved from 7.26600 to 7.25747, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 473ms/step - loss: 7.1533 - accuracy: 0.0054 - val_loss: 7.2575 - val_accuracy: 0.0050\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1601 - accuracy: 0.0075\n",
      "Epoch 8: val_loss improved from 7.25747 to 7.24848, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 474ms/step - loss: 7.1601 - accuracy: 0.0075 - val_loss: 7.2485 - val_accuracy: 0.0050\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1182 - accuracy: 0.0054\n",
      "Epoch 9: val_loss improved from 7.24848 to 7.24173, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 472ms/step - loss: 7.1182 - accuracy: 0.0054 - val_loss: 7.2417 - val_accuracy: 0.0050\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - ETA: 0s - loss: 7.1505 - accuracy: 0.0054\n",
      "Epoch 10: val_loss improved from 7.24173 to 7.23634, saving model to warming_up_10_epochs_warm.h5\n",
      "58/58 [==============================] - 27s 470ms/step - loss: 7.1505 - accuracy: 0.0054 - val_loss: 7.2363 - val_accuracy: 0.0050\n",
      "Epoch 1/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 7.0560 - accuracy: 0.0080\n",
      "Epoch 1: val_loss improved from inf to 7.26265, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 132s 265ms/step - loss: 7.0560 - accuracy: 0.0080 - val_loss: 7.2626 - val_accuracy: 0.0042\n",
      "Epoch 2/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 6.6745 - accuracy: 0.0294\n",
      "Epoch 2: val_loss improved from 7.26265 to 6.77277, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 6.6745 - accuracy: 0.0294 - val_loss: 6.7728 - val_accuracy: 0.0326\n",
      "Epoch 3/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 6.2341 - accuracy: 0.0585\n",
      "Epoch 3: val_loss improved from 6.77277 to 6.46005, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 6.2341 - accuracy: 0.0585 - val_loss: 6.4600 - val_accuracy: 0.0575\n",
      "Epoch 4/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.8669 - accuracy: 0.0904\n",
      "Epoch 4: val_loss improved from 6.46005 to 6.02275, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 5.8669 - accuracy: 0.0904 - val_loss: 6.0228 - val_accuracy: 0.1011\n",
      "Epoch 5/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.5304 - accuracy: 0.1260\n",
      "Epoch 5: val_loss improved from 6.02275 to 5.91919, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 5.5304 - accuracy: 0.1260 - val_loss: 5.9192 - val_accuracy: 0.1222\n",
      "Epoch 6/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 5.1976 - accuracy: 0.1619\n",
      "Epoch 6: val_loss improved from 5.91919 to 5.46014, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 5.1976 - accuracy: 0.1619 - val_loss: 5.4601 - val_accuracy: 0.1468\n",
      "Epoch 7/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.9127 - accuracy: 0.1935\n",
      "Epoch 7: val_loss improved from 5.46014 to 5.23247, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.9127 - accuracy: 0.1935 - val_loss: 5.2325 - val_accuracy: 0.1870\n",
      "Epoch 8/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.6383 - accuracy: 0.2181\n",
      "Epoch 8: val_loss improved from 5.23247 to 4.89236, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.6383 - accuracy: 0.2181 - val_loss: 4.8924 - val_accuracy: 0.2259\n",
      "Epoch 9/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.3870 - accuracy: 0.2430\n",
      "Epoch 9: val_loss improved from 4.89236 to 4.87721, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.3870 - accuracy: 0.2430 - val_loss: 4.8772 - val_accuracy: 0.2162\n",
      "Epoch 10/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 4.1411 - accuracy: 0.2638\n",
      "Epoch 10: val_loss improved from 4.87721 to 4.50193, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 4.1411 - accuracy: 0.2638 - val_loss: 4.5019 - val_accuracy: 0.2669\n",
      "Epoch 11/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.9498 - accuracy: 0.2839\n",
      "Epoch 11: val_loss improved from 4.50193 to 4.25786, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 3.9498 - accuracy: 0.2839 - val_loss: 4.2579 - val_accuracy: 0.2847\n",
      "Epoch 12/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.7130 - accuracy: 0.3174\n",
      "Epoch 12: val_loss improved from 4.25786 to 3.92056, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 3.7130 - accuracy: 0.3174 - val_loss: 3.9206 - val_accuracy: 0.3351\n",
      "Epoch 13/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.5086 - accuracy: 0.3365\n",
      "Epoch 13: val_loss did not improve from 3.92056\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 3.5086 - accuracy: 0.3365 - val_loss: 4.1547 - val_accuracy: 0.2977\n",
      "Epoch 14/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.3213 - accuracy: 0.3567\n",
      "Epoch 14: val_loss improved from 3.92056 to 3.57545, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 3.3213 - accuracy: 0.3567 - val_loss: 3.5755 - val_accuracy: 0.3782\n",
      "Epoch 15/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 3.1490 - accuracy: 0.3792\n",
      "Epoch 15: val_loss did not improve from 3.57545\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 3.1490 - accuracy: 0.3792 - val_loss: 3.7577 - val_accuracy: 0.3633\n",
      "Epoch 16/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.9660 - accuracy: 0.4037\n",
      "Epoch 16: val_loss did not improve from 3.57545\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.9660 - accuracy: 0.4037 - val_loss: 3.7870 - val_accuracy: 0.3565\n",
      "Epoch 17/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.8032 - accuracy: 0.4250\n",
      "Epoch 17: val_loss improved from 3.57545 to 3.24519, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 2.8032 - accuracy: 0.4250 - val_loss: 3.2452 - val_accuracy: 0.4437\n",
      "Epoch 18/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.6625 - accuracy: 0.4440\n",
      "Epoch 18: val_loss improved from 3.24519 to 2.77657, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 2.6625 - accuracy: 0.4440 - val_loss: 2.7766 - val_accuracy: 0.5242\n",
      "Epoch 19/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.5084 - accuracy: 0.4711\n",
      "Epoch 19: val_loss improved from 2.77657 to 2.58006, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.5084 - accuracy: 0.4711 - val_loss: 2.5801 - val_accuracy: 0.5581\n",
      "Epoch 20/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.3802 - accuracy: 0.4908\n",
      "Epoch 20: val_loss improved from 2.58006 to 2.51856, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.3802 - accuracy: 0.4908 - val_loss: 2.5186 - val_accuracy: 0.5787\n",
      "Epoch 21/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.2477 - accuracy: 0.5138\n",
      "Epoch 21: val_loss improved from 2.51856 to 2.51577, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.2477 - accuracy: 0.5138 - val_loss: 2.5158 - val_accuracy: 0.5819\n",
      "Epoch 22/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.1251 - accuracy: 0.5295\n",
      "Epoch 22: val_loss improved from 2.51577 to 2.33377, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 2.1251 - accuracy: 0.5295 - val_loss: 2.3338 - val_accuracy: 0.6119\n",
      "Epoch 23/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 2.0302 - accuracy: 0.5462\n",
      "Epoch 23: val_loss improved from 2.33377 to 2.16051, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 2.0302 - accuracy: 0.5462 - val_loss: 2.1605 - val_accuracy: 0.6574\n",
      "Epoch 24/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.9309 - accuracy: 0.5656\n",
      "Epoch 24: val_loss improved from 2.16051 to 1.98830, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.9309 - accuracy: 0.5656 - val_loss: 1.9883 - val_accuracy: 0.6939\n",
      "Epoch 25/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.8184 - accuracy: 0.5823\n",
      "Epoch 25: val_loss did not improve from 1.98830\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.8184 - accuracy: 0.5823 - val_loss: 2.1053 - val_accuracy: 0.6597\n",
      "Epoch 26/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.7530 - accuracy: 0.5912\n",
      "Epoch 26: val_loss improved from 1.98830 to 1.84956, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.7530 - accuracy: 0.5912 - val_loss: 1.8496 - val_accuracy: 0.7229\n",
      "Epoch 27/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.6135 - accuracy: 0.6172\n",
      "Epoch 27: val_loss improved from 1.84956 to 1.84864, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.6135 - accuracy: 0.6172 - val_loss: 1.8486 - val_accuracy: 0.7239\n",
      "Epoch 28/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.5818 - accuracy: 0.6286\n",
      "Epoch 28: val_loss improved from 1.84864 to 1.78297, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.5818 - accuracy: 0.6286 - val_loss: 1.7830 - val_accuracy: 0.7362\n",
      "Epoch 29/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.5219 - accuracy: 0.6330\n",
      "Epoch 29: val_loss improved from 1.78297 to 1.65374, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.5219 - accuracy: 0.6330 - val_loss: 1.6537 - val_accuracy: 0.7728\n",
      "Epoch 30/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.4349 - accuracy: 0.6563\n",
      "Epoch 30: val_loss improved from 1.65374 to 1.63356, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.4349 - accuracy: 0.6563 - val_loss: 1.6336 - val_accuracy: 0.7801\n",
      "Epoch 31/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3975 - accuracy: 0.6604\n",
      "Epoch 31: val_loss did not improve from 1.63356\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.3975 - accuracy: 0.6604 - val_loss: 1.7270 - val_accuracy: 0.7490\n",
      "Epoch 32/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.3006 - accuracy: 0.6758\n",
      "Epoch 32: val_loss improved from 1.63356 to 1.59470, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 1.3006 - accuracy: 0.6758 - val_loss: 1.5947 - val_accuracy: 0.7890\n",
      "Epoch 33/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.2997 - accuracy: 0.6840\n",
      "Epoch 33: val_loss improved from 1.59470 to 1.48568, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.2997 - accuracy: 0.6840 - val_loss: 1.4857 - val_accuracy: 0.8070\n",
      "Epoch 34/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.2088 - accuracy: 0.7039\n",
      "Epoch 34: val_loss improved from 1.48568 to 1.36882, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.2088 - accuracy: 0.7039 - val_loss: 1.3688 - val_accuracy: 0.8245\n",
      "Epoch 35/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1789 - accuracy: 0.7078\n",
      "Epoch 35: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.1789 - accuracy: 0.7078 - val_loss: 1.5068 - val_accuracy: 0.8096\n",
      "Epoch 36/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.1282 - accuracy: 0.7143\n",
      "Epoch 36: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.1282 - accuracy: 0.7143 - val_loss: 1.5909 - val_accuracy: 0.7788\n",
      "Epoch 37/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0961 - accuracy: 0.7241\n",
      "Epoch 37: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.0961 - accuracy: 0.7241 - val_loss: 1.6450 - val_accuracy: 0.7764\n",
      "Epoch 38/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0790 - accuracy: 0.7309\n",
      "Epoch 38: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.0790 - accuracy: 0.7309 - val_loss: 1.4289 - val_accuracy: 0.8180\n",
      "Epoch 39/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 1.0183 - accuracy: 0.7392\n",
      "Epoch 39: val_loss did not improve from 1.36882\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 1.0183 - accuracy: 0.7392 - val_loss: 1.4383 - val_accuracy: 0.8308\n",
      "Epoch 40/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9792 - accuracy: 0.7497\n",
      "Epoch 40: val_loss improved from 1.36882 to 1.35827, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.9792 - accuracy: 0.7497 - val_loss: 1.3583 - val_accuracy: 0.8282\n",
      "Epoch 41/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9917 - accuracy: 0.7509\n",
      "Epoch 41: val_loss improved from 1.35827 to 1.32865, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.9917 - accuracy: 0.7509 - val_loss: 1.3287 - val_accuracy: 0.8438\n",
      "Epoch 42/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9357 - accuracy: 0.7547\n",
      "Epoch 42: val_loss did not improve from 1.32865\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.9357 - accuracy: 0.7547 - val_loss: 1.3427 - val_accuracy: 0.8410\n",
      "Epoch 43/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9421 - accuracy: 0.7566\n",
      "Epoch 43: val_loss did not improve from 1.32865\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.9421 - accuracy: 0.7566 - val_loss: 1.4990 - val_accuracy: 0.8172\n",
      "Epoch 44/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.7674\n",
      "Epoch 44: val_loss did not improve from 1.32865\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.9012 - accuracy: 0.7674 - val_loss: 1.3993 - val_accuracy: 0.8349\n",
      "Epoch 45/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8490 - accuracy: 0.7797\n",
      "Epoch 45: val_loss improved from 1.32865 to 1.29788, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.8490 - accuracy: 0.7797 - val_loss: 1.2979 - val_accuracy: 0.8467\n",
      "Epoch 46/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8598 - accuracy: 0.7781\n",
      "Epoch 46: val_loss improved from 1.29788 to 1.21988, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.8598 - accuracy: 0.7781 - val_loss: 1.2199 - val_accuracy: 0.8540\n",
      "Epoch 47/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.7865\n",
      "Epoch 47: val_loss did not improve from 1.21988\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.8167 - accuracy: 0.7865 - val_loss: 1.3235 - val_accuracy: 0.8446\n",
      "Epoch 48/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7750 - accuracy: 0.7957\n",
      "Epoch 48: val_loss improved from 1.21988 to 1.14264, saving model to fine_tuning.h5\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7750 - accuracy: 0.7957 - val_loss: 1.1426 - val_accuracy: 0.8736\n",
      "Epoch 49/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.8174 - accuracy: 0.7876\n",
      "Epoch 49: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.8174 - accuracy: 0.7876 - val_loss: 1.3087 - val_accuracy: 0.8425\n",
      "Epoch 50/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.7961\n",
      "Epoch 50: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.7960 - accuracy: 0.7961 - val_loss: 1.6101 - val_accuracy: 0.7999\n",
      "Epoch 51/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7575 - accuracy: 0.8005\n",
      "Epoch 51: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7575 - accuracy: 0.8005 - val_loss: 1.1651 - val_accuracy: 0.8770\n",
      "Epoch 52/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7578 - accuracy: 0.8014\n",
      "Epoch 52: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7578 - accuracy: 0.8014 - val_loss: 1.2461 - val_accuracy: 0.8642\n",
      "Epoch 53/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7231 - accuracy: 0.8073\n",
      "Epoch 53: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.7231 - accuracy: 0.8073 - val_loss: 1.2045 - val_accuracy: 0.8645\n",
      "Epoch 54/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6670 - accuracy: 0.8242\n",
      "Epoch 54: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 264ms/step - loss: 0.6670 - accuracy: 0.8242 - val_loss: 1.3429 - val_accuracy: 0.8480\n",
      "Epoch 55/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.7287 - accuracy: 0.8077\n",
      "Epoch 55: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.7287 - accuracy: 0.8077 - val_loss: 1.3437 - val_accuracy: 0.8558\n",
      "Epoch 56/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.8238\n",
      "Epoch 56: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6741 - accuracy: 0.8238 - val_loss: 1.2437 - val_accuracy: 0.8692\n",
      "Epoch 57/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6406 - accuracy: 0.8283\n",
      "Epoch 57: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.6406 - accuracy: 0.8283 - val_loss: 1.2179 - val_accuracy: 0.8681\n",
      "Epoch 58/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.8180\n",
      "Epoch 58: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6642 - accuracy: 0.8180 - val_loss: 1.2797 - val_accuracy: 0.8676\n",
      "Epoch 59/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6443 - accuracy: 0.8291\n",
      "Epoch 59: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6443 - accuracy: 0.8291 - val_loss: 1.8228 - val_accuracy: 0.7566\n",
      "Epoch 60/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6484 - accuracy: 0.8308\n",
      "Epoch 60: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6484 - accuracy: 0.8308 - val_loss: 1.2013 - val_accuracy: 0.8663\n",
      "Epoch 61/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6326 - accuracy: 0.8320\n",
      "Epoch 61: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6326 - accuracy: 0.8320 - val_loss: 1.2300 - val_accuracy: 0.8699\n",
      "Epoch 62/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6424 - accuracy: 0.8272\n",
      "Epoch 62: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6424 - accuracy: 0.8272 - val_loss: 1.2851 - val_accuracy: 0.8598\n",
      "Epoch 63/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.8287\n",
      "Epoch 63: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.6300 - accuracy: 0.8287 - val_loss: 1.2419 - val_accuracy: 0.8600\n",
      "Epoch 64/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.8347\n",
      "Epoch 64: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.6125 - accuracy: 0.8347 - val_loss: 1.1897 - val_accuracy: 0.8733\n",
      "Epoch 65/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.6009 - accuracy: 0.8416\n",
      "Epoch 65: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.6009 - accuracy: 0.8416 - val_loss: 1.1755 - val_accuracy: 0.8786\n",
      "Epoch 66/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5905 - accuracy: 0.8396\n",
      "Epoch 66: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 261ms/step - loss: 0.5905 - accuracy: 0.8396 - val_loss: 1.3911 - val_accuracy: 0.8423\n",
      "Epoch 67/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5796 - accuracy: 0.8397\n",
      "Epoch 67: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.5796 - accuracy: 0.8397 - val_loss: 1.1707 - val_accuracy: 0.8759\n",
      "Epoch 68/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5414 - accuracy: 0.8547\n",
      "Epoch 68: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.5414 - accuracy: 0.8547 - val_loss: 1.4339 - val_accuracy: 0.8336\n",
      "Epoch 69/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8440\n",
      "Epoch 69: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 124s 263ms/step - loss: 0.5619 - accuracy: 0.8440 - val_loss: 1.3554 - val_accuracy: 0.8694\n",
      "Epoch 70/70\n",
      "470/470 [==============================] - ETA: 0s - loss: 0.5862 - accuracy: 0.8412\n",
      "Epoch 70: val_loss did not improve from 1.14264\n",
      "470/470 [==============================] - 123s 262ms/step - loss: 0.5862 - accuracy: 0.8412 - val_loss: 1.2654 - val_accuracy: 0.8611\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://684b2b9c-21bf-4ed5-b39c-61430eb706fa/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://684b2b9c-21bf-4ed5-b39c-61430eb706fa/assets\n",
      "2023/12/05 10:46:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\andre\\AppData\\Local\\Temp\\tmpgb04_ljx\\model\\model.pkl, flavor: sklearn), fall back to return ['scikit-learn==1.3.2', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback.\n"
     ]
    }
   ],
   "source": [
    "run = mlflow.start_run(\n",
    "    experiment_id = exp_id,\n",
    "    run_name=\"epochs_10_warm_70_train\"\n",
    "    )\n",
    "\n",
    "print(f\"run:{run}\")\n",
    "\n",
    "\n",
    "experimento2(10,70)\n",
    "\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mejor opción encontrada es tomar el modelo base, hacer calentamiento y entrenar durante un número extenso de épocas."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
